[
  {
    "id": 1,
    "questionId": 1,
    "questionText": "A company is implementing an application on Amazon EC2 instances. The application needs to process incoming transactions. When the application detects a transaction that is not valid, the application must send a chat message to the company's support team. To send the message, the application needs to retrieve the access token to authenticate by using the chat API.\nA developer needs to implement a solution to store the access token. The access token must be encrypted at rest and in transit. The access token must also be accessible from other AWS accounts.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "questionHint": null,
    "answers": [
      {
        "id": 1,
        "answerText": "Use an AWS Systems Manager Parameter Store SecureString parameter that uses an AWS Key Management Service (AWS KMS) AWS managed key to store the access token. Add a resource-based policy to the parameter to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Parameter Store. Retrieve the token from Parameter Store with the decrypt flag enabled. Use the decrypted access token to send the message to the chat.",
        "isCorrect": false
      },
      {
        "id": 2,
        "answerText": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) customer managed key. Store the access token in an Amazon DynamoDB table. Update the IAM role of the EC2 instances with permissions to access DynamoDB and AWS KMS. Retrieve the token from DynamoDDecrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the message to the chat.",
        "isCorrect": false
      },
      {
        "id": 3,
        "answerText": "Use AWS Secrets Manager with an AWS Key Management Service (AWS KMS) customer managed key to store the access token. Add a resource-based policy to the secret to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Secrets Manager. Retrieve the token from Secrets Manager. Use the decrypted access token to send the message to the chat.",
        "isCorrect": true
      },
      {
        "id": 4,
        "answerText": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) AWS managed key. Store the access token in an Amazon S3 bucket. Add a bucket policy to the S3 bucket to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Amazon S3 and AWS KMS. Retrieve the token from the S3 bucket. Decrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the massage to the chat.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 2,
    "questionId": 2,
    "questionText": "A company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.\nWhich solution will meet these requirements?",
    "questionHint": null,
    "answers": [
      {
        "id": 5,
        "answerText": "Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.",
        "isCorrect": false
      },
      {
        "id": 6,
        "answerText": "Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.",
        "isCorrect": false
      },
      {
        "id": 7,
        "answerText": "Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.",
        "isCorrect": false
      },
      {
        "id": 8,
        "answerText": "Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 3,
    "questionId": 3,
    "questionText": "An application is using Amazon Cognito user pools and identity pools for secure access. A developer wants to integrate the user-specific file upload and download features in the application with Amazon S3. The developer must ensure that the files are saved and retrieved in a secure manner and that users can access only their own files. The file sizes range from 3 KB to 300 MB.\nWhich option will meet these requirements with the HIGHEST level of security?",
    "questionHint": null,
    "answers": [
      {
        "id": 9,
        "answerText": "Use S3 Event Notifications to validate the file upload and download requests and update the user interface (UI).",
        "isCorrect": false
      },
      {
        "id": 10,
        "answerText": "Save the details of the uploaded files in a separate Amazon DynamoDB table. Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
        "isCorrect": false
      },
      {
        "id": 11,
        "answerText": "Use Amazon API Gateway and an AWS Lambda function to upload and download files. Validate each request in the Lambda function before performing the requested operation.",
        "isCorrect": false
      },
      {
        "id": 12,
        "answerText": "Use an IAM policy within the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 4,
    "questionId": 4,
    "questionText": "A company is building a scalable data management solution by using AWS services to improve the speed and agility of development. The solution will ingest large volumes of data from various sources and will process this data through multiple business rules and transformations.\nThe solution requires business rules to run in sequence and to handle reprocessing of data if errors occur when the business rules run. The company needs the solution to be scalable and to require the least possible maintenance.\nWhich AWS service should the company use to manage and automate the orchestration of the data flows to meet these requirements?",
    "questionHint": null,
    "answers": [
      {
        "id": 13,
        "answerText": "AWS Batch",
        "isCorrect": false
      },
      {
        "id": 14,
        "answerText": "AWS Step Functions",
        "isCorrect": true
      },
      {
        "id": 15,
        "answerText": "AWS Glue",
        "isCorrect": false
      },
      {
        "id": 16,
        "answerText": "AWS Lambda",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 5,
    "questionId": 5,
    "questionText": "A developer has created an AWS Lambda function that is written in Python. The Lambda function reads data from objects in Amazon S3 and writes data to an Amazon DynamoDB table. The function is successfully invoked from an S3 event notification when an object is created. However, the function fails when it attempts to write to the DynamoDB table.\nWhat is the MOST likely cause of this issue?",
    "questionHint": null,
    "answers": [
      {
        "id": 17,
        "answerText": "The Lambda function's concurrency limit has been exceeded.",
        "isCorrect": false
      },
      {
        "id": 18,
        "answerText": "DynamoDB table requires a global secondary index (GSI) to support writes.",
        "isCorrect": false
      },
      {
        "id": 19,
        "answerText": "The Lambda function does not have IAM permissions to write to DynamoDB.",
        "isCorrect": true
      },
      {
        "id": 20,
        "answerText": "The DynamoDB table is not running in the same Availability Zone as the Lambda function.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 6,
    "questionId": 6,
    "questionText": "A developer is creating an AWS CloudFormation template to deploy Amazon EC2 instances across multiple AWS accounts. The developer must choose the EC2 instances from a list of approved instance types.\nHow can the developer incorporate the list of approved instance types in the CloudFormation template?",
    "questionHint": null,
    "answers": [
      {
        "id": 21,
        "answerText": "Create a separate CloudFormation template for each EC2 instance type in the list.",
        "isCorrect": false
      },
      {
        "id": 22,
        "answerText": "In the Resources section of the CloudFormation template, create resources for each EC2 instance type in the list.",
        "isCorrect": false
      },
      {
        "id": 23,
        "answerText": "In the CloudFormation template, create a separate parameter for each EC2 instance type in the list.",
        "isCorrect": false
      },
      {
        "id": 24,
        "answerText": "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 7,
    "questionId": 7,
    "questionText": "A developer has an application that makes batch requests directly to Amazon DynamoDB by using the BatchGetItem low-level API operation. The responses frequently return values in the UnprocessedKeys element.\nWhich actions should the developer take to increase the resiliency of the application when the batch response includes values in UnprocessedKeys? (Choose two.)",
    "questionHint": null,
    "answers": [
      {
        "id": 25,
        "answerText": "Retry the batch operation immediately.",
        "isCorrect": false
      },
      {
        "id": 26,
        "answerText": "Retry the batch operation with exponential backoff and randomized delay.",
        "isCorrect": true
      },
      {
        "id": 27,
        "answerText": "Update the application to use an AWS software development kit (AWS SDK) to make the requests.",
        "isCorrect": false
      },
      {
        "id": 28,
        "answerText": "Increase the provisioned read capacity of the DynamoDB tables that the operation accesses.",
        "isCorrect": true
      },
      {
        "id": 29,
        "answerText": "Increase the provisioned write capacity of the DynamoDB tables that the operation accesses.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 8,
    "questionId": 8,
    "questionText": "A company is running a custom application on a set of on-premises Linux servers that are accessed using Amazon API Gateway. AWS X-Ray tracing has been enabled on the API test stage.\nHow can a developer enable X-Ray tracing on the on-premises servers with the LEAST amount of configuration?",
    "questionHint": null,
    "answers": [
      {
        "id": 30,
        "answerText": "Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service.",
        "isCorrect": false
      },
      {
        "id": 31,
        "answerText": "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
        "isCorrect": true
      },
      {
        "id": 32,
        "answerText": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTraceSegments API call.",
        "isCorrect": false
      },
      {
        "id": 33,
        "answerText": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTelemetryRecords API call.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 9,
    "questionId": 9,
    "questionText": "A company wants to share information with a third party. The third party has an HTTP API endpoint that the company can use to share the information. The company has the required API key to access the HTTP API.\nThe company needs a way to manage the API key by using code. The integration of the API key with the application code cannot affect application performance.\nWhich solution will meet these requirements MOST securely?",
    "questionHint": null,
    "answers": [
      {
        "id": 34,
        "answerText": "Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "isCorrect": true
      },
      {
        "id": 35,
        "answerText": "Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.",
        "isCorrect": false
      },
      {
        "id": 36,
        "answerText": "Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "isCorrect": false
      },
      {
        "id": 37,
        "answerText": "Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 10,
    "questionId": 10,
    "questionText": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.\nHow should the developer retrieve the variables with the FEWEST application changes?",
    "questionHint": null,
    "answers": [
      {
        "id": 38,
        "answerText": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
        "isCorrect": true
      },
      {
        "id": 39,
        "answerText": "Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.",
        "isCorrect": false
      },
      {
        "id": 40,
        "answerText": "Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.",
        "isCorrect": false
      },
      {
        "id": 41,
        "answerText": "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 11,
    "questionId": 11,
    "questionText": "A developer writes an AWS Lambda function in Java that calls a third-party API by using an API key. The developer must keep the API key encrypted and secure from casual observation. The API key can change over time.\n\nHow can the developer meet these requirements in the MOST operationally efficient manner?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114923\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Hard-coding parameters within any code is not a good practice. Lambda function artifacts are not stored with encryption at rest. While the Java code is compiled and not visible from the Lambda console, the source code is not compiled and still carries the values without encryption. Any changes to parameter values would require code modification, rebuilding, and re-deployment, which would not be operationally efficient. For more information about Lambda function best practices, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">Best practices for working with AWS Lambda functions</a></span>.</p>\n\n<p><strong>B:</strong> Amazon EBS volumes can be mounted by Amazon EC2 instances, but not by Lambda functions. For more information about Amazon EBS volumes, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">Amazon Elastic Block Store (Amazon EBS)</a></span>.</p>\n\n<p><strong>C:</strong> While this solution provides a minor improvement over hard-coding parameters, co-packaging parameter values in a deployment artifact is not a Lambda function best practice. Lambda function artifacts are not stored with encryption at rest. Any changes to parameter values would require property file modification, rebuilding, and re-deployment, which is not operationally efficient. For more information about Lambda function best practices, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\">Best practices for working with AWS Lambda functions</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Parameter Store allows for easy externalization of parameters, such as an API key. The secure string option provides for data security by keeping the value encrypted at rest. Authorized access to the parameter is governed by IAM permissions. Parameter values can be easily changed by authorized principals at any time without requiring a re-deployment of the function, although the function would require intelligence to re-read the parameter values. For more information about Parameter Store, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">AWS Systems Manager Parameter Store</a></span>. For more information about the secure string option, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/security-best-practices.html\">Security best practices for Systems Manager</a></span>. For more information about requiring IAM authorization for parameter access, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-access.html\">Restricting access to Systems Manager parameters using IAM policies</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 42,
        "answerText": "Store the API key directly in the Lambda functionâ€™s program code.",
        "isCorrect": false
      },
      {
        "id": 43,
        "answerText": "Store the API key in a file on an Amazon Elastic Block Store (Amazon EBS) volume. Include code to read the file when the Lambda function starts.",
        "isCorrect": false
      },
      {
        "id": 44,
        "answerText": "Store the API key in a property file that is deployed with the Java archive. Include code to read the property file when the Lambda function starts.",
        "isCorrect": false
      },
      {
        "id": 45,
        "answerText": "Store the API key as a secure string in the AWS Systems Manager Parameter Store. Include code to read the parameter when the Lambda function starts.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 12,
    "questionId": 18115023,
    "questionText": "A company developed a warehouse fulfillment application by using AWS Step Functions. The Step Functions activity workers run on tablets that warehouse employees use. Some long-running activities fail because of problems with individual tablets. These problems include loss of battery power. The application must re-assign interrupted activities to another worker as soon as possible. If an activity fails three times, the state machine should fail.\n\nWhich Step Functions configuration will meet these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18115023\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. You set the Retry field at the individual state level, not at the overall state machine level. States.Timeout is an error name and not a field that you can set. For more information about the Retry field, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\">Error Handling in Step Functions</a></span>. For more information about how to configure a workflow to wait for a task to finish, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-activities.html#activities-wait\">Waiting for an Activity Task to Complete</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. States.TaskFailed is an error name. This error name is not a field that you can set. For more information about error handling, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\">Error Handling in Step Functions</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. The TimeoutSeconds attribute defines the maximum task duration before the task is considered failed. A value of 30 would cause any task that runs longer than 30 seconds to fail, even if there were no problems with the tablet. For more information about the TimeoutSeconds attribute, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html\">Task</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> The HeartbeatSeconds attribute defines the maximum interval that the task will wait for a heartbeat signal. If an activity worker fails to send heartbeats within this interval, the state is failed. A retry policy on the state allows another activity worker to attempt to complete the state. For more information about the HeartbeatSeconds attribute, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-activities.html#activities-wait\">Waiting for an Activity Task to Complete</a></span>. For more information about the Retry field, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\">Error Handling in Step Functions</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 46,
        "answerText": "Set the state machine's States.Timeout attribute to 30 seconds. Set the state machine's Retry.MaxAttempts attribute to 3.",
        "isCorrect": false
      },
      {
        "id": 47,
        "answerText": "Set the task's HeartbeatSeconds attribute to 30. Set the state machine's States.TaskFailed attribute to 3.",
        "isCorrect": false
      },
      {
        "id": 48,
        "answerText": "Set the task's TimeoutSeconds attribute to 30. Set the Retry.MaxAttempts attribute to 3.",
        "isCorrect": false
      },
      {
        "id": 49,
        "answerText": "Set the task's HeartbeatSeconds attribute to 30 seconds. Set the Retry.MaxAttempts attribute to 3.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 13,
    "questionId": 18114943,
    "questionText": "A developer built an application that stores data in an Amazon RDS Multi-AZ DB instance. The database performs reads and writes constantly and is responding slowly. The intensive read requests are received unpredictably several times each hour. The application cannot tolerate reading stale data. The developer must increase the retrieval speed for the intensive read requests.\n\nWhich strategy will meet these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114943\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> An ElastiCache cluster with a write-through strategy will allow for the read requests to be redirected to ElastiCache efficiently. The strategy will allow for the most up-to-date data to be retrieved. For more information about the ElastiCache write-through caching strategy, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">Caching strategies</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. DAX is an in-memory acceleration service that accelerates DynamoDB tables. DAX cannot be used with RDS databases. For more information about DynamoDB read request acceleration, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\">In-Memory Acceleration with DynamoDB Accelerator (DAX)</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. An RDS Multi-AZ standby replica is intended for failover. A standby replica is not accessible for read or write operations. For more information about RDS Multi-AZ DB instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">High availability (Multi-AZ) for Amazon RDS</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. An RDS read replica is replicated asynchronously from the primary RDS database. It is possible that the RDS read replica might not contain the latest data requested by the read requests. For more information about RDS read replicas, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">Working with read replicas</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 50,
        "answerText": "Use an Amazon ElastiCache cluster with a write-through strategy. Configure the application to direct the intensive read operations to ElastiCache.",
        "isCorrect": true
      },
      {
        "id": 51,
        "answerText": "Use an Amazon DynamoDB Accelerator (DAX) cluster with a write-through strategy. Configure the application to direct the intensive read operations to the DAX cluster.",
        "isCorrect": false
      },
      {
        "id": 52,
        "answerText": "Configure the application to direct the intensive read operations to the Multi-AZ standby replica in the second Availability Zone.",
        "isCorrect": false
      },
      {
        "id": 53,
        "answerText": "Add an RDS read replica. Configure the application to direct the intensive read operations to the read replica.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 13,
    "questionId": 18114943,
    "questionText": "A developer built an application that stores data in an Amazon RDS Multi-AZ DB instance. The database performs reads and writes constantly and is responding slowly. The intensive read requests are received unpredictably several times each hour. The application cannot tolerate reading stale data. The developer must increase the retrieval speed for the intensive read requests.\n\nWhich strategy will meet these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114943\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> An ElastiCache cluster with a write-through strategy will allow for the read requests to be redirected to ElastiCache efficiently. The strategy will allow for the most up-to-date data to be retrieved. For more information about the ElastiCache write-through caching strategy, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">Caching strategies</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. DAX is an in-memory acceleration service that accelerates DynamoDB tables. DAX cannot be used with RDS databases. For more information about DynamoDB read request acceleration, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\">In-Memory Acceleration with DynamoDB Accelerator (DAX)</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. An RDS Multi-AZ standby replica is intended for failover. A standby replica is not accessible for read or write operations. For more information about RDS Multi-AZ DB instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">High availability (Multi-AZ) for Amazon RDS</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. An RDS read replica is replicated asynchronously from the primary RDS database. It is possible that the RDS read replica might not contain the latest data requested by the read requests. For more information about RDS read replicas, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">Working with read replicas</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 50,
        "answerText": "Use an Amazon ElastiCache cluster with a write-through strategy. Configure the application to direct the intensive read operations to ElastiCache.",
        "isCorrect": true
      },
      {
        "id": 51,
        "answerText": "Use an Amazon DynamoDB Accelerator (DAX) cluster with a write-through strategy. Configure the application to direct the intensive read operations to the DAX cluster.",
        "isCorrect": false
      },
      {
        "id": 52,
        "answerText": "Configure the application to direct the intensive read operations to the Multi-AZ standby replica in the second Availability Zone.",
        "isCorrect": false
      },
      {
        "id": 53,
        "answerText": "Add an RDS read replica. Configure the application to direct the intensive read operations to the read replica.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 14,
    "questionId": 18114913,
    "questionText": "A company is working on a project to enhance its serverless application development process. The company hosts applications on AWS Lambda. The development team regularly updates the Lambda code and wants to use stable code in production.\n\nWhich combination of steps should the development team take to configure Lambda functions to meet both development and production requirements? (Select TWO.)",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114913\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. Using a Lambda layer, you can access additional code and content. It is not suitable for code updates. For more information about Lambda layers, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\" tabindex=\"-1\">Creating and sharing Lambda layers</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Lambda function versions are designed to manage deployment of functions. They can be used for code changes, without affecting the stable production version of the code. For more information about Lambda function versions, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\" tabindex=\"-1\">Lambda function versions</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. Using a Lambda layer, you can access additional code and content. It is not suitable for code updates. Additionally, a Lambda function alias points to a Lambda function version. For more information about Lambda layers, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\" tabindex=\"-1\">Creating and sharing Lambda layers</a></span>. For more information about Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\" tabindex=\"-1\">Lambda function aliases</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> By creating separate aliases for Production and Development, systems can initiate the correct alias as needed. A Lambda function alias can be used to point to a specific Lambda function version. Using the functionality to update an alias and its linked version, the development team can update the required version as needed. The $LATEST version is the newest published version. For more information about Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\" tabindex=\"-1\">Lambda function aliases</a></span>. For more information about Lambda function versions, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\" tabindex=\"-1\">Lambda function versions</a></span>.</p>\n\n<p><strong>E:</strong> Incorrect. A Lambda function alias can only point to a qualified function ARN. LAMBDA_TASK_ROOT is a variable used by Lambda to record the path to the code. However, it cannot be used in the way the response describes. For more information about Lambda function versions, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\" tabindex=\"-1\">Lambda function versions</a></span>. For more information about Lambda variables, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-config\" tabindex=\"-1\">Configuring environment variables</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 54,
        "answerText": "Create a new Lambda layer every time a new code release needs testing.",
        "isCorrect": false
      },
      {
        "id": 55,
        "answerText": "Create a new Lambda version every time a new code release needs testing.",
        "isCorrect": true
      },
      {
        "id": 56,
        "answerText": "Create two Lambda function aliases. Name one as Production and the other as Development. Point the Production alias to a production-ready Lambda layer Amazon Resource Name (ARN). Point the Development alias to the $LATEST layer ARN.",
        "isCorrect": false
      },
      {
        "id": 57,
        "answerText": "Create two Lambda function aliases. Name one as Production and the other as Development. Point the Production alias to a production-ready qualified Amazon Resource Name (ARN) version. Point the Development alias to the $LATEST version.",
        "isCorrect": true
      },
      {
        "id": 58,
        "answerText": "Create two Lambda function aliases. Name one as Production and the other as Development. Point the Production alias to the production-ready unqualified Amazon Resource Name (ARN) version. Point the Development alias to the variable LAMBDA_TASK_ROOT.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 15,
    "questionId": 18114833,
    "questionText": "A developer builds an inventory application that runs on employee tablets. The tablet application serves as an activity worker for an AWS Step Functions state machine. The application must retrieve a scheduled task, periodically report on task progress, and report task completion or task failure.\n\nWhich Step Functions API actions does the tablet application need to make?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114833\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. You use the StartExecution and StopExecution API actions to start and stop a whole state machine. You do not use these API actions to start and stop a task activity. For more information about the StartExecution API action, see <a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_StartExecution.html\"><span style=\"text-decoration: underline;\">StartExecution</span></a>. For more information about the StopExecution API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_StopExecution.html\">StopExecution</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. You use the GetActivityTask API action to retrieve a scheduled task. For more information about the GetActivityTask API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_GetActivityTask.html\">GetActivityTask</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. You use the GetActivityTask API action to retrieve a scheduled task. The DeleteActivityTask API action is not necessary in this scenario. For more information about the GetActivityTask API action, see <span style=\"text-decoration-line: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_GetActivityTask.html\">GetActivityTask</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> The activity worker must first call the CreateActivity API action to obtain the activity Amazon Resource Name (ARN). The GetActivityTask API action then retrieves a scheduled task. The SendTaskHeartbeat API action periodically reports on task progress. The SendTaskFailure or SendTaskSuccess API actions report success or failure. For more information about the CreateActivity API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_CreateActivity.html\">CreateActivity</a></span>. For more information about the GetActivityTask API action, see <span style=\"text-decoration-line: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_GetActivityTask.html\">GetActivityTask</a></span>. For more information about the SendTaskHeartbeat API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_SendTaskHeartbeat.html\">SendTaskHeartbeat</a></span>. For more information about the SendTaskSuccess API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_SendTaskSuccess.html\">SendTaskSuccess</a></span>. For more information about the SendTaskFailure API action, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/step-functions/latest/apireference/API_SendTaskFailure.html\">SendTaskFailure</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 59,
        "answerText": "StartExecution, GetActivityTask, SendTaskHeartbeat, StopExecution",
        "isCorrect": false
      },
      {
        "id": 60,
        "answerText": "CreateActivity, SendTaskHeartbeat, SendTaskFailure, SendTaskSuccess",
        "isCorrect": false
      },
      {
        "id": 61,
        "answerText": "CreateActivity, SendTaskHeartbeat, DeleteActivity",
        "isCorrect": false
      },
      {
        "id": 62,
        "answerText": "CreateActivity, GetActivityTask, SendTaskHeartbeat, SendTaskFailure, SendTaskSuccess",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 16,
    "questionId": 18114863,
    "questionText": "A company notices performance degradation in one of its production web applications. The application is running on AWS services and uses a microservices architecture. The company suspects that one of these microservices is causing the performance issue.\n\nWhich AWS solution should the company use to identify the service that is contributing to higher application latency?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114863\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> X-Ray creates a map of services used by your application with trace data. You can use the trace data to drill into specific services or issues. This data provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. For more information about X-Ray, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://aws.amazon.com/xray/features/\">AWS X-Ray features</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. CloudTrail event history is used to record API calls for governance, compliance operation, and risk auditing purposes. CloudTrail is not a service that is used for identifying application performance issues. For more information about CloudTrail and its uses, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">What Is AWS CloudTrail?</a></span>. For more information about CloudTrail event history, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html\">Viewing Events with CloudTrail Event History</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. EventBridge delivers a near-real-time stream of system events that describe changes in Amazon Web Services resources. It is not used for identifying application performance issues. For more information about EventBridge, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\">What Is Amazon EventBridge?</a></span>. For more information about EventBridge events, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-events.html\">Amazon EventBridge events</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. Trusted Advisor provides real-time guidance to help provision AWS resources to follow AWS best practices. It can report overall system utilization, but it is not used for identifying application performance issues. For more information about Trusted Advisor, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html\">AWS Trusted Advisor</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 63,
        "answerText": "AWS X-Ray service map",
        "isCorrect": true
      },
      {
        "id": 64,
        "answerText": "AWS CloudTrail event history",
        "isCorrect": false
      },
      {
        "id": 65,
        "answerText": "Amazon EventBridge events",
        "isCorrect": false
      },
      {
        "id": 66,
        "answerText": "AWS Trusted Advisor performance report",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 17,
    "questionId": 18115003,
    "questionText": "A developer tests code running on the developer's laptop. The code is using the AWS SDK for Python (Boto3) to access AWS services. The .aws/credentials file is set up with the user's IAM user name and password. The developer runs the code and receives this error message:\n\nAn error occurred (InvalidAccessKeyId)\n\nWhich action will resolve this error?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18115003\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. AWS software development kits (SDKs) require an access key ID and a secret access key to make programmatic calls. An IAM user name and password are used for AWS Management Console access. Moving the IAM user name and password from the .aws/credentials file to the .aws/config file will not resolve the error.</p>\n\n<p><strong>B:</strong> Incorrect. AWS_PROFILE specifies a collection of settings and credentials applied when using AWS CLI or AWS software development kits (SDKs). This environment variable cannot be used to specify the ARN of a IAM role. For more information about the AWS_PROFILE environment variable, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\">Named profiles for the AWS CLI</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. AWS software development kits (SDKs) require an access key ID and a secret access key to make programmatic calls to AWS. An IAM user name and password are used for AWS Management Console access. The access key ID and the secret access key can be specified by using the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. There are no environment variables available for IAM user name and password. For more information about AWS environment variables, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/sdkref/latest/guide/environment-variables.html\">Supported environment variables</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> AWS software development kits (SDKs) require an access key ID and a secret access key to make programmatic calls to AWS. An IAM user name and password are used for AWS Management Console access. For more information about AWS credentials, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">Programmatic access</a></span>. For more information about config and credentials files for SDKs, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/sdkref/latest/guide/creds-config-files.html\">The shared config and credentials files</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 67,
        "answerText": "Move the IAM user name and password to the .aws/config file.",
        "isCorrect": false
      },
      {
        "id": 68,
        "answerText": "Place the Amazon Resource Name (ARN) of a valid IAM role in the AWS_PROFILE environment variable.",
        "isCorrect": false
      },
      {
        "id": 69,
        "answerText": "Move the user name and password to environment variables.",
        "isCorrect": false
      },
      {
        "id": 70,
        "answerText": "Replace the IAM user name and password with an access key ID and a secret access key.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 18,
    "questionId": 18114973,
    "questionText": "A developer configures AWS CodeDeploy to install an application on Amazon EC2 instances in an Amazon EC2 Auto Scaling group.\n\nWhere should the developer place the appspec.yml file?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114973\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> To deploy an application on EC2 instances, the appspec.yml file must be placed in the root of the directory structure of an application's source code. For more information about CodeDeploy appspec.yml file configuration when using EC2 instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html#appspec-reference-server\">CodeDeploy AppSpec File reference</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. The contents of the appspec.yml file can be directly added into the CodeDeploy console only when an AWS Lambda application deployment is created. This scenario is not using Lambda, so the CodeDeploy console is not a valid option.</p>\n\n<p><strong>C:</strong> Incorrect. The .ebextensions folder is used to perform advanced configuration of an AWS Elastic Beanstalk application. This scenario is not using Elastic Beanstalk, so the .ebextensions folder is not a valid option. For more information about configuring Elastic Beanstalk with the .ebextensions folder, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">Advanced environment customization with configuration files (.ebextensions)</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. The appspec.yml file must be placed in the bundle with the application's source code.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 71,
        "answerText": "In the root of the directory structure of the application's source code",
        "isCorrect": true
      },
      {
        "id": 72,
        "answerText": "Directly into the CodeDeploy console",
        "isCorrect": false
      },
      {
        "id": 73,
        "answerText": "In the .ebextensions folder in the application's source code",
        "isCorrect": false
      },
      {
        "id": 74,
        "answerText": "In the same Amazon S3 bucket as the application source code bundle",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 19,
    "questionId": 18114853,
    "questionText": "A company hosts its web application backend on Amazon Elastic Container Service (Amazon ECS). The application's Amazon ECS tasks run behind an Application Load Balancer (ALB). The application supports three environments: production, testing, and development. The application uses the ALB to route traffic to the correct environment.\n\nThe company has configured three listener rules for the ALB to forward traffic to a different target group based on the port number (Port 80 for production target group, Port 8080 for testing target group, and Port 8081 for development target group).\n\nThe company decides to migrate the application backend to a serverless architecture by using an Amazon API Gateway API backed by AWS Lambda functions. The company plans to use the URI path pattern to access the desired environment instead of the port number. The company has created the Lambda functions for the application backend. Each Lambda function has three aliases (production, testing, and development).\n\nWhich option includes the next steps the company must take to complete the process?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114853\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> To add \"stageVariable\" to the Lambda ARN, you should use the following format: ${stageVariable.stageVariableName}. For more information about API Gateway stage variables, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/amazon-api-gateway-using-stage-variables.html\">Using Amazon API Gateway Stage Variables</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. To add \"stageVariable\" to the Lambda ARN, you do not use the Lambda alias name. You should use the following format: ${stageVariable.stageVariableName}.</p>\n\n<p><strong>C:</strong> Incorrect. You use the Lambda execution role to grant Lambda permission to AWS resources. The resource-based policy allows other services to invoke the Lambda function. For more information about Lambda and AWS Identity and Access Management (IAM), see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html\">AWS Lambda Permissions</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. To add \"stageVariable\" to the Lambda ARN, you do not use the Lambda alias name. You should use the following format: ${stageVariable.stageVariableName}. You use the Lambda execution role to grant Lambda permission to AWS resources. The resource-based policy allows other services to invoke the Lambda function. For more information about Lambda and AWS Identity and Access Management (IAM), see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html\">AWS Lambda Permissions</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 75,
        "answerText": "Create an API Gateway API and configure the routes to use Lambda proxy integration. Target the corresponding Lambda function Amazon Resource Name (ARN) that is concatenated with the expression ${stageVariables.LambdaAlias}. Modify the Lambda resource-based policy by adding the permission lambda:InvokeFunction. Create production, testing, and development stages. Add the LambdaAlias stage variable to the corresponding stage.",
        "isCorrect": true
      },
      {
        "id": 76,
        "answerText": "Create an API Gateway API and configure the routes to use Lambda proxy integration. Target the corresponding Lambda function Amazon Resource Name (ARN) that is concatenated with the name of the Lambda alias. Modify the Lambda resource-based policy by adding the permission lambda:InvokeFunction. Create production, testing, and development stages. Add the LambdaAlias stage variable to the corresponding stage.",
        "isCorrect": false
      },
      {
        "id": 77,
        "answerText": "Create an API Gateway API and configure the routes to use Lambda proxy integration. Target the corresponding Lambda function Amazon Resource Name (ARN) that is concatenated with the expression ${stageVariables.LambdaAlias}. Modify the Lambda execution role by adding the permission apigateway:*. Create production, testing, and development stages. Add the LambdaAlias stage variable to the corresponding stage.",
        "isCorrect": false
      },
      {
        "id": 78,
        "answerText": "Create an API Gateway API and configure the routes with Lambda proxy integration. Target the corresponding Lambda function Amazon Resource Name (ARN) that is concatenated with the name of the Lambda alias. Modify the Lambda execution role by adding the permission apigateway:*. Create production, testing, and development stages. Add the LambdaAlias stage variable to the corresponding stage.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 20,
    "questionId": 18114983,
    "questionText": "A developer needs to query a relational database from an AWS Lambda function. The Lambda function will be called frequently. The developer needs to avoid the latency introduced by the initial JDBC connection to the database in subsequent invocations of the Lambda function.\n\nHow can the developer ensure the database connection is reused by Lambda?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114983\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. Database connection details can be stored as an environment variable. However, the developer still needs to initialize the connection within the code for the connection to be used in queries. For more information about Lambda function environment variables, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">Using AWS Lambda environment variables</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> The Lambda environment can reuse the same database connection for subsequent invocations when defining the database connection details in the code but outside the handler method. For more information about reusing the Lambda execution environment, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html\">AWS Lambda execution environment</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. Database connection details can be stored in Systems Manager Parameter Store. However, initializing the connection within the handler method will result in it re-initializing each subsequent Lambda function invocation. For more information about storing parameters in AWS, see <span style=\"text-decoration-line: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">AWS Systems Manager Parameter Store</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. Initializing the connection within the handler method will result in it re-initializing each subsequent Lambda function invocation.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 79,
        "answerText": "Initialize the database connection outside the Lambda function code as a Lambda function environment variable.",
        "isCorrect": false
      },
      {
        "id": 80,
        "answerText": "Initialize the database connection within the Lambda function code but outside the handler method.",
        "isCorrect": true
      },
      {
        "id": 81,
        "answerText": "Store the database connection string in AWS Systems Manager Parameter Store. Ensure the Lambda function handler initializes the database connection within the handler method.",
        "isCorrect": false
      },
      {
        "id": 82,
        "answerText": "Store the database connection string in the Lambda function code. Ensure the function handler initializes the database connection within the handler method.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 21,
    "questionId": 18114903,
    "questionText": "A company is developing a Python application that submits data to an Amazon DynamoDB table. The company requires client-side encryption of specific data items and end-to-end protection for the encrypted data in transit and at rest.\n\nWhich combination of steps will meet the requirement to encrypt specific data items? (Select TWO.)",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114903\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> When you configure the AWS Database Encryption SDK to use AWS KMS, the AWS Database Encryption SDK uses a KMS key that is always encrypted when the key is used outside of AWS KMS. This cryptographic materials provider returns a unique encryption key and a signing key for every table item. This method of encryption uses a symmetric KMS key. For more information about the Direct KMS Materials Provider that the AWS Database Encryption SDK uses, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/direct-kms-provider.html\">Direct KMS Materials Provider</a></span>. For more information about KMS keys, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">AWS KMS Concepts</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. When you configure the AWS Database Encryption SDK to use AWS KMS, the AWS Database Encryption SDK uses a KMS key that is always encrypted when the key is used outside of AWS KMS. This cryptographic materials provider returns a unique encryption key and a signing key for every table item. This method of encryption would require a symmetric KMS key, not an asymmetric key.</p>\n\n<p><strong>C (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> The AWS Database Encryption SDK provides end-to-end protection for your data in transit and at rest. You can encrypt selected items or attribute values in a table. For more information about DynamoDB client-side and server-side encryption, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/client-server-side.html\">Client-Side and Server-Side Encryption</a></span>. For more information about the AWS Database Encryption SDK, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/what-is-database-encryption-sdk.html\">What Is the AWS Database Encryption SDK?</a></span>. For more information about how the AWS Database Encryption SDK works, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/how-it-works.html\">How the AWS Database Encryption SDK Works</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. You can use this method for server-side encryption at rest. This solution does not meet the requirement for client-side encryption.</p>\n\n<p><strong>E:</strong> Incorrect. You can use this method for server-side encryption at rest. This solution does not meet the requirement for client-side encryption.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 83,
        "answerText": "Generate symmetric encryption keys with AWS Key Management Service (AWS KMS).",
        "isCorrect": true
      },
      {
        "id": 84,
        "answerText": "Generate asymmetric encryption keys with AWS Key Management Service (AWS KMS).",
        "isCorrect": false
      },
      {
        "id": 85,
        "answerText": "Use generated keys with the AWS Database Encryption SDK.",
        "isCorrect": true
      },
      {
        "id": 86,
        "answerText": "Use generated keys to configure DynamoDB table encryption with AWS managed KMS keys.",
        "isCorrect": false
      },
      {
        "id": 87,
        "answerText": "Use generated keys to configure DynamoDB table encryption with AWS owned KMS keys.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 22,
    "questionId": 18114933,
    "questionText": "A developer migrates a web application from an on-premises data center to the AWS Cloud. Authenticated customers use the application from many different clients simultaneously, including laptops, smartphones, and tablets. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group. Upon initial testing, users report that when switching devices, activity from their previous sessions is not saved.\n\nWhich solution will make the session state information persist across devices?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114933\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. The sticky session feature of the Application Load Balancer does not solve the problem across devices. Sticky sessions rely on a cookie that is not going to be consistent across devices. For more information about sticky sessions, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html\">Sticky Sessions for your Application Load Balancer</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> ElastiCache for Redis is a fast in-memory data store that provides sub-millisecond latency to power internet-scale applications in real time. The data will not be stored on the instance itself. This choice is ideal for ensuring that the session state information persists across devices. For more information about Well-Architected Framework principles, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/design-interactions-in-a-distributed-system-to-mitigate-or-withstand-failures.html\">Design interactions in a distributed system to mitigate or withstand failures</a></span>. For more information about using ElastiCache for Redis for session states, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">Getting Started tutorial, Building a fast session store for your online applications</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. A local file could store session state information persistently over time, but this does not solve the problem presented. If session state information is stored locally on the instance when a request is sent to another node behind the Application Load Balancer, the other node does not have access to the session state information.</p>\n\n<p><strong>D:</strong> Incorrect. Systems Manager State Manager is not for use of session state information management. Systems Manager State Manager is used to manage the state of an instance itself, such as specific instance configurations or software installations. For more information about Systems Manager State Manager, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-state.html\">AWS Systems Manager State Manager</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 88,
        "answerText": "Implement sticky sessions at the Application Load Balancer.",
        "isCorrect": false
      },
      {
        "id": 89,
        "answerText": "Store session state information in an Amazon ElastiCache for Redis cluster.",
        "isCorrect": true
      },
      {
        "id": 90,
        "answerText": "Implement session state information storage in a local file on the webserver.",
        "isCorrect": false
      },
      {
        "id": 91,
        "answerText": "Store session state information in AWS Systems Manager State Manager.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 23,
    "questionId": 18114883,
    "questionText": "A developer has written several custom applications that read and write to the same Amazon DynamoDB table. Each time the data in the DynamoDB table is modified, this change should be sent to an external API.\n\nWhich combination of steps should the developer perform to accomplish this task? (Select TWO.)",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114883\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> You can enable DynamoDB Streams on a table to create an event that invokes an AWS Lambda function. For more information about how to use DynamoDB Streams, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\" tabindex=\"-1\">DynamoDB Streams and AWS Lambda Triggers</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. EventBridge is used to connect applications with a variety of data. It would not be able to detect updates to a DynamoDB table. For more information about EventBridge, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\" tabindex=\"-1\">What Is Amazon EventBridge?</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. A trigger cannot be enabled on a DynamoDB table. To create a trigger, a DynamoDB stream should be enabled on the specific DynamoDB table. For more information about how to use DynamoDB Streams to create an event that invokes an AWS Lambda function, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\" tabindex=\"-1\">DynamoDB Streams and AWS Lambda Triggers</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. You cannot use a DynamoDB stream to deliver data to an SNS topic. For more information about how to capture changes to DynamoDB tables, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/streamsmain.html\" tabindex=\"-1\" target=\"_blank\">Change Data Capture with Amazon DynamoDB</a></span>. For more information about how Amazon SNS works, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\" tabindex=\"-1\" target=\"_blank\">What is Amazon SNS?</a></span>. For more information about how to create a topic in Amazon SNS, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html\" tabindex=\"-1\" target=\"_blank\">Creating an Amazon SNS topic</a></span>.</p>\n\n<p><strong>E (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> With DynamoDB Streams, you can trigger a Lambda function to perform additional tasks. For example, an additional task could be calling an external API each time a DynamoDB table is updated. For more information about how to use DynamoDB Streams, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\" tabindex=\"-1\">DynamoDB Streams and AWS Lambda Triggers</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 92,
        "answerText": "Enable DynamoDB Streams on the table.",
        "isCorrect": true
      },
      {
        "id": 93,
        "answerText": "Configure an event in Amazon EventBridge that publishes the change to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) data stream.",
        "isCorrect": false
      },
      {
        "id": 94,
        "answerText": "Create a trigger in the DynamoDB table to publish the change to an Amazon Kinesis data stream.",
        "isCorrect": false
      },
      {
        "id": 95,
        "answerText": "Deliver the stream to an Amazon Simple Notification Service (Amazon SNS) topic and subscribe the API to the topic.",
        "isCorrect": false
      },
      {
        "id": 96,
        "answerText": "Configure an AWS Lambda function to be triggered by the DynamoDB stream and call the external API.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 24,
    "questionId": 18115013,
    "questionText": "A developer builds an application that uses the AWS SDK for Python (Boto3) to query an Amazon DynamoDB table. When the application is tested on an Amazon EC2 instance, the application returns this error message:\n\n\"An error occurred (AccessDenied) when calling the operation\"\n\nThe EC2 instance is associated with an existing IAM role named myRole.\n\nWhich set of actions would resolve this error?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18115013\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. An EC2 instance, or any AWS principal, service, or resource, can only assume one role at a time. For more information about associating IAM roles with EC2 instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">IAM roles for EC2</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> \"AccessDenied\" indicates an authorization error related to permissions. Multiple policies can be attached to a single IAM role. For more information about authorization in the AWS Cloud, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html#intro-structure-authorization\">Authorization</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. \"AccessDenied\" indicates an authorization error related to permissions, not an authentication problem caused by credentials. The API call is already authenticated. Accessing or modifying the location of AWS credentials will have no effect. The EC2 instance profile has already performed the assume-role call because the role is already associated with the EC2 instance. For more information about associating IAM roles with EC2 instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">IAM roles for EC2</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. \"AccessDenied\" indicates an authorization error related to permissions, not an authentication problem caused by credentials. The API call is already authenticated. Accessing or modifying the location of AWS credentials will have no effect. For more information about associating IAM roles with EC2 instances, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">IAM roles for EC2</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 97,
        "answerText": "Create a new IAM role with the necessary Amazon DynamoDB permissions. Attach this IAM role as an additional role on the EC2 instance.",
        "isCorrect": false
      },
      {
        "id": 98,
        "answerText": "Create a new IAM policy with the necessary Amazon DynamoDB permissions. Attach this policy to the myRole IAM role.",
        "isCorrect": true
      },
      {
        "id": 99,
        "answerText": "Run the aws sts assume-role command by using the myRole Amazon Resource Name (ARN). Obtain the access key ID and the secret access key from the output. Run the aws configure command to store these values on the EC2 instance.",
        "isCorrect": false
      },
      {
        "id": 100,
        "answerText": "Query http://169.254.169.254/latest/meta-data/iam/security-credentials/myRole to obtain the access key ID and the secret access key. Run the aws configure command to store these values on the EC2 instance.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 25,
    "questionId": 18114953,
    "questionText": "An ecommerce company deploys more than 20 services behind Amazon API Gateway. The interaction between services is complex. Each service can potentially call several others, making performance issues and errors difficult to identify. Some individual API calls have experienced slow response times. The development team needs to quickly identify the underlying causes of the slowdowns.\n\nWhich approach would MOST quickly identify the underlying cause of performance issues?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114953\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. It is time consuming to examine and compare metric values of various services. The number of CPU or network calls a service makes indicates relative work load. The number of these CPU or network calls does not indicate poor performance. X-Ray is mainly used to study a service's interaction time with other services. It has limited utility to diagnose an individual service's performance. However, X-Ray is excellent at quickly finding poorly performing services within a web of interaction. CloudWatch metrics and logs can be used to study the targeted services.</p>\n\n<p><strong>B:</strong> Incorrect. Not all services send logs to CloudWatch Logs. It is time consuming to examine a large number of different log files. Log output is application-specific and may not provide any insight into performance issues. CloudTrail records API calls, not application-level activity. For more information about CloudTrail, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">What is AWS CloudTrail?</a></span>.</p>\n\n<p><strong>C (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Unlike metrics or logs, X-Ray can help users quickly identify services by their relative response times. X-Ray can identify a poorly performing service from within a web of interacting services. Once identified, CloudWatch provides the context, including the logs and metrics necessary to study specific issues. For more information about CloudWatch metrics, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html\">Using Amazon CloudWatch metrics</a></span>. For more information about CloudWatch Logs, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">What is Amazon CloudWatch Logs?</a></span>. For more information about X-Ray, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\">What is AWS X-Ray?</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. CloudTrail records API calls. CloudTrail is not an application log aggregator, such as CloudWatch Logs. Any API calls that individual services make would not be likely to affect application performance. Also, X-Ray is mainly used to study a service's interaction time with other services. It has limited utility to diagnose an individual service's performance. For more information about CloudTrail, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">What is AWS CloudTrail?</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 101,
        "answerText": "Use Amazon CloudWatch metrics to find the service invocations with slow response times. Configure and use AWS X-Ray to examine these services to discover their performance issues.",
        "isCorrect": false
      },
      {
        "id": 102,
        "answerText": "Use AWS CloudWatch Logs to find the service invocations with slow response times. Use AWS CloudTrail to examine these services to discover their performance issues.",
        "isCorrect": false
      },
      {
        "id": 103,
        "answerText": "Configure and use AWS X-Ray to find the service invocations with slow response times. Use Amazon CloudWatch metrics and logs to examine these services to discover their performance issues.",
        "isCorrect": true
      },
      {
        "id": 104,
        "answerText": "Use AWS CloudTrail to find the service invocations with slow response times. Configure and use AWS X-Ray to examine these services to discover their performance issues.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 26,
    "questionId": 18114843,
    "questionText": "A company uses an AWS Lambda function to call a third-party REST endpoint. Personally identifiable information (PII) is exposed upon a successful request. The third party that manages the REST endpoint requires the company to change the API key that the company uses to invoke the endpoint every 4 months.\n\nThe company retrieves the API key by calling an endpoint that the third party owns. The endpoint uses basic authentication (username and password). The new API key is available and active one month prior to the inactivation of the old API key. When the company retrieves the new API key, the company needs to store the key for use in future invocations of the REST endpoint. The company needs a secure solution that eliminates downtime while the company sets up the new API key.\n\nWhich solution will meet these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114843\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. Although SecureStrings are available in Parameter Store, you should use Secrets Manager to implement password rotation lifecycles. For more information about Parameter Store, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">AWS Systems Manager Parameter Store</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> API key rotation (including third-party key rotation with a Lambda function) is available in Secrets Manager. For more information about how to rotate third-party secrets, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_how.html\">How Rotation Works</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. Although it is possible to store the API key in DynamoDB, this solution is not the most secure option. Anyone with access to the DynamoDB table would have access to the key. For more information about security on DynamoDB, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-protection.html\">Data Protection in DynamoDB</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. The best practice is to use Secrets Manager instead of Lambda environment variables. For more information about environment variables, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">Using AWS Lambda Environment Variables</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 105,
        "answerText": "Store the API key in Parameter Store, a capability of AWS Systems Manager, as a SecureString. Configure rotation to obtain the new API key from the third party and to update the parameter value.",
        "isCorrect": false
      },
      {
        "id": 106,
        "answerText": "Store the API key in AWS Secrets Manager. Create a Lambda function to obtain the new API key from the third party. Configure rotation in Secrets Manager to use the Lambda function to obtain a new API key. Store the new API key in Secrets Manager. Configure rotation to occur every 4 months.",
        "isCorrect": true
      },
      {
        "id": 107,
        "answerText": "Store the API key in an Amazon DynamoDB table. Create a Lambda function to retrieve the new API key from the third party and to update the value in DynamoDB. Use an Amazon EventBridge scheduled rule to invoke the Lambda function every 4 months.",
        "isCorrect": false
      },
      {
        "id": 108,
        "answerText": "Store the API key as a Lambda environment variable. Retrieve the new API key from the third party by using open source software. Manually update the Lambda environment variable.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 27,
    "questionId": 18114893,
    "questionText": "A developer wants to monitor invocations of an AWS Lambda function by using Amazon CloudWatch Logs. The developer added a number of print statements to the function code that write the logging information to the stdout stream. After running the function, the developer does not see any log data being generated.\n\nWhy does the log data NOT appear in the CloudWatch logs?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114893\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. To output logs from your function code, you can use the print method or any logging library that writes to stdout or stderr. For more information about Lambda function logging, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/python-logging.html\">AWS Lambda function logging in Python</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. Lambda automatically monitors Lambda functions and reports metrics through CloudWatch. Lambda automatically tracks the number of requests, the invocation duration per request, and the number of requests that result in an error. For more information about troubleshooting Lambda applications, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting.html\">Monitoring and troubleshooting Lambda applications</a></span>.</p>\n\n<p><strong>C (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> The function needs permission to call CloudWatch Logs. Update the execution role to grant the permission. You can use the managed policy of AWSLambdaBasicExecutionRole. For more information about troubleshooting execution issues in Lambda, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting-execution.html\">Troubleshoot execution issues in Lambda</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. Lambda automatically stores logs generated by your code through CloudWatch Logs. You can view logs for Lambda functions by using the Lambda console, the CloudWatch console, the AWS CLI, or the CloudWatch API. You can transfer the logs from this CloudWatch log to an S3 bucket. For more information about monitoring Lambda applications, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting.html\">Monitoring and troubleshooting Lambda applications</a></span>. For more information about exporting CloudWatch Logs data from CloudWatch to Amazon S3, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">Exporting log data to Amazon S3</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 109,
        "answerText": "The log data is not written to the stderr stream.",
        "isCorrect": false
      },
      {
        "id": 110,
        "answerText": "Lambda function logging is not automatically enabled.",
        "isCorrect": false
      },
      {
        "id": 111,
        "answerText": "The execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs.",
        "isCorrect": true
      },
      {
        "id": 112,
        "answerText": "The Lambda function outputs the logs to an Amazon S3 bucket.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 28,
    "questionId": 18114993,
    "questionText": "A company is developing an image processing application. When an image is uploaded to an Amazon S3 bucket, a number of independent and separate services must be invoked to process the image. The services do not have to be available immediately, but they must process every image.\n\nWhich application design satisfies these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114993\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. After a consumer retrieves and processes a message from a queue, it deletes the message in the queue. The result is that only one service receives the message. For more information about Amazon SQS architecture, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">Basic Amazon SQS architecture</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. An Amazon SQS delivery policy defines how Amazon SNS retries the delivery of messages when a delivery error occurs. Amazon SNS stops retrying the delivery and discards the message when a delivery policy is exhausted. If one of the services is temporarily unavailable, it would not receive the message. For more information about Amazon SNS message delivery retries, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html\">Amazon SNS message delivery retries</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. An Amazon SQS queue is not a supported event source for Amazon SNS. For more information about Amazon SNS event sources, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources.html\">Amazon SNS event sources</a></span>.</p>\n\n<p><strong>D (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Each service can subscribe to an individual Amazon SQS queue, which receives an event notification from the Amazon SNS topic. This is a fanout architectural implementation. For more information about Amazon SNS fanout architecture, see <span style=\"text-decoration: underline;\"><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html\">Common Amazon SNS scenarios</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 113,
        "answerText": "Configure an Amazon S3 event notification that publishes to an Amazon SQS queue. Each service pulls the message from the same queue.",
        "isCorrect": false
      },
      {
        "id": 114,
        "answerText": "Configure an Amazon S3 event notification that publishes to an Amazon SNS topic. Each service subscribes to the same topic.",
        "isCorrect": false
      },
      {
        "id": 115,
        "answerText": "Configure an Amazon S3 event notification that publishes to an Amazon SQS queue. Subscribe a separate Amazon SNS topic for each service to an Amazon SQS queue.",
        "isCorrect": false
      },
      {
        "id": 116,
        "answerText": "Configure an Amazon S3 event notification that publishes to an Amazon SNS topic. Subscribe a separate Amazon SQS queue for each service to the Amazon SNS topic.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 29,
    "questionId": 18114873,
    "questionText": "Each time a developer publishes a new version of an AWS Lambda function, all the dependent event source mappings need to be updated with the reference to the new versionâ€™s Amazon Resource Name (ARN). These updates are time consuming and error-prone.\n\nWhich combination of actions should the developer take to avoid performing these updates when publishing a new Lambda version? (Select TWO.)",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114873\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. A Lambda event source mapping can either point to the version ARN, or an alias ARN, but not to the layer ARN. For more information about managing Lambda function versions by using an ARN, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\">Lambda function versions</a></span>. For more information about creating and using Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">Lambda function aliases</a></span>. For more information about Lambda layers, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">Creating and sharing Lambda layers</a></span>.</p>\n\n<p><strong>B (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> A Lambda alias is a pointer to a specific Lambda function version. For more information about creating and using Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">Lambda function aliases</a></span>.</p>\n\n<p><strong>C:</strong> Incorrect. This solution does not address your problem. Every alias has its own unique ARN. Therefore, you would still need to update the event source mapping with the new ARN when a new version is published. For more information about creating and using Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">Lambda function aliases</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. A Lambda alias cannot point to another Lambda alias, only to a Lambda function version. For more information about Lambda function aliases, including their creation and management, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html#using-aliases\">Using aliases</a></span>.</p>\n\n<p><strong>E (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Instead of using ARNs for the Lambda function in event source mappings, you can use an alias ARN. You do not need to update your event source mappings when you promote a new version or roll back to a previous version. For more information about creating and using Lambda function aliases, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">Lambda function aliases</a></span>.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 117,
        "answerText": "Update event source mappings with the ARN of the Lambda layer.",
        "isCorrect": false
      },
      {
        "id": 118,
        "answerText": "Point a Lambda alias to a new version of the Lambda function.",
        "isCorrect": true
      },
      {
        "id": 119,
        "answerText": "Create a Lambda alias for each published version of the Lambda function.",
        "isCorrect": false
      },
      {
        "id": 120,
        "answerText": "Point a Lambda alias to a new Lambda function alias.",
        "isCorrect": false
      },
      {
        "id": 121,
        "answerText": "Update the event source mappings with the Lambda alias ARN.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 30,
    "questionId": 18114963,
    "questionText": "A company has an inventory system that receives sporadic inventory updates from a fulfillment system in the form of large JSON files. While many files can be sent in a short time period, days can pass when no files are sent. The company wants to process these files as soon as they arrive.\n\nWhich solution will meet these requirements?",
    "questionHint": "<section data-v-ad49d49a=\"\" data-v-358122a4=\"\" data-id=\"18114963\" class=\"split-main question-content question-container\" data-v-36ec6550=\"\"><div data-v-ad49d49a=\"\" class=\"content-left\"><div data-v-ad49d49a=\"\" class=\"content-left-inner answer-content-wrapper\"><div data-v-ad49d49a=\"\" class=\"content-container\"><div id=\"content-question-start\"><span class=\"sr-only\">Question</span><div class=\"aws-question-content\">\n<p><strong>A:</strong> Incorrect. Lambda functions can connect to Amazon EFS file systems. However, Amazon EFS file system changes cannot invoke Lambda functions. For more information about using Lambda functions with Amazon EFS, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">Using Amazon EFS with Lambda</a></span>.</p>\n\n<p><strong>B:</strong> Incorrect. Lambda functions can run on a schedule and connect to Amazon EFS file systems. However, a scheduled/polling process would not process the files as soon as they arrive. For more information about using Lambda functions with Amazon EFS, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">Using Amazon EFS with Lambda</a></span>.</p>\n\n<p><strong>C (Correct):</strong> <span style=\"color: green; font-weight: bold;\">Correct.</span> Lambda functions can be invoked from S3 events and can read from S3 buckets. An event-based invocation would allow for the S3 objects to be processed as soon as they arrive. For more information about invoking a Lambda function from S3, see <span style=\"text-decoration: underline;\"><a target=\"_blank\" href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html\">Tutorial: Using an Amazon S3 trigger to invoke a Lambda function</a></span>.</p>\n\n<p><strong>D:</strong> Incorrect. Lambda functions can run on a schedule and read from S3 buckets. However, a scheduled/polling process would not process the S3 objects as soon as they arrive.</p>\n</div></div></div></div></div></div></section>",
    "answers": [
      {
        "id": 122,
        "answerText": "Send the JSON files to Amazon Elastic File System (Amazon EFS). Configure an AWS Lambda function with an Amazon EFS event source to process the files.",
        "isCorrect": false
      },
      {
        "id": 123,
        "answerText": "Send the JSON files to Amazon Elastic File System (Amazon EFS). Schedule an AWS Lambda function to process the files once each hour.",
        "isCorrect": false
      },
      {
        "id": 124,
        "answerText": "Send the JSON files to an Amazon S3 bucket. Configure an AWS Lambda function with an S3 event source to process the S3 objects.",
        "isCorrect": true
      },
      {
        "id": 125,
        "answerText": "Send the JSON files to an Amazon S3 bucket. Schedule an AWS Lambda function to process the S3 objects once each hour.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 31,
    "questionId": null,
    "questionText": "Your team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and you would like to prevent other unauthorized domains from accessing your API. Which of the following actions should you take?",
    "questionHint": null,
    "answers": [
      {
        "id": 126,
        "answerText": "Restrict access by using CORS",
        "isCorrect": true
      },
      {
        "id": 127,
        "answerText": "Use Account-level throttling",
        "isCorrect": false
      },
      {
        "id": 128,
        "answerText": "Use Mapping Templates",
        "isCorrect": false
      },
      {
        "id": 129,
        "answerText": "Assign a Security Group to your API Gateway",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 32,
    "questionId": null,
    "questionText": "A development team has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The team is, however, unable to connect to the service running on the Amazon EC2 instance. As a developer associate, which of the following will you recommend to fix this issue?",
    "questionHint": null,
    "answers": [
      {
        "id": 130,
        "answerText": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": true
      },
      {
        "id": 131,
        "answerText": "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": false
      },
      {
        "id": 132,
        "answerText": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs",
        "isCorrect": false
      },
      {
        "id": 133,
        "answerText": "Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic behavior",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 33,
    "questionId": null,
    "questionText": "A developer has created a new Application Load Balancer but has not registered any targets with the target groups. Which of the following errors would be generated by the Load Balancer?",
    "questionHint": null,
    "answers": [
      {
        "id": 134,
        "answerText": "HTTP 503 Service Unavailable",
        "isCorrect": true
      },
      {
        "id": 135,
        "answerText": "HTTP 502 Bad Gateway",
        "isCorrect": false
      },
      {
        "id": 136,
        "answerText": "HTTP 504 Gateway Timeout",
        "isCorrect": false
      },
      {
        "id": 137,
        "answerText": "HTTP 500 Internal Server Error",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 34,
    "questionId": null,
    "questionText": "A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing. Which of the following methods do you recommend?",
    "questionHint": null,
    "answers": [
      {
        "id": 138,
        "answerText": "Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment.",
        "isCorrect": true
      },
      {
        "id": 139,
        "answerText": "You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment.",
        "isCorrect": false
      },
      {
        "id": 140,
        "answerText": "Use only one Beanstalk environment and perform configuration changes using an Ansible script.",
        "isCorrect": false
      },
      {
        "id": 141,
        "answerText": "Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in ebextensions/ to know how to handle the traffic coming from the ALB.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 35,
    "questionId": null,
    "questionText": "What section of a CloudFormation template canâ€™t be associated with a condition?",
    "questionHint": "In AWS CloudFormation, the Outputs section cannot have a Condition directly associated with it. While most top-level sections in a CloudFormation template (such as Resources, Mappings, Parameters, and Conditions) support conditional logic, the Outputs section does not support the Condition attribute directly at the top level of an output declaration.\n\nWorkaround: To conditionally define an output, you must use an intrinsic function like Fn::If within the Value field of the output.\n\nExample:\n```yaml\nOutputs:\n  MyOutput:\n    Description: \"A conditional output\"\n    Value: !If \n      - MyCondition\n      - \"This output is visible\"\n      - !Ref \"AWS::NoValue\"\n```\nHere, `!Ref \"AWS::NoValue\"` acts as a way to omit the output if the condition is false.",
    "answers": [
      {
        "id": 142,
        "answerText": "Outputs",
        "isCorrect": true
      },
      {
        "id": 143,
        "answerText": "Resources",
        "isCorrect": false
      },
      {
        "id": 144,
        "answerText": "Mappings",
        "isCorrect": false
      },
      {
        "id": 145,
        "answerText": "Parameters",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 36,
    "questionId": null,
    "questionText": "A firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The developer team has been told to use the user data feature of EC2 instances. Which of the following are true about the user data EC2 configuration? (Select two)",
    "questionHint": "By default, scripts entered as user data are executed with root user privileges.\nâœ… True â€” EC2 user data scripts are executed as the root user by default, so they can perform administrative tasks such as installing software or modifying system configuration.\n\nBy default, user data runs only during the boot cycle when you first launch an instance.\nâœ… True â€” By default, user data runs once, when the instance is first launched (i.e., the first boot). It does not run again on instance reboots unless explicitly configured to do so.\n\nBy default, user data is executed every time an EC2 instance is re-started.\nâŒ False â€” User data is not re-executed on every reboot unless you modify the script or instance settings to allow that (e.g., by using a cloud-init configuration to re-run it).\n\nWhen an instance is running, you can update user data by using root user credentials.\nâŒ False â€” You cannot directly update user data on a running instance through root credentials alone. To update user data, you must stop the instance and modify it through the EC2 management console or AWS CLI/SDK.\n\nBy default, scripts entered as user data do not have root user privileges for executing.\nâŒ False â€” As stated earlier, they do have root privileges by default.",
    "answers": [
      {
        "id": 146,
        "answerText": "By default, scripts entered as user data are executed with root user privileges",
        "isCorrect": true
      },
      {
        "id": 147,
        "answerText": "By default, user data runs only during the boot cycle when you first launch an instance",
        "isCorrect": true
      },
      {
        "id": 148,
        "answerText": "By default, user data is executed every time an EC2 instance is re-started",
        "isCorrect": false
      },
      {
        "id": 149,
        "answerText": "When an instance is running, you can update user data by using root user credentials",
        "isCorrect": false
      },
      {
        "id": 150,
        "answerText": "By default, scripts entered as user data do not have root user privileges for executing",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 37,
    "questionId": null,
    "questionText": "How can you retrieve instance-specific information such as the instance ID and security groups for an Amazon EC2 instance?",
    "questionHint": "To retrieve instance-specific information such as the instance ID and security groups, you can use the EC2 instance metadata service, which is available at the special IP address 169.254.169.254.\n\n**Example:**\n\n**Instance ID:**\n```bash\ncurl http://169.254.169.254/latest/meta-data/instance-id\n```\n\n**Security Groups:**\n```bash\ncurl http://169.254.169.254/latest/meta-data/security-groups\n```\n\n**Why the other options are incorrect:**\n\n- **Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call:**\n  While this is valid for calling AWS APIs (like `describe-instances`), itâ€™s not needed to get local instance metadata such as security groups or instance ID.\n\n- **Query the user data at http://169.254.169.254/latest/user-data:**\n  This returns user-provided data, not metadata like instance ID or security group.\n\n- **Query the user data at http://254.169.254.169/latest/meta-data:**\n  This is an incorrect IP address. The correct metadata service IP is `169.254.169.254`.",
    "answers": [
      {
        "id": 151,
        "answerText": "Query the metadata at http://169.254.169.254/latest/meta-data",
        "isCorrect": true
      },
      {
        "id": 152,
        "answerText": "Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call",
        "isCorrect": false
      },
      {
        "id": 153,
        "answerText": "Query the user data at http://169.254.169.254/latest/user-data",
        "isCorrect": false
      },
      {
        "id": 154,
        "answerText": "Query the user data at http://254.169.254.169/latest/meta-data",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 38,
    "questionId": null,
    "questionText": "A Developer is configuring an Amazon EC2 Auto Scaling group to scale dynamically. Which metric below is NOT part of Target Tracking Scaling Policy?",
    "questionHint": "Target Tracking Scaling Policy in Amazon EC2 Auto Scaling allows you to automatically adjust the number of EC2 instances based on a specific metric target, such as average CPU utilization or request count per target.\n\n**Metrics commonly used in Target Tracking policies:**\n\n- **ASGAverageCPUUtilization** â€“ Yes, this is commonly used.\n- **ASGAverageNetworkOut** â€“ Yes, this is supported.\n- **ALBRequestCountPerTarget** â€“ Yes, this is also supported and tracks requests to a target in an Application Load Balancer.\n\n**ApproximateNumberOfMessagesVisible:**\n- This is a metric for Amazon SQS (Simple Queue Service).\n- It shows the number of messages waiting in the queue.\n- This metric is not directly supported by EC2 Auto Scaling Target Tracking Policies. You would need to use a custom metric or step scaling policy if you want to scale based on this.",
    "answers": [
      {
        "id": 155,
        "answerText": "ApproximateNumberOfMessagesVisible",
        "isCorrect": true
      },
      {
        "id": 156,
        "answerText": "ASGAverageCPUUtilization",
        "isCorrect": false
      },
      {
        "id": 157,
        "answerText": "ASGAverageNetworkOut",
        "isCorrect": false
      },
      {
        "id": 158,
        "answerText": "ALBRequestCountPerTarget",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 39,
    "questionId": null,
    "questionText": "A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world. Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?",
    "questionHint": "CloudFront signed URLs allow the company to:\n\n- Share access only with authorized individuals\n- Set an expiration time to limit how long the file is accessible\n- Ensure secure, direct delivery of the file globally via CloudFront\n\n**Why the other options are not optimal:**\n\n- **Signed Cookies:** Better suited when a user needs access to multiple files behind CloudFront. Adds unnecessary complexity for just one file.\n- **AWS WAF:** Useful for filtering malicious traffic, not for controlling download access to a specific document.\n- **Field-Level Encryption:** Used to encrypt form fields, not for restricting access to or encrypting downloadable content like PDF files.",
    "answers": [
      {
        "id": 159,
        "answerText": "Use CloudFront signed URL feature to control access to the file",
        "isCorrect": true
      },
      {
        "id": 160,
        "answerText": "Use CloudFront signed cookies feature to control access to the file",
        "isCorrect": false
      },
      {
        "id": 161,
        "answerText": "Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront",
        "isCorrect": false
      },
      {
        "id": 162,
        "answerText": "Using CloudFront's Field-Level Encryption to help protect sensitive data",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 40,
    "questionId": null,
    "questionText": "A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications. Which of the following types of APIs supported by the Amazon API Gateway is the right fit?",
    "questionHint": "For real-time alerts and notifications without implementing complex polling, WebSocket APIs are the best choice. They enable full-duplex communication between client and server, allowing the backend to push messages to clients instantly.\n\n**Why the other options are not suitable:**\n\n- **REST API:** Typically request-response. Clients poll the server to get updates, which is not ideal for real-time notifications.\n- **HTTP API:** Similar to REST but more optimized and cheaper; still mainly request-response.\n- **WebSocket API:** Enables full-duplex communication, making it ideal for real-time use cases like alerts and notifications.",
    "answers": [
      {
        "id": 163,
        "answerText": "REST API",
        "isCorrect": false
      },
      {
        "id": 164,
        "answerText": "HTTP API",
        "isCorrect": false
      },
      {
        "id": 165,
        "answerText": "WebSocket API",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 41,
    "questionId": null,
    "questionText": "How can you delete all messages from an Amazon SQS queue?",
    "questionHint": "To delete all messages from an Amazon SQS queue efficiently, use the **PurgeQueue** action. It deletes all messages in the queue asynchronously.\n\n**Details:**\n- You can call PurgeQueue only once every 60 seconds for each queue.\n- The purge is asynchronous â€” messages might still be delivered for a short time after the request.\n\n**Alternative Approach (if PurgeQueue is not an option):**\n- Poll messages using **ReceiveMessage**.\n- Delete each message individually using **DeleteMessage**. This approach is slower and less efficient but provides finer control.",
    "answers": [
      {
        "id": 166,
        "answerText": "Use PurgeQueue to delete all messages in the queue",
        "isCorrect": true
      },
      {
        "id": 167,
        "answerText": "Use ReceiveMessage to poll messages and DeleteMessage to delete them one by one",
        "isCorrect": false
      },
      {
        "id": 168,
        "answerText": "Manually delete each message using the AWS Management Console",
        "isCorrect": false
      },
      {
        "id": 169,
        "answerText": "Use DeleteQueue to delete the queue and all its messages",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 42,
    "questionId": null,
    "questionText": "An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
    "questionHint": "AWS provides VPC endpoints to privately connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\n\n**Types of VPC Endpoints:**\n\n- **Gateway endpoints:** Used for S3 and DynamoDB.\n- **Interface endpoints (powered by AWS PrivateLink):** Used for most other AWS services.\n\n**Why this is the correct choice:**\n\n- **S3:** The best and most cost-effective option for private access is a Gateway endpoint.\n- **DynamoDB:** While you can use a gateway endpoint, using an interface endpoint provides more control and security (such as security groups and fine-grained access control with PrivateLink).\n\n**Why the other options are incorrect:**\n\n- Creating separate gateway endpoints for both S3 and DynamoDB is acceptable, but less common for DynamoDB in use cases requiring high security or private DNS.\n- Creating interface endpoints for S3 is invalid â€” S3 does not support interface endpoints.\n- Creating an API endpoint for S3 is not a valid AWS concept.",
    "answers": [
      {
        "id": 170,
        "answerText": "Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC.",
        "isCorrect": false
      },
      {
        "id": 171,
        "answerText": "Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address.",
        "isCorrect": true
      },
      {
        "id": 172,
        "answerText": "Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address.",
        "isCorrect": false
      },
      {
        "id": 173,
        "answerText": "Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 43,
    "questionId": null,
    "questionText": "A company has built its technology stack on AWS serverless architecture for managing all its business functions. To expedite development for a new business requirement, the company is looking at using pre-built serverless applications. Which AWS service represents the easiest solution to address this use-case?",
    "questionHint": "The easiest solution for using pre-built serverless applications in AWS is **AWS Serverless Application Repository (SAR)**.\n\n**Explanation:**\n\n- **AWS Serverless Application Repository (SAR):**\n  - Purpose: Allows developers to discover, deploy, and share serverless applications built by AWS, its partners, and the developer community.\n  - Ease of Use: Deploy pre-built serverless apps with just a few clicks or using AWS SAM/CLI.\n  - Integration: Built using AWS Lambda and integrates with other AWS services like API Gateway, DynamoDB, S3, etc.\n  - Customization: Customize parameters before deployment to suit your needs.\n\n**Why Not Other Services?**\n\n- **AWS Lambda:** Used to write functions, but doesnâ€™t provide pre-built applications.\n- **AWS CloudFormation:** Manages infrastructure as code but doesnâ€™t offer pre-built serverless apps directly.\n- **AWS Marketplace:** Offers software solutions, but SAR is specialized for serverless apps.\n- **Amazon API Gateway:** Supports serverless APIs, but does not provide pre-built apps.",
    "answers": [
      {
        "id": 174,
        "answerText": "AWS Serverless Application Repository",
        "isCorrect": true
      },
      {
        "id": 175,
        "answerText": "AWS Lambda",
        "isCorrect": false
      },
      {
        "id": 176,
        "answerText": "AWS CloudFormation",
        "isCorrect": false
      },
      {
        "id": 177,
        "answerText": "AWS Marketplace",
        "isCorrect": false
      },
      {
        "id": 178,
        "answerText": "Amazon API Gateway",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 44,
    "questionId": null,
    "questionText": "A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department. Which of the below options is NOT feasible for cross-account access of S3 bucket objects?",
    "questionHint": "To determine which option is NOT feasible for cross-account access of S3 bucket objects in AWS, consider the following:\n\n**Feasible Options:**\n\n- **Bucket Policy:** The Finance department can attach a bucket policy allowing HRâ€™s IAM roles or users access.\n- **IAM Role with Trust Relationship:** Finance sets up an IAM role that HR can assume. The role has access to the bucket, and the trust policy allows HR to assume it.\n- **Access Control List (ACL):** ACLs can grant access to another AWS account, though this is less recommended.\n- **S3 Access Points:** S3 Access Points can be configured to allow cross-account access.\n\n**Not Feasible Option (Correct Answer):**\n\n- **Creating IAM users in the Finance account for Human Resources access:** This violates AWS best practices, introduces credential management overhead, and is not truly cross-account access. It's neither scalable nor secure.",
    "answers": [
      {
        "id": 179,
        "answerText": "Using S3 Bucket Policy referencing IAM users in the same account",
        "isCorrect": false
      },
      {
        "id": 180,
        "answerText": "Creating IAM users in the Finance account for Human Resources access",
        "isCorrect": true
      },
      {
        "id": 181,
        "answerText": "Using IAM Role with Trust Relationship",
        "isCorrect": false
      },
      {
        "id": 182,
        "answerText": "Using Access Control List (ACL)",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 45,
    "questionId": null,
    "questionText": "ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks. Which of the following solutions is the best choice for these workloads?",
    "questionHint": "To support ECS Fargate container tasks with persistent, cross-AZ shared access to data volumes:\n\n**Key Requirements:**\n\n- **Fargate Compatibility:** The storage must integrate with Fargate tasks.\n- **Cross-AZ Shared Access:** The data should be accessible across multiple Availability Zones simultaneously.\n- **Persistent Volumes:** The storage must persist beyond the life of individual container tasks.\n\n**Why Amazon EFS is the Best Solution:**\n\n- **Natively supports Fargate:** Can be mounted to Fargate tasks using the ECS volume configuration.\n- **Cross-AZ Access:** EFS is multi-AZ by design, providing high availability and redundancy.\n- **Shared Access:** Multiple tasks across AZs can mount the same EFS file system simultaneously.\n- **POSIX-compliant:** Acts like a traditional file system â€” ideal for workloads needing file locks or shared reads/writes.\n\n**Why Other Options Fail:**\n\n- **Amazon EBS:** Can only be attached to a single EC2 instance in one AZ. Not shareable across AZs. Not supported by Fargate.\n- **Amazon S3:** Object storage, not suitable for file systemâ€“level mounts. No POSIX compatibility, not usable as a native data volume in Fargate.\n- **EFS One-Zone:** Limited to one AZ â€” doesn't meet cross-AZ requirement.\n- **Self-managed NFS on EC2:** Requires you to manage EC2s and doesn't integrate natively with Fargate. Complex, not recommended.",
    "answers": [
      {
        "id": 183,
        "answerText": "Amazon EBS",
        "isCorrect": false
      },
      {
        "id": 184,
        "answerText": "Amazon S3",
        "isCorrect": false
      },
      {
        "id": 185,
        "answerText": "Amazon EFS",
        "isCorrect": true
      },
      {
        "id": 186,
        "answerText": "Self-managed NFS on EC2",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 46,
    "questionId": null,
    "questionText": "A website serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from an application load balancer. The user base is spread across the world and latency should be minimized for a better user experience. Which technology/service can help access the static and dynamic content while keeping the data latency low?",
    "questionHint": "To minimize latency for a global user base while accessing both static and dynamic content, the best AWS service to use is **Amazon CloudFront**.\n\n**How CloudFront helps:**\n\n- **Static content:**\n  - **Origin:** Amazon S3\n  - **CloudFront Role:** Caches and delivers static content (like images, CSS, JavaScript) from edge locations close to users.\n\n- **Dynamic content:**\n  - **Origin:** Application Load Balancer (ALB)\n  - **CloudFront Role:** Routes requests to ALB with optimized edge routing.\n\n**Why CloudFront is the best choice:**\n\n- **Content Delivery Network (CDN):** Distributes content through a global network of edge locations.\n- **Caching:** Caches static content, reducing latency by bringing it closer to users.\n- **Routing:** Intelligently routes dynamic requests to the ALB or origin servers based on request path patterns or behaviors.",
    "answers": [
      {
        "id": 187,
        "answerText": "Amazon CloudFront",
        "isCorrect": true
      },
      {
        "id": 188,
        "answerText": "Amazon Route 53",
        "isCorrect": false
      },
      {
        "id": 189,
        "answerText": "Amazon S3 Transfer Acceleration",
        "isCorrect": false
      },
      {
        "id": 190,
        "answerText": "AWS Global Accelerator",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 47,
    "questionId": null,
    "questionText": "You are designing a high-performance application that requires millions of connections. You have several EC2 instances running Apache2 web servers, and the application will require capturing the user's source IP address and source port with the use of X-Forwarded-For. Which of the following options meet your needs?",
    "questionHint": "To design a high-performance application that handles millions of connections, captures the user's source IP and port, and uses X-Forwarded-For, consider the following:\n\n**Correct Option:**\n\n- **Use a Network Load Balancer (NLB) and enable proxy protocol:**\n  - **Why it works:**\n    - NLB operates at Layer 4 (TCP) and can handle millions of requests per second with low latency.\n    - Enabling the Proxy Protocol preserves the source IP and port of the client and passes it to the backend servers.\n    - Apache or another intermediate proxy (like HAProxy or Nginx) can parse the Proxy Protocol header and optionally add the X-Forwarded-For header.\n\n**Why Other Options Are Incorrect:**\n\n- **Application Load Balancer (ALB) with X-Forwarded-For:**\n  - ALBs include X-Forwarded-For headers but do not include source port information.\n  - ALBs may not scale as efficiently as NLBs when handling millions of simultaneous TCP connections.\n\n- **Classic Load Balancer:**\n  - Legacy technology, not recommended for high-performance applications.\n  - Poor scaling compared to ALB or NLB.\n\n- **Direct connections to EC2 instances:**\n  - Defeats the purpose of load balancing and scalability.\n  - Insecure and difficult to manage under high traffic.",
    "answers": [
      {
        "id": 191,
        "answerText": "Use a Network Load Balancer (NLB) and enable proxy protocol",
        "isCorrect": true
      },
      {
        "id": 192,
        "answerText": "Use an Application Load Balancer (ALB) with X-Forwarded-For",
        "isCorrect": false
      },
      {
        "id": 193,
        "answerText": "Use a Classic Load Balancer",
        "isCorrect": false
      },
      {
        "id": 194,
        "answerText": "Direct connections to EC2 instances",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 48,
    "questionId": null,
    "questionText": "An IT company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a developer creates a stack called NetworkStack which will export the subnetId that can be used when creating EC2 instances in another stack. To use the exported value in another stack, which of the following functions must be used?",
    "questionHint": "To use the exported value from one AWS CloudFormation stack in another stack (cross-stack referencing), the correct function to use is **Fn::ImportValue**.\n\n**How It Works:**\n\n- **Stack 1 (e.g., NetworkStack):**\n  - Exports a value using the Export field in an output.\n  ```yaml\n  Outputs:\n    SubnetId:\n      Value: !Ref MySubnet\n      Export:\n        Name: MySubnetID\n  ```\n\n- **Stack 2 (e.g., ComputeStack):**\n  - Imports that value using Fn::ImportValue.\n  ```yaml\n  Resources:\n    MyEC2Instance:\n      Type: AWS::EC2::Instance\n      Properties:\n        SubnetId: !ImportValue MySubnetID\n  ```\n\n**Summary:**\n- **Function to use:** Fn::ImportValue\n- **Purpose:** Import an exported output value from another CloudFormation stack.\n- **Use case:** Cross-stack referencing for values like VPC ID, Subnet ID, etc.",
    "answers": [
      {
        "id": 195,
        "answerText": "Fn::ImportValue",
        "isCorrect": true
      },
      {
        "id": 196,
        "answerText": "Fn::GetAtt",
        "isCorrect": false
      },
      {
        "id": 197,
        "answerText": "Fn::Join",
        "isCorrect": false
      },
      {
        "id": 198,
        "answerText": "Fn::Sub",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 49,
    "questionId": null,
    "questionText": "A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET, and JavaScript. The team wants to model the cloud infrastructure using any of these programming languages. Which AWS service/tool should the team use for the given use-case?",
    "questionHint": "The best AWS service/tool for this use case is **AWS Cloud Development Kit (CDK)**.\n\n**Why AWS CDK?**\n\n- **Infrastructure as Code (IaC):** CDK allows you to define cloud infrastructure using familiar programming languages like Python, .NET (C#), and JavaScript/TypeScript.\n- **Multi-language support:** Perfect for a team using different languages, as CDK supports all of the ones mentioned.\n- **Serverless-friendly:** CDK has first-class support for AWS Lambda, API Gateway, DynamoDB, and other serverless services.\n- **Constructs:** High-level abstractions (constructs) simplify configuration and promote reuse.\n\n**Example (Python):**\n```python\nfrom aws_cdk import (\n    aws_lambda as _lambda,\n    core\n)\n\nclass MyLambdaStack(core.Stack):\n    def __init__(self, scope: core.Construct, id: str, **kwargs):\n        super().__init__(scope, id, **kwargs)\n\n        _lambda.Function(\n            self, 'MyLambda',\n            runtime=_lambda.Runtime.PYTHON_3_9,\n            handler='app.handler',\n            code=_lambda.Code.from_asset('lambda')\n        )\n```\n\n**Why Not Other Options?**\n\n- **AWS SAM (Serverless Application Model):** Good for serverless, but uses YAML and is less flexible with language choice.\n- **Terraform/CDK for Terraform:** Multi-language support via CDK-style interface, but not native AWS tooling.\n\n**Conclusion:**\nAWS CDK is the best choice here â€” it meets the teamâ€™s need for:\n\n- Serverless support\n- Multi-language programming\n- Modeling infrastructure as code.",
    "answers": [
      {
        "id": 199,
        "answerText": "AWS Cloud Development Kit (CDK)",
        "isCorrect": true
      },
      {
        "id": 200,
        "answerText": "AWS Serverless Application Model (SAM)",
        "isCorrect": false
      },
      {
        "id": 201,
        "answerText": "Terraform",
        "isCorrect": false
      },
      {
        "id": 202,
        "answerText": "AWS Management Console",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 50,
    "questionId": null,
    "questionText": "Does Amazon RDS for MySQL have auto-scaling?",
    "questionHint": "Amazon RDS for MySQL does not support traditional 'auto-scaling' for compute resources. Here's what it supports and does not support:\n\n**âœ… What RDS MySQL Can Do:**\n\n- **Storage Auto Scaling:**\n  - Automatically scales storage as needed (up to 64 TiB).\n  - Enabled via the `Enable storage autoscaling` option when creating or modifying a DB instance.\n\n- **Read Scalability with Read Replicas:**\n  - Allows horizontal scaling of read workloads by creating read replicas.\n  - Requires manual management to create and connect to replicas.\n\n**âŒ What RDS MySQL Cannot Do:**\n\n- **Automatic Compute Scaling:**\n  - Does not auto-scale CPU/memory (instance class).\n  - You must manually modify the instance type to increase compute capacity, which can cause downtime unless Multi-AZ deployments are used.\n\n- **No Serverless Mode:**\n  - RDS MySQL is not serverless. For serverless scaling, consider Amazon Aurora Serverless v2 (MySQL-compatible).\n\n**ðŸ” Alternatives for Auto Scaling:**\n\n- **Aurora Serverless v2 (MySQL-compatible):**\n  - Scales compute in fine-grained increments based on load.\n  - Ideal for unpredictable workloads or development environments.",
    "answers": [
      {
        "id": 203,
        "answerText": "RDS MySQL supports storage auto scaling and read scalability with read replicas.",
        "isCorrect": true
      },
      {
        "id": 204,
        "answerText": "RDS MySQL supports automatic compute scaling for CPU and memory.",
        "isCorrect": false
      },
      {
        "id": 205,
        "answerText": "RDS MySQL supports serverless mode for unpredictable workloads.",
        "isCorrect": false
      },
      {
        "id": 206,
        "answerText": "RDS MySQL scales compute resources automatically without downtime.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 51,
    "questionId": null,
    "questionText": "A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use. Which of the following statements are true regarding data security on EBS?",
    "questionHint": "EBS provides strong data security by supporting encryption at rest and in transit. Key points include:\n\n- **Encryption:** EBS volumes can be encrypted using AWS KMS keys (AWS-managed or customer-managed). Encryption applies to data at rest, in transit between EC2 and EBS, and snapshots.\n- **Automatic Snapshot Encryption:** Snapshots created from encrypted EBS volumes are automatically encrypted.\n- **Transparency:** Data encryption is seamless and transparent to applications using the volumes.\n- **Durability:** EBS replicates data within the availability zone to ensure durability.\n- **Consistent IOPS:** To achieve 14,000 consistent IOPS, use io2 or io2 Block Express volumes.\n\n**Example:**\n```java\nCreateVolumeRequest request = CreateVolumeRequest.builder()\n    .availabilityZone(\"us-east-1a\")\n    .size(100) // size in GiB\n    .volumeType(\"io2\")\n    .iops(14000)\n    .encrypted(true) // Enable encryption\n    .kmsKeyId(\"arn:aws:kms:region:account-id:key/key-id\") // Customer managed key\n    .build();\n```",
    "answers": [
      {
        "id": 207,
        "answerText": "EBS volumes can be encrypted using AWS KMS keys, ensuring data is encrypted at rest and in transit.",
        "isCorrect": true
      },
      {
        "id": 208,
        "answerText": "All snapshots created from encrypted EBS volumes are automatically encrypted.",
        "isCorrect": true
      },
      {
        "id": 209,
        "answerText": "Data encryption is transparent to the applications using the volumes.",
        "isCorrect": true
      },
      {
        "id": 210,
        "answerText": "EBS replicates data within the availability zone to provide durability.",
        "isCorrect": true
      },
      {
        "id": 211,
        "answerText": "EBS volumes automatically scale compute resources to meet IOPS requirements.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 52,
    "questionId": null,
    "questionText": "A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams. Which API integration type is best suited for this requirement?",
    "questionHint": "The best API integration type to support isolating development efforts with simulated API components is **API Mocking (Simulated APIs)**.\n\n**Why API Mocking fits:**\n\n- Teams can create mock versions of APIs that simulate real API behavior without needing the actual backend implemented or available.\n- Enables parallel development and testing of dependent components.\n- Helps catch integration issues early by validating contract and interactions against mocks.\n- Facilitates continuous integration and collaborative development.\n\n**Why Other Options Are Less Suitable:**\n\n- **Synchronous REST API Calls:** Requires live services, which limits isolation.\n- **Asynchronous Messaging/Event-Driven:** Focuses on decoupling communication patterns, not simulating APIs.\n- **API Gateway with Routing:** Helps manage traffic but does not inherently support simulating or isolating development efforts.",
    "answers": [
      {
        "id": 212,
        "answerText": "API Mocking (Simulated APIs)",
        "isCorrect": true
      },
      {
        "id": 213,
        "answerText": "Synchronous REST API Calls",
        "isCorrect": false
      },
      {
        "id": 214,
        "answerText": "Asynchronous Messaging/Event-Driven",
        "isCorrect": false
      },
      {
        "id": 215,
        "answerText": "API Gateway with Routing",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 53,
    "questionId": null,
    "questionText": "Which AWS database engines can be configured with IAM Database Authentication?",
    "questionHint": "**AWS IAM Database Authentication** can be configured with the following AWS database engines:\n\n**âœ… Supported DB Engines:**\n\n- **Amazon RDS for MySQL**\n  - Versions: 5.6, 5.7, 8.0\n  - Notes: IAM authentication must be enabled on the DB instance.\n\n- **Amazon RDS for PostgreSQL**\n  - Versions: 9.5 and higher\n  - Notes: Also supported on Amazon Aurora PostgreSQL.\n\n- **Amazon Aurora MySQL-Compatible Edition**\n  - Versions: 5.6 and 5.7 compatible Aurora\n  - Notes: IAM authentication supported via the same method as RDS.\n\n- **Amazon Aurora PostgreSQL-Compatible Edition**\n  - Versions: All supported versions\n  - Notes: IAM authentication is tightly integrated.\n\n**âŒ Not Supported:**\n\n- **RDS for Oracle:** Not supported.\n- **RDS for SQL Server:** Not supported.\n- **Amazon Redshift:** Uses a different IAM-based authentication, not the same as IAM DB Auth.\n- **DynamoDB:** Doesn't need IAM DB Auth; uses IAM for all API access already.",
    "answers": [
      {
        "id": 216,
        "answerText": "Amazon RDS for MySQL",
        "isCorrect": true
      },
      {
        "id": 217,
        "answerText": "Amazon RDS for PostgreSQL",
        "isCorrect": true
      },
      {
        "id": 218,
        "answerText": "Amazon Aurora MySQL-Compatible Edition",
        "isCorrect": true
      },
      {
        "id": 219,
        "answerText": "Amazon Aurora PostgreSQL-Compatible Edition",
        "isCorrect": true
      },
      {
        "id": 220,
        "answerText": "Amazon RDS for Oracle",
        "isCorrect": false
      },
      {
        "id": 221,
        "answerText": "Amazon RDS for SQL Server",
        "isCorrect": false
      },
      {
        "id": 222,
        "answerText": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "id": 223,
        "answerText": "Amazon DynamoDB",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 54,
    "questionId": null,
    "questionText": "What should you use while migrating from GitHub to AWS CodeCommit over HTTPS?",
    "questionHint": "When migrating from GitHub to AWS CodeCommit over HTTPS, follow these steps:\n\n**1. Use AWS IAM for Authentication:**\n- Generate HTTPS Git credentials for an IAM user with CodeCommit permissions.\n- Attach the `AWSCodeCommitPowerUser` policy or a custom policy to the IAM user.\n\n**2. Clone or Migrate Repositories:**\n- Clone your GitHub repository locally if not already:\n  ```bash\n  git clone https://github.com/your-username/your-repo.git\n  cd your-repo\n  ```\n- Set the CodeCommit HTTPS remote URL:\n  ```bash\n  git remote set-url origin https://git-codecommit.<region>.amazonaws.com/v1/repos/your-repo\n  ```\n- Push to CodeCommit:\n  ```bash\n  git push origin --all\n  git push origin --tags\n  ```\n\n**3. (Optional) Configure .git-credentials for Convenience:**\n- To avoid entering credentials every time:\n  ```bash\n  git config --global credential.helper store\n  ```\n\n**4. Use AWS CLI Credential Helper (Preferred):**\n- Configure Git to use the AWS CLI as a credential helper:\n  ```bash\n  git config --global credential.helper '!aws codecommit credential-helper $@'\n  git config --global credential.UseHttpPath true\n  ```\n- This securely manages credentials using the AWS CLI.\n\n**Summary:**\n- **Authentication:** IAM + HTTPS Git credentials\n- **Repo Access:** CodeCommit HTTPS URL\n- **Migration Command:** `git push origin --all`",
    "answers": [
      {
        "id": 224,
        "answerText": "Generate HTTPS Git credentials from AWS IAM for authentication.",
        "isCorrect": true
      },
      {
        "id": 225,
        "answerText": "Use SSH keys for authentication with CodeCommit.",
        "isCorrect": false
      },
      {
        "id": 226,
        "answerText": "Use the AWS CLI credential helper to manage credentials securely.",
        "isCorrect": true
      },
      {
        "id": 227,
        "answerText": "Directly push to CodeCommit without configuring IAM credentials.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 54,
    "questionId": null,
    "questionText": "What should you use while migrating from GitHub to AWS CodeCommit over HTTPS?",
    "questionHint": "When migrating from GitHub to AWS CodeCommit over HTTPS, follow these steps:\n\n**1. Use AWS IAM for Authentication:**\n- Generate HTTPS Git credentials for an IAM user with CodeCommit permissions.\n- Attach the `AWSCodeCommitPowerUser` policy or a custom policy to the IAM user.\n\n**2. Clone or Migrate Repositories:**\n- Clone your GitHub repository locally if not already:\n  ```bash\n  git clone https://github.com/your-username/your-repo.git\n  cd your-repo\n  ```\n- Set the CodeCommit HTTPS remote URL:\n  ```bash\n  git remote set-url origin https://git-codecommit.<region>.amazonaws.com/v1/repos/your-repo\n  ```\n- Push to CodeCommit:\n  ```bash\n  git push origin --all\n  git push origin --tags\n  ```\n\n**3. (Optional) Configure .git-credentials for Convenience:**\n- To avoid entering credentials every time:\n  ```bash\n  git config --global credential.helper store\n  ```\n\n**4. Use AWS CLI Credential Helper (Preferred):**\n- Configure Git to use the AWS CLI as a credential helper:\n  ```bash\n  git config --global credential.helper '!aws codecommit credential-helper $@'\n  git config --global credential.UseHttpPath true\n  ```\n- This securely manages credentials using the AWS CLI.\n\n**Summary:**\n- **Authentication:** IAM + HTTPS Git credentials\n- **Repo Access:** CodeCommit HTTPS URL\n- **Migration Command:** `git push origin --all`",
    "answers": [
      {
        "id": 224,
        "answerText": "Generate HTTPS Git credentials from AWS IAM for authentication.",
        "isCorrect": true
      },
      {
        "id": 225,
        "answerText": "Use SSH keys for authentication with CodeCommit.",
        "isCorrect": false
      },
      {
        "id": 226,
        "answerText": "Use the AWS CLI credential helper to manage credentials securely.",
        "isCorrect": true
      },
      {
        "id": 227,
        "answerText": "Directly push to CodeCommit without configuring IAM credentials.",
        "isCorrect": false
      }
    ]
  },
  {
    "id": 55,
    "questionId": null,
    "questionText": "A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions. Which of the following statements should the developer consider while defining the signers?",
    "questionHint": "When defining signers for Amazon CloudFront signed URLs, consider the following:\n\n**âœ… Valid Considerations:**\n\n- **Trusted Signer Must Be Specified in the Distribution Settings:** The AWS account that owns the CloudFront key pair (used for signing URLs) must be added as a trusted signer in the CloudFront distribution settings. Only trusted signers can generate valid signed URLs.\n\n- **Key Pair is Required for Legacy Signed URLs:** If using the CloudFront key pair method (legacy), the signer must use an RSA key pair associated with their AWS account. The private key is used to sign the URL, and the key pair ID is included in the URL.\n\n- **Use of AWS Key Management Service (KMS) is Not Supported for URL Signing:** CloudFront does not support KMS for signing URLs directly. Only CloudFront key pairs or Lambda@Edge with custom logic are used for this purpose.\n\n- **If Using Signed Cookies, the Signers Work Similarly:** Trusted signers and key pair IDs are also required when generating signed cookies, and the same principles apply.\n\n- **IAM Roles or Users Cannot Directly Be Signers:** A signer is not an IAM user or role, but rather the AWS account that owns the CloudFront key pair.\n\n**âŒ Common Misconceptions:**\n\n- **\"Any IAM user in my account can be a signer\":** Incorrect. IAM users cannot be signers; only the AWS account's key pair can be.\n- **\"KMS keys can be used to sign URLs\":** Incorrect. KMS is not integrated with CloudFront URL signing.",
    "answers": [
      {
        "id": 228,
        "answerText": "The AWS account that owns the CloudFront key pair must be added as a trusted signer in the CloudFront distribution settings.",
        "isCorrect": true
      },
      {
        "id": 229,
        "answerText": "KMS keys can be used to sign URLs for Amazon CloudFront distributions.",
        "isCorrect": false
      },
      {
        "id": 230,
        "answerText": "IAM users in the AWS account can directly act as signers for generating signed URLs.",
        "isCorrect": false
      },
      {
        "id": 231,
        "answerText": "Trusted signers and key pair IDs are required for generating signed cookies.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 55,
    "questionId": null,
    "questionText": "A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions. Which of the following statements should the developer consider while defining the signers?",
    "questionHint": "When defining signers for Amazon CloudFront signed URLs, consider the following:\n\n**âœ… Valid Considerations:**\n\n- **Trusted Signer Must Be Specified in the Distribution Settings:** The AWS account that owns the CloudFront key pair (used for signing URLs) must be added as a trusted signer in the CloudFront distribution settings. Only trusted signers can generate valid signed URLs.\n\n- **Key Pair is Required for Legacy Signed URLs:** If using the CloudFront key pair method (legacy), the signer must use an RSA key pair associated with their AWS account. The private key is used to sign the URL, and the key pair ID is included in the URL.\n\n- **Use of AWS Key Management Service (KMS) is Not Supported for URL Signing:** CloudFront does not support KMS for signing URLs directly. Only CloudFront key pairs or Lambda@Edge with custom logic are used for this purpose.\n\n- **If Using Signed Cookies, the Signers Work Similarly:** Trusted signers and key pair IDs are also required when generating signed cookies, and the same principles apply.\n\n- **IAM Roles or Users Cannot Directly Be Signers:** A signer is not an IAM user or role, but rather the AWS account that owns the CloudFront key pair.\n\n**âŒ Common Misconceptions:**\n\n- **\"Any IAM user in my account can be a signer\":** Incorrect. IAM users cannot be signers; only the AWS account's key pair can be.\n- **\"KMS keys can be used to sign URLs\":** Incorrect. KMS is not integrated with CloudFront URL signing.",
    "answers": [
      {
        "id": 228,
        "answerText": "The AWS account that owns the CloudFront key pair must be added as a trusted signer in the CloudFront distribution settings.",
        "isCorrect": true
      },
      {
        "id": 229,
        "answerText": "KMS keys can be used to sign URLs for Amazon CloudFront distributions.",
        "isCorrect": false
      },
      {
        "id": 230,
        "answerText": "IAM users in the AWS account can directly act as signers for generating signed URLs.",
        "isCorrect": false
      },
      {
        "id": 231,
        "answerText": "Trusted signers and key pair IDs are required for generating signed cookies.",
        "isCorrect": true
      }
    ]
  },
  {
    "id": 56,
    "questionId": null,
    "questionText": "Which two statements are correct when defining signers for Amazon CloudFront signed URLs?",
    "questionHint": "**Correct Answers:**\n\n1. **When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL:**\n   - In the CloudFront key pair method, the public key is uploaded to CloudFront, and the private key is used by the trusted signer to generate signed URLs or cookies.\n\n2. **When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account:**\n   - Each AWS account (using the root user) can have a maximum of two active CloudFront key pairs at a time.\n\n**Incorrect Options:**\n\n- **You can also use IAM permissions policies to restrict what the root user can do with CloudFront key pairs:**\n  - IAM policies do not apply to the root user. The root user has unrestricted access by default.\n\n- **CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources:**\n  - Only the root user of the AWS account can create CloudFront key pairs (for the legacy method).\n\n- **Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs:**\n  - While key groups can be managed via API, CloudFront key pairs must be managed via the AWS Management Console, not APIs.",
    "answers": [
      {
        "id": 232,
        "answerText": "You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs.",
        "isCorrect": false
      },
      {
        "id": 233,
        "answerText": "When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL.",
        "isCorrect": true
      },
      {
        "id": 234,
        "answerText": "CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources.",
        "isCorrect": false
      },
      {
        "id": 235,
        "answerText": "Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs.",
        "isCorrect": false
      },
      {
        "id": 236,
        "answerText": "When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account.",
        "isCorrect": true
      }
    ]
  }
]