[
  {
    "answerId": 39740285,
    "question": "Kinesis ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_1.jpg\"><span>Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it’s fine if the number of shards exceeds the number of instances.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Since the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">4 shards : 2 instances</strong></b><span>&nbsp;is the correct answer.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">1 shard : 6 instances</strong></b><span>&nbsp;ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">6 shards : 1 instance</strong></b><span>&nbsp;ratio is incorrect because&nbsp;having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn’t have a backup instance to process the shards in the event of an outage.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">4 shards : 8 instances</strong></b><span>&nbsp;ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition,&nbsp;this option is not the most cost-effective choice as well.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569391,
        "value": "4 shards : 8 instances",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569392,
        "value": "4 shards : 2 instances",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569393,
        "value": "1 shard : 6 instances",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569394,
        "value": "6 shards : 1 instance",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 0,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569394,
        "questionId": 391448,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391448,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740286,
    "question": "X-Ray",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM controls access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI) your users employ.&nbsp;To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXrayReadOnlyAccess</strong></b><span>&nbsp;managed policy to your IAM user.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For local development and testing, create an IAM user with read and write permissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray daemon, the AWS CLI, and the AWS SDK.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To deploy your instrumented app to AWS, create an IAM role with write permissions and assign it to the resources running your application.&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXRayDaemonWriteAccess</strong></b><span>&nbsp;includes permission to upload traces and some read permissions as well to support the use of sampling rules.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The read and write policies do not include permission to configure encryption key settings and sampling rules. Use&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXrayFullAccess</strong></b><span>&nbsp;to access these settings or add configuration APIs in a custom policy. For encryption and decryption with a customer managed key that you create, you also need permission to use the key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXrayReadOnlyAccess</strong></b><span>&nbsp;managed policy is the most appropriate one to grant to the developer in order for her to access the X-Ray console. This also abides with the standard security advice of granting least privilege, or granting only the permissions required to perform a task.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXRayDaemonWriteAccess</strong></b><span>&nbsp;is incorrect because this policy is more suitable if you want to grant permission to upload traces to X-Ray.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWSXrayFullAccess</strong></b><span>&nbsp;is incorrect. Although this can provide the required access to the developer, it does not abide with the standard security advice of granting least privilege. Hence, this is not the most appropriate policy to use.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AmazonS3ReadOnlyAccess</strong></b><span>&nbsp;is incorrect because&nbsp;this policy just provides the instance permission to download the X-Ray daemon from Amazon S3.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/security.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/xray/latest/devguide/security.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569395,
        "value": "AmazonS3ReadOnlyAccess",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569396,
        "value": "AWSXrayFullAccess",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569397,
        "value": "AWSXrayReadOnlyAccess",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569398,
        "value": "AWSXRayDaemonWriteAccess",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 1,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569397,
        "questionId": 391449,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391449,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A newly hired developer has been instructed to debug an application. She tried to access the X-Ray console to view service maps and segments but her current access is insufficient.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST appropriate managed policy that should be granted to the developer?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740287,
    "question": " Lambda function",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a&nbsp;custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can&nbsp;develop your function code in the Lambda console&nbsp;as long as you keep your deployment package under 3 MB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_3.jpg\"><span>A function can use up to 5 layers at a time. The total unzipped size of the function and all layers can’t exceed the unzipped deployment package size limit of 250 MB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to use Lambda&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Layers</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Alias</strong></b><span>&nbsp;is incorrect because this is just like a pointer to a specific Lambda function version. This will not enable your function to pull in additional code and other dependencies from another source.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Execution Context</strong></b><span>&nbsp;is incorrect because&nbsp;this is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Environment Variable</strong></b><span>&nbsp;is incorrect because this just enables you to dynamically pass settings to your function code and libraries, without making changes to your code.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/limits.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/limits.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569399,
        "value": "Execution Context",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569400,
        "value": "Environment Variable",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569401,
        "value": "Alias",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569402,
        "value": "Layers",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 2,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569402,
        "questionId": 391450,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391450,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A Lambda function is over 80 MB in size, which exceeds the deployment package size limit for direct uploads. You want to refactor the function to pull in additional code and other dependencies from another source, which will reduce the size of the deployment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which feature of Lambda should you use in order to implement the above task?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740288,
    "question": "Cross-device syncing",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_4.jpg\"><span>The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Cognito Sync</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">.</strong></b></i></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Cognito User Pools&nbsp;</strong></b><span>is incorrect because&nbsp;this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Cognito Identity Pools</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Device Farm</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569403,
        "value": "Amazon Cognito Identity Pools",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569404,
        "value": "AWS Device Farm",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569405,
        "value": "Amazon Cognito User Pools",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569406,
        "value": "Amazon Cognito Sync",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 3,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569403,
        "questionId": 391451,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391451,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the most suitable feature that you have to use to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740289,
    "question": "EC2 instance",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Aside from the usual metrics, it also tracks the&nbsp;memory, swap, and disk space utilization metrics of your server.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_5.jpg\"><span>Hence, the most suitable solution to use in this scenario is to:</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">Install the Amazon CloudWatch Logs agent to the EC2 instance.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use AWS CloudShell to consolidate all metrics in a single dashboard</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because&nbsp;this service is simply a command-line interface used for managing AWS resources from a terminal. It is not a monitoring tool and does not collect or display EC2 instance metrics.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Using detailed monitoring in CloudWatch&nbsp;</strong></b><span>is incorrect because this will typically send metric data of the EC2 instance to CloudWatch in 1-minute periods instead of 5-minute intervals.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Install the AWS X-Ray daemon on the EC2 instance&nbsp;</strong></b><span>is incorrect. Although this is helpful for troubleshooting your application, it does not have the capability to track the memory and swap usage of the instance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569407,
        "value": "Use detailed monitoring in CloudWatch.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569408,
        "value": "Install the Amazon CloudWatch Logs agent to the EC2 instance.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569409,
        "value": "Install the AWS X-Ray daemon on the EC2 instance.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569410,
        "value": "Use AWS CloudShell to consolidate all metrics in a single dashboard.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 4,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569409,
        "questionId": 391452,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391452,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have frequently reported that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to identify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage, and the count of idle and active processes running on the instance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which solution would be the MOST appropriate to implement in this situation?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740290,
    "question": "AWS service",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_6.jpg\"><span>All other options are incorrect because these services do not provide real-time processing, unlike Kinesis.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/kinesis/data-streams/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/kinesis/data-streams/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/kinesis/data-streams/faqs/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569411,
        "value": "AWS DynamoDB",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569412,
        "value": "AWS Kinesis Data Stream",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569413,
        "value": "AWS Redshift",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569414,
        "value": "AWS Elastic Map Reduce",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 5,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569412,
        "questionId": 391453,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391453,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are developing an application that continuously collects data about player-game interactions and feeds the real-time data into your gaming platform. There is a requirement to make the system highly scalable to accommodate the sudden influx of gamers that will use the platform.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which AWS service will help you achieve this?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740291,
    "question": "Amazon Elastic Beanstalk ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For applications that need to read or write multiple items, DynamoDB provides the&nbsp;BatchGetItem&nbsp;and&nbsp;BatchWriteItem&nbsp;operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_7.jpg\"><span>The batch operations are essentially wrappers around multiple read or write requests. For example, if a&nbsp;BatchGetItem&nbsp;request contains five items, DynamoDB performs five&nbsp;GetItem&nbsp;operations on your behalf. Similarly, if a&nbsp;BatchWriteItem&nbsp;request contains two put requests and four delete requests, DynamoDB performs two&nbsp;PutItem&nbsp;and four&nbsp;DeleteItem&nbsp;requests.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In general, a batch operation does not fail unless&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">all</em></i><span>&nbsp;of the requests in the batch fail. For example, suppose you perform a&nbsp;BatchGetItemoperation but one of the individual&nbsp;GetItem&nbsp;requests in the batch fails. In this case,&nbsp;BatchGetItem&nbsp;returns the keys and data from the&nbsp;GetItemrequest that failed. The other&nbsp;GetItem&nbsp;requests in the batch are not affected.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">use DynamoDB Batch Operations API for GET, PUT, and DELETE operations</strong></b><span>&nbsp;in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Upgrading the EC2 instances to a higher instance type&nbsp;</strong></b><span>is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Enabling DynamoDB Streams</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Refactoring the application to use DynamoDB transactional read and write APIs</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily.&nbsp;Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests.&nbsp;Hence, this entails a significant increase in costs which contradicts the requirements of the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569415,
        "value": "Refactor the application to use DynamoDB transactional read and write APIs .",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569416,
        "value": "Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569417,
        "value": "Enable DynamoDB Streams",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569418,
        "value": "Upgrade the EC2 instances to a higher instance type.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 6,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569416,
        "questionId": 391454,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391454,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application’s performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740292,
    "question": "Lambda function ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have two types of integration:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;– Lambda proxy integration</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;– Lambda custom integration</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration’s HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In Lambda non-proxy (or custom) integration, in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the&nbsp;</span><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>function-invoking action&nbsp;</span></a><span>of the Lambda service.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"http://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_8.jpg\"><span>The&nbsp;Lambda proxy integration type (AWS_PROXY)&nbsp;lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function. With this type of integration, also known as the Lambda proxy integration, you do not set the integration request or the integration response. API Gateway passes the incoming request from the client as the input to the backend Lambda function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The integrated Lambda function takes the&nbsp;</span><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>input of this format</span></a><span>&nbsp;and parses the input from all available sources, including request headers, URL path variables, query string parameters, and applicable body. The function returns the result following this&nbsp;</span><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-output-format\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>output format</span></a><span>. This is the preferred integration type to call a Lambda function through API Gateway and is not applicable to any other AWS service actions, including Lambda actions other than the function-invoking action.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence,&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Lambda proxy integration</strong></b><span>&nbsp;is correct&nbsp;as it matches the description depicted in the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Lambda custom integration</strong></b><span>&nbsp;is incorrect because this&nbsp;type requires you to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">HTTP custom integration</strong></b><span>&nbsp;is incorrect because this type is only used where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Take note that the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">HTTP proxy integration&nbsp;</strong></b><span>is incorrect because&nbsp;the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569419,
        "value": "HTTP Proxy integration",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569420,
        "value": "Lambda custom integration",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569421,
        "value": "Lambda proxy integration",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569422,
        "value": "HTTP custom integration",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 7,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569419,
        "questionId": 391455,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391455,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer has recently released a new Lambda function which calculates accruals, interests, and other financial data. This function must have a streamlined integration setup with API Gateway. The requirement is to pass the incoming request from the client as the input to the backend Lambda function, via HTTPS, in the following format:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><pre data-highlight-language=\"java\"><code>{\n\"resource\": \"Resource path\",\n\"path\": \"Path parameter\",\n\"httpMethod\": \"Incoming request's method name\"\n\"headers\": {String containing incoming request headers}\n\"multiValueHeaders\": {List of strings containing incoming request headers}\n\"queryStringParameters\": {query string parameters }\n\"multiValueQueryStringParameters\": {List of query string parameters}\n\"pathParameters\": {path parameters}\n\"stageVariables\": {Applicable stage variables}\n\"requestContext\": {Request context, including authorizer-returned key-value pairs}\n\"body\": \"A JSON string of the request payload.\"\n\"isBase64Encoded\": \"A boolean flag to indicate if the applicable request payload is Base64-encode\"\n}</code></pre><span>Which of the following options is the MOST appropriate method to use to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740293,
    "question": "Auto Scaling group",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn’t automatically delete the message. Because Amazon SQS is a distributed system, there’s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_9.jpg\"><span>Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a&nbsp;</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">visibility timeout</strong></b></i><span>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn’t call the&nbsp;</span><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_DeleteMessage.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>DeleteMessage</span></a><span>&nbsp;action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Every Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the best solution in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Configuring the queue to use short polling by setting the&nbsp;WaitTimeSeconds&nbsp;parameter of the&nbsp;ReceiveMessage&nbsp;request to 0</strong></b><span>&nbsp;is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Configuring the queue to use long polling by setting the&nbsp;Receive Message Wait Time&nbsp;parameter to a value greater than 0</strong></b><i><em class=\"Editor__editor-text-italic___C9n8O\">&nbsp;</em></i><span>is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">when there are no messages available for a ReceiveMessage request</em></i><span>) and false empty responses (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">when messages are available but aren’t included in a response</em></i><span>). A more appropriate solution in this scenario is to configure the visibility timeout of the messages.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Postponing the delivery of new messages by using a delay queue&nbsp;</strong></b><span>is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/sqs/faqs/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/sqs/faqs/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569423,
        "value": "Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569424,
        "value": "Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569425,
        "value": "Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569426,
        "value": "Postpone the delivery of new messages by using a delay queue.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 8,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569423,
        "questionId": 391456,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391456,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the BEST solution that the developer should implement to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740294,
    "question": "Lambda function ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A gateway response is identified by a response type defined by API Gateway. The response consists of an HTTP status code, a set of additional headers that are specified by parameter mappings, and a payload that is generated by a non-VTL (Apache Velocity Template Language) mapping template.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_10.jpg\"><span>You can set up a gateway response for a supported response type at the API level. Whenever API Gateway returns a response of the type, the header mappings and payload mapping templates defined in the gateway response are applied to return the mapped results to the API caller.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">INTEGRATION_FAILURE</strong></b><span>&nbsp;– The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">INTEGRATION_TIMEOUT</strong></b><span>&nbsp;– The gateway response for an integration timed-out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In this scenario, there is an issue where the users are getting HTTP 504 errors in the serverless application. This means the Lambda function is working fine at times, but there are instances when it throws an error. Based on this analysis, the most likely cause of the issue is the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">INTEGRATION_TIMEOUT</strong></b><span>&nbsp;error since you will only get an&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">INTEGRATION_FAILURE</strong></b><span>&nbsp;error if your AWS Lambda integration does not work at all in the first place.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">The memory allocated for the Lambda function is insufficient</strong></b><span>&nbsp;is incorrect. The fact that no errors were found in the CloudWatch Logs suggests that the function is not the bottleneck.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors</strong></b><span>&nbsp;is incorrect because&nbsp;a large number of incoming requests will most likely produce an HTTP 502 or 429 error but not a 504 error.&nbsp;If executing the function would cause you to exceed a concurrency limit at either the account level (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">ConcurrentInvocationLimitExceeded</em></i><span>) or function level (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">ReservedFunctionConcurrentInvocationLimitExceeded</em></i><span>),&nbsp;Lambda may return a&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">TooManyRequestsException</em></i><span>&nbsp;as a response.&nbsp;For functions with a long timeout, your client might be disconnected during synchronous invocation while it waits for a response and returns an HTTP 504 error.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">There is an authorization failure occurring between API Gateway and the Lambda function</strong></b><span>&nbsp;is incorrect because&nbsp;an authentication issue usually produces HTTP 403 errors and not 504s.&nbsp;The gateway response for authorization failures for missing authentication token errors, invalid AWS signature errors, or Amazon Cognito authentication problems is HTTP 403, which is why this option is unlikely to cause this issue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/customize-integration-timeouts-in-amazon-api-gateway/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/about-aws/whats-new/2017/11/customize-integration-timeouts-in-amazon-api-gateway/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569427,
        "value": "There is an authorization failure occurring between API Gateway and the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569428,
        "value": "The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569429,
        "value": "The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569430,
        "value": "The memory allocated for the Lambda function is insufficient",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 9,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569428,
        "questionId": 391457,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391457,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A serverless application, which uses a Lambda function integrated with API Gateway, provides data to a front-end application written in ReactJS. The users are complaining that they are getting HTTP 504 errors intermittently when they are using the application in peak times. The developer found no errors in the CloudWatch logs of the Lambda function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST likely cause of this issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740295,
    "question": "CORS configuration",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You add the XML document as the cors subresource to the bucket either programmatically or by using the Amazon S3 console as shown below:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_11.jpg\"><span>A CORS configuration is an XML file that contains a series of rules within a&nbsp;&lt;CORSRule&gt;. A configuration can have up to 100 rules. A rule is defined by one of the following tags:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AllowedOrigin</strong></b><span>&nbsp;– Specifies domain origins that you allow to make cross-domain requests.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AllowedMethod</strong></b><span>&nbsp;– Specifies a type of request you allow (GET, PUT, POST, DELETE, HEAD) in cross-domain requests.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AllowedHeader</strong></b><span>&nbsp;– Specifies the headers allowed in a preflight request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;Below are some of the&nbsp;CORSRule&nbsp;elements:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">MaxAgeSeconds</strong></b><span>&nbsp; – Specifies the amount of time in seconds (in this example, 3000) that the browser caches an Amazon S3 response to a preflight OPTIONS request for the specified resource. By caching the response, the browser does not have to send preflight requests to Amazon S3 if the original request will be repeated.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">ExposeHeader</strong></b><span>&nbsp; – Identifies the response headers (in this example,&nbsp;x-amz-server-side-encryption,&nbsp;x-amz-request-id, and&nbsp;x-amz-id-2) that customers are able to access from their applications (for example, from a JavaScript&nbsp;XMLHttpRequest&nbsp;object).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers in this scenario are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– It allows a user to view, add, remove or update objects inside the S3 bucket from the domain abc.com</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– This will cause the browser to cache an Amazon S3 response of a preflight OPTIONS request for 1 hour</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the request will fail if the&nbsp;x-amz-meta-custom-header&nbsp;header is not included</strong></b><span>&nbsp;is incorrect because the&nbsp;ExposeHeader&nbsp;element&nbsp;refers to the header that will be exposed to the response and not a constraint for the request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">this configuration authorizes the user to perform actions on the S3 bucket</strong></b><span>&nbsp;is incorrect because&nbsp;this configuration actually does the opposite. It doesn’t authorize the user to perform actions on the S3 bucket.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;all HTTP Methods are allowed</strong></b><span>&nbsp;is incorrect because the configuration didn’t include the&nbsp;HEAD&nbsp;HTTP method.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569431,
        "value": "This configuration authorizes the user to perform actions on the S3 bucket.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569432,
        "value": "This will cause the browser to cache the response of the preflight OPTIONS request for 1 hour.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569433,
        "value": "The request will fail if the x-amz-meta-custom-header header is not included.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569434,
        "value": "It allows a user to view, add, remove or update objects inside the S3 bucket from the domain abc.com.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569435,
        "value": "All HTTP Methods are allowed.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 10,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569434,
        "questionId": 391458,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569432,
        "questionId": 391458,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391458,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You were recently hired as a developer for a leading insurance firm in Asia which has a hybrid cloud architecture with AWS. The project that was assigned to you involves setting up a static website using Amazon S3 with a CORS configuration as shown below:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><pre data-highlight-language=\"java\"><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; \n&lt;CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt; \n    &lt;CORSRule&gt; \n        &lt;AllowedOrigin&gt;https://abc.com&lt;/AllowedOrigin&gt; \n        &lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt; \n        &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt; \n        &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt; \n        &lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt; \n        &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt; \n        &lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt; \n        &lt;ExposeHeader&gt;x-amz-meta-custom-header&lt;/ExposeHeader&gt; \n        &lt;MaxAgeSeconds&gt;3600&lt;/MaxAgeSeconds&gt; \n    &lt;/CORSRule&gt; \n&lt;/CORSConfiguration&gt;</code></pre><span>Which of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740296,
    "question": "Serverless application",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function’s handler method when the function is invoked. You can include a runtime in your function’s deployment package in the form of an executable file named&nbsp;bootstrap.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A runtime is responsible for running the function’s setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to Lambda.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Your custom runtime runs in the standard Lambda&nbsp;</span><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>execution environment</span></a><span>. It can be a shell script, a script in a language that’s included in Amazon Linux, or a binary executable file that’s compiled in Amazon Linux.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Uploading the deployment package to S3 and then using CloudFormation to deploy Lambda function with a reference to the S3 URL of the package&nbsp;</strong></b><span>is incorrect because you have to implement a Custom Runtime in order to execute the C++ code. Take note that this programming language is not natively supported yet in Lambda, which is why the use of a Custom Runtime is essential.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Creating a Lambda function with the C++ code and directly uploading it to AWS&nbsp;</strong></b><span>is incorrect because there is a 50 MB deployment package size limit in Lambda if you’ll directly upload the package. Just as mentioned above, you have to implement a Custom Runtime for this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using AWS Serverless Application Model (AWS SAM) to deploy the Lambda function</strong></b><span>&nbsp;is incorrect because using SAM alone is not enough to run the C++ code in Lambda. You have to use a Custom Runtime.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569436,
        "value": "Create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569437,
        "value": "Upload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3 URL of the package.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569438,
        "value": "Create a Lambda function with the C++ code and directly upload it to AWS.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569439,
        "value": "Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 11,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569438,
        "questionId": 391459,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391459,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A leading technology company is building a serverless application in AWS using the C++ programming language. The application will use DynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the compute resources to AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following steps should you implement to properly deploy the serverless application?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740297,
    "question": " DynamoDB database",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>By default, the DynamoDB write operations (PutItem,&nbsp;UpdateItem,&nbsp;DeleteItem) are&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">unconditional</em></i><span>: each of these operations will overwrite an existing item that has the specified primary key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_13.jpg\"><span>Conditional writes are helpful in many situations. For example, you might want a&nbsp;PutItem&nbsp;operation to succeed only if there is not already an item with the same primary key. Or you could prevent an&nbsp;UpdateItem&nbsp;operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Therefore,&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">configuring the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid</strong></b><span>&nbsp;is the most suitable solution in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers</strong></b><span>&nbsp;is incorrect because DynamoDB Streams is primarily used to capture a time-ordered sequence of item-level modifications in the table. Although you can use a Lambda function that checks if the current and new bid prices are correct, this solution is cumbersome to implement as opposed to using conditional writes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid</strong></b><span>&nbsp;is incorrect because the Optimistic locking strategy simply ensures that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. It doesn’t have the capability to specify certain conditions to meet the requirement.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Enabling DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price&nbsp;</strong></b><span>is incorrect because the DynamoDB Transactions feature simply provides developers atomicity, consistency, isolation, and durability (ACID) across one or more tables within a single AWS account and region. Although this can be used in building applications that require coordinated inserts, deletes, or updates to multiple items as part of a single logical business operation, the implementation of this solution entails a lot of work. Using conditional updates and conditional writes is still a more effective solution in this scenario instead of using DynamoDB Transactions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569440,
        "value": "Enable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569441,
        "value": "Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569442,
        "value": "Configure the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569443,
        "value": "Use DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 12,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569442,
        "questionId": 391460,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391460,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate properties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the auction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be greater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to meet the requirements.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST effective solution that will satisfy the requirement in this scenario?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740298,
    "question": "Amazon EventBridge ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">DynamoDB stream</em></i><span>&nbsp;is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">stream record</em></i><span>&nbsp;contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the “before” and “after” images of modified items.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_14.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>All data in DynamoDB Streams is subject to a 24-hour lifetime. You can retrieve and analyze the last 24 hours of activity for any given table; however, data older than 24 hours is susceptible to trimming (removal) at any moment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you disable a stream on a table, the data in the stream will continue to be readable for 24 hours. After this time, the data expires and the stream records are automatically deleted. Note that there is no mechanism for manually deleting an existing stream; you just need to wait until the retention limit expires (24 hours), and all the stream records will be deleted.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">decrease the interval of running your function to 24 hours</strong></b><span>&nbsp;because in DynamoDB Streams, data older than 24 hours is susceptible to trimming (removal) at any moment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Increasing the interval of running your function to 48 hours&nbsp;</strong></b><span>is incorrect because this will actually make the problem worse. Considering that the data in DynamoDB Streams only lasts for 24 hours, you should actually decrease the interval of running your function and not further increase it.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Setting the value of&nbsp;StreamViewType&nbsp;parameter in DynamoDB Streams to&nbsp;NEW_AND_OLD_IMAGES</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this just configures DynamoDB to write both the new and the old item images of the item to the stream. Setting the Stream View Type is actually irrelevant in this scenario since the problem is about missing data and not missing old or new values of the items.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Setting the value of&nbsp;StreamViewType&nbsp;parameter in DynamoDB Streams to&nbsp;NEW_IMAGE</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this just configures the DynamoDB to write only the new image of the item to the stream. Just as mentioned above, the root cause of the issue is the length of the interval of running the Lambda function and not the DynamoDB Streams configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569444,
        "value": "Set the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569445,
        "value": "Decrease the interval of running your function to 24 hours.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569446,
        "value": "Increase the interval of running your function to 48 hours.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569447,
        "value": "Set the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 13,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569445,
        "questionId": 391461,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391461,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company is developing an online system that lets patients schedule appointments with their preferred doctors at medical centers all over the country. The company uses Amazon DynamoDB as its primary database. The DynamoDB Streams feature is enabled on the DynamoDB table to capture all changes made to the booking data. A Lambda function integrated with Amazon EventBridge (Amazon CloudWatch Events) is used to process the data stream every 36 hours and then store the results in an S3 bucket.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>There are a lot of updated items in DynamoDB that are not sent to the S3 bucket, even though there are no errors in the logs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST appropriate solution for this issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740299,
    "question": "Amazon DynamoDB table",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For example, you can define a stage variable in a stage configuration, and then set its value as the URL string of an HTTP integration for a method in your REST API. Later, you can reference the URL string using the associated stage variable name from the API setup. This way, you can use the same API setup with a different endpoint at each stage by resetting the stage variable value to the corresponding URLs. You can also access stage variables in the mapping templates, or pass configuration parameters to your AWS Lambda or HTTP backend.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example,&nbsp;http://abc.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example,&nbsp;beta.abc.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_15.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can also use stage variables to pass configuration parameters to a Lambda function through your mapping templates. For example, you may want to re-use the same Lambda function for multiple stages in your API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In the mapping templates that generate the request for the Lambda function, you can use stage variables to pass the table name to Lambda.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Stage variables are not applied to the security definitions section of the API specification. For example, you cannot use different Amazon Cognito user pools for different stages.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">use stage variables.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Setting up an API Gateway Private Integration to the Lambda function&nbsp;</strong></b><span>is incorrect because&nbsp;this is just used to expose your HTTP/HTTPS resources behind an Amazon VPC to allow access to clients outside of your VPC.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Creating environment variables in the Lambda function&nbsp;</strong></b><span>is incorrect because using an environment variable alone is not enough to meet this requirement. You have to integrate your Lambda function with API Gateway by using a stage variable, along with a proper mapping configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Setting up traffic shifting with Lambda Aliases</strong></b><span>&nbsp;is incorrect because&nbsp;this is just like a pointer to a specific Lambda function version.&nbsp;By using aliases, you can access the Lambda function which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. Take note that we are not talking about different versions of the Lambda functions here as the scenario explicitly mentioned that we have passed the configuration parameters from API Gateway to the Lambda function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/amazon-api-gateway-using-stage-variables.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/amazon-api-gateway-using-stage-variables.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-configuration.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/lambda-configuration.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569448,
        "value": "Set up traffic shifting with Lambda Aliases.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569449,
        "value": "Create environment variables in the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569450,
        "value": "Set up an API Gateway Private Integration to the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569451,
        "value": "Use Stage Variables.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 14,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569451,
        "questionId": 391462,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391462,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company decided to re-use the same Lambda function for multiple stages of their API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In order to accomplish this, they instructed the developer to pass configuration parameters to a Lambda function through mapping templates in API Gateway.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST suitable solution that the developer should use to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740300,
    "question": "EC2 instances",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To enable HTTPS connections to your website or application in AWS, you need an&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">SSL/TLS&nbsp;</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">server certificate</strong></b></i><span>. For certificates in a Region supported by AWS Certificate Manager (ACM), it is recommended that you use ACM to provision, manage, and deploy your server certificates. In unsupported Regions, you must use IAM as a certificate manager.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_16.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>ACM is the preferred tool to provision, manage, and deploy your server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a&nbsp;supported Region, you can use ACM to manage server certificates from the console or programmatically</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Use IAM as a certificate manager only when you must support HTTPS connections in a Region that is not&nbsp;supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. Hence, the correct answers are&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Certificate Manager (ACM)&nbsp;</strong></b><span>and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">IAM certificate store.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">A private S3 bucket with versioning enabled</strong></b><span>&nbsp;is incorrect as S3 is not a suitable service to store the SSL certificate. You have to import it to either AWS Certificate Manager (ACM) or IAM certificate store.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Cognito</strong></b><span>&nbsp;is incorrect because&nbsp;this is just a user identity and data synchronization service that helps you securely manage and synchronize app data for your users across their mobile devices. This service can’t be used to store or import your SSL certificates.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">CloudFront</strong></b><span>&nbsp;is incorrect. Although you can upload certificates to CloudFront, it doesn’t mean that you can import third-party SSL certificates on it. If you got your certificate from a third-party CA then you have to import the certificate into ACM or upload it to the IAM certificate store first. You would also not be able to export the certificate that you have loaded in CloudFront nor assign them to your EC2 or ELB instances as it would be tied to a single CloudFront distribution.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/certificate-manager/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/certificate-manager/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569452,
        "value": "CloudFront",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569453,
        "value": "A private S3 bucket with versioning enabled",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569454,
        "value": "Amazon Cognito",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569455,
        "value": "IAM certificate store",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569456,
        "value": "AWS Certificate Manager",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 15,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569455,
        "questionId": 391463,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": false,
        "choiceId": 1569452,
        "questionId": 391463,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391463,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company has a static website running in an Auto Scaling group of EC2 instances which they want to convert as a dynamic e-commerce web portal. One of the requirements is to use HTTPS to improve the security of their portal and also improve their search ranking as a reputable and secure site. A developer recently requested an SSL/TLS certificate from a third-party certificate authority (CA) which is ready to be imported to AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following services can the developer use to safely import the SSL/TLS certificate? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740301,
    "question": "Architectural pattern ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An event-driven architecture uses events to trigger and communicate between decoupled services and is common in modern applications built with microservices. An event is a change in state or an update, like an item being placed in a shopping cart on an e-commerce website. Events can either carry the state (the item purchased, its price, and a delivery address) or events can be identifiers (a notification that an order was shipped).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Event-driven architectures have three key components: event sources/producers, event routers, and event consumers.&nbsp;A producer publishes an event to the router, which filters and pushes the events to consumers. Producer services and consumer services are decoupled, which allows them to be scaled, updated, and deployed independently.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_17.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the scenario, the DynamoDB table is the source of events.&nbsp;Updates made to an inventory item are tracked by DynamoDB Streams. Leveraging an Event-Driven pattern, a Lambda function can immediately be triggered by this change in the stream. The function can then execute the required business logic for inventory replenishment and send a notification to an Amazon SNS topic to promptly notify the business owner about the inventory status. This immediate response ensures timely replenishment actions and notifications.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications&nbsp;</strong></b><span>is incorrect.&nbsp;With this approach, if an item’s stock goes below the threshold immediately after a check, the system won’t be aware of it until the next scheduled check, making it not a suitable pattern for replenishing inventory in a timely manner.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications</strong></b><span>&nbsp;is incorrect because this pattern is designed to process a group of records collectively. It means that immediate action might not be taken when a single item’s stock goes below the threshold. Instead, it would wait for a batch of records to be queued up, making it not ideal for a scenario where timely action on individual stock items is critical.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications&nbsp;</strong></b><span>is incorrect.&nbsp;This pattern is primarily suited for scenarios where one event needs to initiate multiple independent actions in parallel.&nbsp;For instance, converting a video clip in different formats. In contrast, the scenario requires succeeding actions that are dependent: after processing the stock level, a notification is sent if a certain condition is met.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/what-is/eda/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/what-is/eda/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569457,
        "value": "An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569458,
        "value": "A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569459,
        "value": "A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569460,
        "value": "A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 16,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569457,
        "questionId": 391464,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391464,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A small retail business hired a developer to replace their spreadsheet-based inventory tracking system. They want an automated system that does the following:</span></p><ol class=\"Editor__editor-list-ol___2onc5\"><li value=\"1\" class=\"Editor__editor-listitem___EW2Qh Editor__editor-nested-listitem___1xhQS\"><ol class=\"Editor__editor-list-ol___2onc5\"><li value=\"1\" class=\"Editor__editor-listitem___EW2Qh\"><span>Process       an inventory replenishment when a stock level goes below a certain       threshold</span></li><li value=\"2\" class=\"Editor__editor-listitem___EW2Qh\"><span>Send       a notification to the business owner about the inventory and       replenishment status.</span></li></ol></li></ol><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The inventory data has already been migrated to an Amazon DynamoDB table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which architectural pattern should the developer adopt to meet the requirements?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740302,
    "question": "React application",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amplify provides two services:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">-Amplify Hosting</strong></b><span>&nbsp;– provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">-Amplify Studio&nbsp;</strong></b><span>–&nbsp;a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use Amplify Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend with AWS resources, and then connect the two together.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amplify Hosting provides deep integration with Cypress for End-to-End (E2E) testing, allowing developers to generate a UI report for their tests.&nbsp;To add Cypress tests to your application, you can update the build settings in the&nbsp;amplify.yml&nbsp;configuration file, which will enable Amplify to run the tests during the build process.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;– Connect the Github repository to AWS Amplify Hosting</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;– Update the&nbsp;amplify.yml&nbsp;file with appropriate configuration settings for Cypress.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run&nbsp;amplify pull --appId APP_ID --envName ENV_NAME</strong></b><span>&nbsp;is incorrect. This is not necessary, as you will not be recreating the application from scratch.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Include the location of the Cypress configuration file in the&nbsp;aws-exports.js&nbsp;file</strong></b><span>&nbsp;is incorrect. The&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">aws-exports.js</strong></b><span>&nbsp;is simply a configuration file containing information for an Amplify application’s region, user pool, or identity pool. You can’t include build settings for Cypress in this file.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Update the&nbsp;amplifyconfiguration.json&nbsp;with appropriate configuration settings for Cypress</strong></b><span>&nbsp;is incorrect because this file is just the same as&nbsp;aws-exports.js, but for Android and iOS projects.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/mobile/running-end-to-end-cypress-tests-for-your-fullstack-ci-cd-deployment-with-amplify-console/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/mobile/running-end-to-end-cypress-tests-for-your-fullstack-ci-cd-deployment-with-amplify-console/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://amplify-sns.workshop.aws/en/80_e2e_test/00_-cypress.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://amplify-sns.workshop.aws/en/80_e2e_test/00_-cypress.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569461,
        "value": "Update the amplify.yml file with appropriate configuration settings for Cypress.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569462,
        "value": "Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569463,
        "value": "Update the amplifyconfiguration.json with appropriate configuration settings for Cypress.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569464,
        "value": "Include the location of the Cypress configuration file in the aws-exports.js file.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569465,
        "value": "Connect the Github repository to AWS Amplify Hosting",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 17,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569461,
        "questionId": 391465,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391465,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any UI issues before going live, the developer must perform end-to-end (E2E) testing using Cypress.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which combination of actions should the developer take? (Select Two)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740303,
    "question": "CloudWatch",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon CloudWatch agent</strong></b><span>&nbsp;enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-CPU core.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_19.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Metrics are data about the performance of your systems. By default, several services provide free metrics for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring on some resources, such as your Amazon EC2 instances, or publish your own application metrics. Amazon CloudWatch can load all the metrics in your account (both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>However, take note that CloudWatch does not monitor the memory, swap, and disk space utilization of your instances. If you need to track these metrics, you can install a CloudWatch agent in your EC2 instances.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">CloudWatch does not track memory utilization by default.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">The detailed monitoring is not enabled in CloudWatch</strong></b><span>&nbsp;is incorrect because&nbsp;this will just send metric data for your instance to CloudWatch in 1-minute periods, but not including the memory utilization.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">X-Ray Daemon is not installed to the EC2 instances</strong></b><span>&nbsp;is incorrect because X-Ray is primarily used for troubleshooting applications and not to monitor the actual EC2 instances. Even if the X-Ray Daemon is installed and is running in the instance, it will still not send the memory utilization metrics to CloudWatch.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;The&nbsp;.ebextensions/xray-daemon.config&nbsp;file in Elastic Beanstalk is missing</strong></b><span>&nbsp;is incorrect because just like what is mentioned above, X-Ray will not send the memory utilization of the EC2 instance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569466,
        "value": "The detailed monitoring is not enabled in CloudWatch.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569467,
        "value": "CloudWatch does not track memory utilization by default.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569468,
        "value": "X-Ray Daemon is not installed on the EC2 instances.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569469,
        "value": "The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 18,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569469,
        "questionId": 391466,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391466,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the application’s performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in CloudWatch. You can see the CPU utilization of the instances but not the memory utilization.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST likely cause of this issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740304,
    "question": "Online game",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Today’s applications have more demanding requirements than ever before. For example, an online game might start out with just a few users and a very small amount of data. However, if the game becomes successful, it can easily outstrip the resources of the underlying database management system. It is not uncommon for web-based applications to have hundreds, thousands, or millions of concurrent users, with terabytes or more of new data generated per day. Databases for such applications must handle tens (or hundreds) of thousands of reads and writes per second.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><img src=\"https://static.cdn.epam.com/uploads/edd5f87315fbce8b3b6bc0b7376c0f0e/ex3/hint_20.jpg\"></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon</strong></b><span>&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">DynamoDB</strong></b><span>&nbsp;is well-suited for these kinds of workloads. As a developer, you can start with a small amount of provisioned throughput and gradually increase it as your application becomes more popular. DynamoDB scales seamlessly to handle very large amounts of data and very large numbers of users.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon RDS</strong></b><span>&nbsp;is incorrect because this is a type of SQL database, and it can’t scale seamlessly to handle very large amounts of data and the number of users, as compared to DynamoDB. Using Amazon Aurora database could also provide scalability, which may be at par with DynamoDB, but this will entail the additional cost of setting up several replicas in various AWS regions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Redshift</strong></b><span>&nbsp;is incorrect because this is a columnar-based database, which is more appropriate for online analytical processing (OLAP) applications.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon S3</strong></b><span>&nbsp;is incorrect because this is primarily used for object storage and not as a database.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/products/databases/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/products/databases/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569470,
        "value": "Amazon Redshift",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569471,
        "value": "Amazon DynamoDB",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569472,
        "value": "Amazon RDS",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569473,
        "value": "Amazon S3",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 19,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569470,
        "questionId": 391467,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391467,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is working on an online game based on a popular movie, which may have a few users in its first few weeks of release. However, it is expected to grow and reach millions of concurrent users, with terabytes or more of new data generated per day. The database must seamlessly handle hundreds of thousands of reads and writes per second.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following would be the MOST ideal data store to choose for this application?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740305,
    "question": "S3 bucket",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you are getting an&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Access Denied</strong></b><span>&nbsp;error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS KMS key, then you have to confirm that you have permission to perform&nbsp;kms:Decrypt&nbsp;actions on the AWS KMS key that you’re using to encrypt the object.</span></p>",
    "choices": [
      {
        "id": 1569474,
        "value": "The developer does not have the kms:Encrypt permission.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569475,
        "value": "The developer does not have the kms:Decrypt permission.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569476,
        "value": "The maximum size that can be encrypted in KMS is only 100 GB.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569477,
        "value": "The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569478,
        "value": "The AWS CLI S3 commands perform a multipart upload when the file is large.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 20,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569475,
        "questionId": 391468,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569478,
        "questionId": 391468,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391468,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer runs a shell script that uses the&nbsp;aws s3 cp&nbsp;CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server-side encryption with AWS Key Management Service (SSE-KMS). An&nbsp;Access Denied&nbsp;error always shows up whenever the developer uploads a file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following are possible reasons why this issue is happening? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740306,
    "question": "CodeDeploy",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>CodeDeploy provides two deployment type options:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">In-place deployment</strong></b><span>: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Blue/green deployment</strong></b><span>: The behavior of your deployment depends on which compute platform you use:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>–&nbsp;</span><u><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-underline___y38TS\">Blue/green on an EC2/On-Premises compute platform:</strong></b></u><span>&nbsp;The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>–&nbsp;</span><u><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-underline___y38TS\">Blue/green on an AWS Lambda compute platform</strong></b></u><span>: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>–&nbsp;</span><u><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-underline___y38TS\">Blue/green on an Amazon ECS compute platform:</strong></b></u><span>&nbsp;Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>It is also important to note that the CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Therefore, the supported deployment types in CodeDeploy are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;&nbsp;Blue/green deployments to ECS</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– In-place deployments to on-premises servers&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Rolling deployments to ECS&nbsp;</strong></b><span>is incorrect because only blue/green deployment is allowed if you used the AWS CodeDeploy service to deploy the new version of your application to ECS. Take note that in CodeDeploy, only the EC2/On-Premises compute platform can use both in-place deployments and blue/green deployment. For Lambda and ECS, you can only do a blue/green deployment in CodeDeploy. This type of deployment is actually done in Elastic Beanstalk for Multicontainer docker environment which implicitly uses ECS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">In-place deployments to AWS Lambda</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because&nbsp;AWS Lambda compute platform deployments cannot use an in-place deployment type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Blue/green deployments to on-premises servers</strong></b><span>&nbsp;is incorrect because, in CodeDeploy, blue/green deployments only work with Amazon EC2 instances only.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</span></a></p>",
    "choices": [
      {
        "id": 1569479,
        "value": "Rolling deployments to ECS.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569480,
        "value": "In-place deployments to AWS Lambda.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569481,
        "value": "In-place deployments to on-premises servers",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569482,
        "value": "Blue/green deployments to on-premises servers.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569483,
        "value": "Blue/green deployments to ECS.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 21,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569483,
        "questionId": 391469,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": false,
        "choiceId": 1569479,
        "questionId": 391469,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391469,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as their deployment service, which can automate their application deployments on their hybrid cloud architecture.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following deployment types does CodeDeploy support? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740307,
    "question": "Cross-Region Replication ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Cross-region replication&nbsp;(CRR)</strong></b><span>&nbsp;enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. Cross-region replication is enabled with a bucket-level configuration. You add the replication configuration to your source bucket.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To enable the cross-region replication feature in S3, the following items should be met:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– The source and destination buckets must have&nbsp;</span><u><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-underline___y38TS\">versioning</strong></b></u><span>&nbsp;enabled.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– The source and destination buckets must be in different AWS Regions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the most likely root cause for this scenario is that the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">versioning is not enabled in the bucket.&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon S3 Object Lock is enabled in the bucket&nbsp;</strong></b><span>is incorrect because this feature simply enables you to store objects using a write-once-read-many (WORM) model. You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely, but it will not affect your cross-region replication configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">The bucket should be configured as a static web hosting first&nbsp;</strong></b><span>is incorrect because this feature won’t affect the&nbsp;cross-region replication in S3. This is primarily used to make your S3 bucket a static website, as its name implies.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">S3 Transfer Acceleration is not enabled in the bucket</strong></b><span>&nbsp;is incorrect because this just enables fast, easy, and secure transfers of files to and from your bucket. Just like the other options, it does not affect cross-region replication in any way.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr-how-setup.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/dev/crr-how-setup.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569484,
        "value": "S3 Transfer Acceleration is not enabled in the bucket.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569485,
        "value": "Amazon S3 Object Lock is enabled in the bucket.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569486,
        "value": "The bucket should be configured as a static web hosting first.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569487,
        "value": "Versioning is not enabled in the bucket.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 22,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569487,
        "questionId": 391470,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391470,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer has been instructed to configure Cross-Region Replication (CRR) to their S3 bucket as part of the company’s disaster recovery plan. She is using the&nbsp;put-bucket-replication&nbsp;AWS CLI to enable CRR on the bucket but it fails whenever she attempts to issue the command. However, the same command works for the other S3 buckets.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following options is the MOST likely reason for this issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740308,
    "question": " ECS Cluster",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>By default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">task placement strategy</strong></b><span>&nbsp;is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon ECS supports the following task placement strategies:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">binpack</strong></b><span>&nbsp;– Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">random</strong></b><span>&nbsp;– Place tasks randomly.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">spread</strong></b><span>&nbsp;– Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs,&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">instanceId</em></i><span>, or&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">host</em></i><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The spread strategy, contrary to the binpack strategy, tries to put your tasks on as many different instances as possible. It is typically used to achieve high availability and mitigate risks, by making sure that you don’t put all your task-eggs in the same instance-baskets. Spread across Availability Zones, therefore, is the default placement strategy used for services.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When using the spread strategy, you must also indicate a&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">field&nbsp;</em></i><span>parameter. It is used to indicate the&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">bins</em></i><span>&nbsp;that you are considering. The accepted values are&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">instanceID</em></i><span>,&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">host</em></i><span>, or a custom attribute key:value pairs such as&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">attribute:ecs.availability-zone</em></i><span>&nbsp;to balance tasks across zones. There are several AWS attributes that start with the&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">ecs</em></i><span>&nbsp;prefix, but you can be creative and create your own attributes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the task placement configuration which has a value of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">\"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\"</strong></b><span>&nbsp;is correct, because this is using the appropriate strategy for task placement.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The configuration which has a value of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">“field”: “instanceId”, “type”: “spread”</strong></b><span>&nbsp;is incorrect because although it is using a spread task placement strategy, it distributes tasks evenly across all instances and not to Availability Zones.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The configuration which has a value of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">“field”: “memory”, “type”: “binpack”</strong></b><span>&nbsp;is incorrect because this is using a strategy that bin packs tasks based on memory. Take note that the scenario is asking for a configuration which will evenly place the tasks across multiple Availability Zones, and not based on memory.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The configuration which has a value of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">“type”: “random”</strong></b><span>&nbsp;is incorrect because this will just place the tasks on instances randomly. You have to use the spread task placement strategy instead to meet the requirements of the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569488,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>\"placementStrategy\": [\n{\n\"type\": \"random\"\n}\n]</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569489,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>\"placementStrategy\": [\n{\n\"field\": \"attribute:ecs.availability-zone\",\n\"type\": \"spread\"\n}\n]</code></pre></p>",
        "isCorrect": true,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569490,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>\"placementStrategy\": [\n{\n\"field\": \"memory\",\n\"type\": \"binpack\"\n}\n]</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569491,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>\"placementStrategy\": [\n{\n\"field\": \"instanceId\",\n\"type\": \"spread\"\n}\n]</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      }
    ],
    "idx": 23,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569489,
        "questionId": 391471,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391471,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands of investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service disruptions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST suitable&nbsp;placementStrategy&nbsp;configuration that you should use in your task definition?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740309,
    "question": "OpenAPI",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can paste a&nbsp;</span><a href=\"http://swagger.io/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>Swagger</span></a><span>&nbsp;API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below:</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can also import your Swagger definition through the AWS CLI and SDKs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is to</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using CodeDeploy to migrate and deploy the company’s web services to API Gateway&nbsp;</strong></b><span>is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using AWS SAM to migrate and deploy the company’s web services to API Gateway</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Creating models and templates for request and response mappings based on the company’s API definitions&nbsp;</strong></b><span>is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-from-example.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-from-example.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569492,
        "value": "Use CodeDeploy to migrate and deploy the company's web services to API Gateway.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569493,
        "value": "Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569494,
        "value": "Use AWS SAM to migrate and deploy the company's web services to API Gateway.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569495,
        "value": "Create models and templates for request and response mappings based on the company's API definitions.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 24,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569492,
        "questionId": 391472,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391472,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the EASIEST way to accomplish this task?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740310,
    "question": "API Gateway Lambda Authorizer",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API’s methods, API Gateway calls your Lambda authorizer, which takes the caller’s identity as input and returns an IAM policy as output.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>There are two types of Lambda authorizers:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;– A&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">token-based</strong></b><span>&nbsp;Lambda authorizer (also called a TOKEN authorizer) receives the caller’s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;– A&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">request parameter-based</strong></b><span>&nbsp;Lambda authorizer (also called a REQUEST authorizer) receives the caller’s identity in a combination of headers, query string parameters, stageVariables, and $context variables.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function, by using a Cross-Account Lambda Authorizer.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Therefore, the correct answer is to use a&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Request Parameter-based Authorization</strong></b><span>&nbsp;as it uses a combination of headers, query string parameters, stageVariables, and $context variables for authentication.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Cognito User Pools Authorizer&nbsp;</strong></b><span>is incorrect because this&nbsp;is just an alternative to using IAM roles and policies or Lambda authorizers. An Amazon Cognito user pool is primarily used to control who can access your API in Amazon API Gateway using identity token or access token, and not&nbsp;header and query string parameters from the API caller.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Token-based authorization&nbsp;</strong></b><span>is incorrect because this is only suitable if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Cross-Account Lambda Authorizer&nbsp;</strong></b><span>is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid&nbsp;Lambda authorizer type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569496,
        "value": "Amazon Cognito User Pools Authorizer",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569497,
        "value": "Token-based Authorization",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569498,
        "value": "Cross-Account Lambda Authorizer",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569499,
        "value": "Request Parameter-based Authorization",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 25,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569499,
        "questionId": 391473,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391473,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is using API Gateway Lambda Authorizer to securely authenticate the API requests to their web application. The authentication process should be implemented using a custom authorization scheme which accepts header and query string parameters from the API caller.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following methods should the developer use to properly implement the above requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740311,
    "question": "API Caching",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can enable API caching in Amazon API Gateway to cache your endpoint’s responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the&nbsp;Cache-Control: max-age=0&nbsp;header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To grant permission for a client, attach a policy of the following format to an IAM execution role for the user:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><pre data-highlight-language=\"java\"><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"execute-api:InvalidateCache\"\n      ],\n      \"Resource\": [\n        \"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\"\n      ]\n    }\n  ]\n}\n</code></pre><span>Hence, the option which has the same policy shown above is the correct answer since attaching the policy that allows the action&nbsp;\"execute-api:InvalidateCache\"&nbsp;to your IAM execution role will allow the API Gateway execution service to invalidate the cache results for requests on the specified resource (or resources).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the list of options, you have to take a look at the values of the “</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Effect</strong></b><span>” and “</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Action</strong></b><span>” fields.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option which has a policy of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">\"Effect\": \"Deny\", \"Action\": [\"execute-api:InvalidateCache\"]</strong></b><span>&nbsp;is incorrect because this will deny any request to invalidate cache results in API Gateway.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option which has a policy of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">\"Effect\": \"Deny\", \"Action\": [\"execute-api:*\"]</strong></b><span>&nbsp;is incorrect because this will deny all requests&nbsp;to your API Gateway such as API invocation, cache invalidation, and all other actions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option which has a policy of&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">\"Effect\": \"Allow\", \"Action\": [\"execute-api:Invoke\"]&nbsp;</strong></b><span>is incorrect because this will just allow API invocation requests in API Gateway and not cache invalidation requests.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#override-api-gateway-stage-cache-for-method-cache\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#override-api-gateway-stage-cache-for-method-cache</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569500,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"execute-api:*\"\n      ],\n      \"Resource\": [\n        \"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\"\n      ]\n    }\n  ]\n}  \n</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569501,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"execute-api:InvalidateCache\"\n      ],\n      \"Resource\": [\n        \"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\"\n      ]\n    }\n  ]\n}</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569502,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"execute-api:InvalidateCache\"\n      ],\n      \"Resource\": [\n        \"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\"\n      ]\n    }\n  ]\n}\n</code></pre></p>",
        "isCorrect": true,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      },
      {
        "id": 1569503,
        "value": "<p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n      \"execute- api:Invoke\"\n      ],\n      \"Resource\": [\n      \"arn:aws:execute-api:region:account-id:api- id/stage-name/GET/resource-path-specifier\"\n      ]\n   }\n ]\n}\n</code></pre></p>",
        "isCorrect": false,
        "isAdvancedEditor": true,
        "language": "java",
        "responses": []
      }
    ],
    "idx": 26,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569502,
        "questionId": 391474,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391474,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer has enabled API Caching on his application endpoints in Amazon API Gateway. For testing purposes, he wants to fetch the latest data, and not the cache data, whenever he sends a GET request with a&nbsp;Cache-Control: max-age=0&nbsp;header to a specific resource.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following policies will allow him to invalidate the cache for requests?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740312,
    "question": "S3 bucket",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon S3 Transfer Acceleration</strong></b><span>&nbsp;enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Transfer for SFTP&nbsp;</strong></b><span>is incorrect because this is just a fully managed service that enables the transfer of files directly into and out of Amazon S3 using the Secure File Transfer Protocol (SFTP) which is also known as Secure Shell (SSH) File Transfer Protocol.&nbsp;It does not provide a&nbsp;fast, easy, and secure way to transfer files over long distances between your client and your Amazon S3 bucket.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Direct Connect&nbsp;</strong></b><span>is incorrect because you have users all around the world and not just on your on-premises data center. Direct Connect would be too costly and is definitely not suitable for this purpose.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">CloudFront</strong></b><span>&nbsp;is incorrect because&nbsp;this service is primarily used to serve static content and not as a transfer accelerator going to or from Amazon S3. CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569504,
        "value": "CloudFront",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569505,
        "value": "AWS Transfer for SFTP",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569506,
        "value": "AWS Direct Connect",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569507,
        "value": "Amazon S3 Transfer Acceleration",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 27,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569507,
        "questionId": 391475,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391475,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A global financial company has hundreds of users from all over the world that regularly upload terabytes of transactional data to a centralized S3 bucket. You noticed that there are some users from different parts of the globe that take a lot of time to upload their data, which causes delays in the processing. You need to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user’s location.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following features should you use to satisfy the above requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740313,
    "question": "EBS-backed EC2 instance",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If an EBS volume is the root device of an instance, you must&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">stop the instance before you can detach the volume.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The options that say&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">unmount the volume from the OS and then detach</strong></b><span>&nbsp;and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">unmount the volume, stop the instance, and then detach</strong></b><span>&nbsp;are both incorrect because you can’t unmount the root volume on a running instance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you</strong></b><span>&nbsp;is incorrect because unmounting the volume is not managed by AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Reference:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569508,
        "value": "Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569509,
        "value": "Unmount the volume from the OS and then detach.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569510,
        "value": "Unmount the volume, stop the instance, and then detach.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569511,
        "value": "Stop the instance then detach the volume.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 28,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569511,
        "questionId": 391476,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391476,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security vulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms, Trojan horses, or spyware.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What steps would you take to detach the root volume from the compromised EC2 instance?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740314,
    "question": "DynamoDB table",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">write capacity unit</em></i><span>&nbsp;(WCU) represents one write per second, for an item up to 1 KB in size.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in size per second. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require 2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item size. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity units for a transactional write request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Suppose that you want to write 100 items per second to your table, and that the items are 512 bytes in size. Each write requires one provisioned write capacity unit. First,&nbsp;you have to round up your item size to the nearest whole number per KB. Then to determine the write capacity unite per item, you need to divide the item size of the operation by 1 KB. Once you got the value, you just simply have to multiply it with the number of write request per second (1 x 100) hence, the WCU that you should provision is 100.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Here are the 3 steps to get the total number of WCU needed for your application. In this scenario, you are storing&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">100</strong></b><span>&nbsp;items to a DynamoDB table every second, where each item is&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">1.5</strong></b><span>&nbsp;KB in size. To get the WCU, you just have to follow these 3 simple steps below:</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #1&nbsp;Get the Average Item Size round up by 1 KB</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Average Item Size = 1.5 KB =&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">2 KB</strong></b><span>&nbsp;(Rounded up)</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #2&nbsp;Get the&nbsp;</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">WCU per Item</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;by dividing the Average Item Size by 1 KB&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Divide the Average Item Size by 1KB and round up the result:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;= 2 KB / 1 KB</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>&nbsp;=&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">2 WCU per Item</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #3&nbsp;Multiply the WCU per item to the number of items to be written per second&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">=&nbsp;2&nbsp;WCU per item ×&nbsp;100&nbsp;writes per second</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">=&nbsp;200&nbsp;WCU</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">increase the WCU to&nbsp;200</strong></b><span>&nbsp;because your DynamoDB table only got 100 WCU, which is causing the issue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Enabling DynamoDB Accelerator (DAX)&nbsp;</strong></b><span>is incorrect. Although it will significantly improve the read performance of the mobile app, the issue with the throttling write requests will still persist. You have to increase the WCU to properly address the issue in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Implementing database caching with an ElastiCache cluster&nbsp;</strong></b><span>is incorrect because just as mentioned above, the main issue here is the write performance of the DynamoDB table and not its read performance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using strong consistency in the write operations&nbsp;</strong></b><span>is incorrect because a strong consistency model is primarily used for read operations and not for writes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569512,
        "value": "Enable DynamoDB Accelerator (DAX).",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569513,
        "value": "Increase the WCU to 200.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569514,
        "value": "Implement database caching with an ElastiCache cluster.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569515,
        "value": "Use strong consistency in the write operations.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 29,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569512,
        "questionId": 391477,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391477,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second to the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled by DynamoDB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What is the MOST suitable solution in order to rectify this throttling issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740315,
    "question": "SQS queue",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Systems Manager Parameter Store</strong></b><span>&nbsp;provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Parameter Store offers the following benefits and features:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Use a secure, scalable, hosted secrets management service (No servers to manage).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Improve your security posture by separating your data from your code.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Store configuration data and secure strings in hierarchies and track versions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Control and audit access at granular levels.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Configure change notifications and trigger automated actions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Tag parameters individually, and then secure access from different levels, including operational, parameter, Amazon EC2 tag, or path levels.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Reference AWS Secrets Manager secrets by using Parameter Store parameters.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct solution for this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">use AWS Systems Manager Parameter Store as a Secure String Parameter.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Encrypting the database credentials and storing them in an S3 bucket which the Lambda functions can fetch&nbsp;</strong></b><span>is incorrect because it is a security risk to store sensitive database passwords and credentials in S3, even though the data is encrypted. A more suitable and secure way is to use the AWS Secrets Manager or the Systems Manager Parameter Store.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Storing the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions&nbsp;</strong></b><span>is incorrect because even though the credentials will be encrypted, these environment variables will only be used by an individual Lambda function and cannot be shared.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using IAM DB Authentication in RDS to allow encrypted connections from each Lambda function&nbsp;</strong></b><span>is incorrect. While using IAM DB Authentication can significantly enhance security for database connections in AWS environments, it’s more about controlling access rather than managing the sharing of encrypted credentials like a database connection string across multiple applications or services. Additionally, this method is limited to certain types of databases supported by AWS, such as Amazon RDS for MySQL and PostgreSQL, and might not be available for all database engines. Lastly, in the scenario where Lambda functions need to share and securely access a connection string, other AWS services like the Systems Manager Parameter Store would be more appropriate to meet all the specified needs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/</span></a></p>",
    "choices": [
      {
        "id": 1569516,
        "value": "Encrypt the database credentials and store them in an S3 bucket which the Lambda functions can fetch.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569517,
        "value": "Use AWS Systems Manager Parameter Store as a Secure String Parameter.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569518,
        "value": "Store the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569519,
        "value": "Use IAM DB Authentication in RDS to allow encrypted connections from each Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 30,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569517,
        "questionId": 391478,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391478,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Your team is developing a serverless application, which is composed of multiple Lambda functions which process data from an SQS queue and stores the results to an RDS database. To comply with the strict IT policy of the company, you were instructed to configure these functions to share the same connection string that should be properly secured and encrypted.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should you do to protect, encrypt, and share your database credentials in AWS?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740316,
    "question": "DynamoDB database",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">local secondary index</em></i><span>&nbsp;maintains an alternate sort key for a given partition key value. It also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimensions. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A local secondary index (LSI) is “local” in the sense that every partition of an LSI is scoped to a base table partition that has the same partition key value.&nbsp;It lets you query over a single partition, as specified by the partition key value in the query. When you query a local secondary index, you can choose either eventual consistency or strong consistency.&nbsp;Queries or scans on a local secondary index consume read capacity units from the base table. When you write to a table, its local secondary indexes are also updated; these updates consume write capacity units from the base table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Take note that local secondary indexes are created at the same time you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct option in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">add a local secondary index before the table is created.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding a local secondary index after the table has been created&nbsp;</strong></b><span>is incorrect because&nbsp;as mentioned above, you cannot add a local secondary index to an existing table. This index must be created at the same time you create your table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding a global secondary index before the table is created&nbsp;</strong></b><span>is incorrect because&nbsp;a global secondary index (GSI) is primarily used&nbsp;if you want to query over the entire table, across all partitions. GSI only supports eventual consistency and not strong consistency. You have to use a local secondary index instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding a global secondary index&nbsp;after the table has been created</strong></b><i><em class=\"Editor__editor-text-italic___C9n8O\">&nbsp;</em></i><span>is incorrect because you should use a local secondary index (LSI) instead of a&nbsp;global secondary index (GSI) just as mentioned above. Although you can add a GSI to an already existing table, unlike LSI, this option is still not sufficient to meet the specified requirement.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569520,
        "value": "Add a local secondary index before the table is created.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569521,
        "value": "Add a global secondary index before the table is created.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569522,
        "value": "Add a global secondary index after the table has been created.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569523,
        "value": "Add a local secondary index after the table has been created.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 31,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569520,
        "questionId": 391479,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391479,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are developing a Node.js application which uses a DynamoDB database. The architecture should be designed to allow you to query over a single partition of the table, as specified by the partition key value in the query. It should also support both eventually consistent and strongly consistent reads.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should you do to satisfy this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740317,
    "question": "AWS Fargate",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Cache key normalization</strong></b><span>&nbsp;– You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Header manipulation</strong></b><span>&nbsp;– You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a&nbsp;True-Client-IP&nbsp;header to every request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Status code modification and body generation</strong></b><span>&nbsp;– You can evaluate headers and respond back to viewers with customized content.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– URL redirects or rewrites</strong></b><span>&nbsp;– You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Request authorization</strong></b><span>&nbsp;– You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>CloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response).</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the given scenario, by attaching the validation function to the Viewer Request event, requests can be authenticated right when they hit the CloudFront cache and before they are forwarded to your AWS Fargate service.&nbsp;This method not only helps in reducing unnecessary traffic to your Fargate tasks but also improves overall latency for valid requests, as they don’t have to wait behind unauthenticated requests being processed by your backend infrastructure. In addition, because CloudFront Functions operate at the edge locations of AWS’s infrastructure, they are highly scalable and can handle a very high number of requests per second.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function&nbsp;</strong></b><span>is incorrect. This option doesn’t eliminate the problem of unauthenticated requests reaching the backend infrastructure. All requests, regardless of authentication, will still need to traverse the network to reach the ALB and Lambda, which is not operationally efficient and could increase latency.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Enable auto-scaling on the Fargate tasks&nbsp;</strong></b><span>is incorrect. While auto-scaling could help handle the increased load by adding more tasks as demand increases, it is a reactive measure and not a long-term solution. It doesn’t prevent unauthenticated requests from consuming resources on Fargate.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Create a Lambda&nbsp;function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution&nbsp;</strong></b><span>is incorrect.&nbsp;The Origin Response event is triggered after the request has been processed by your origin (in this case, the ALB) and the origin has returned a response to CloudFront. Validating the JWT at this stage would not reduce the number of unauthenticated requests reaching your Fargate service. It’s too late in the request processing flow to prevent unauthenticated requests from consuming resources on your backend infrastructure. Moreover, this could also introduce errors in the CloudFront flow.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-validate-token.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-validate-token.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569524,
        "value": "Enable auto-scaling on the Fargate tasks.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569525,
        "value": "Create a Lambda function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569526,
        "value": "Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569527,
        "value": "Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 32,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569526,
        "questionId": 391480,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391480,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which solution would reduce the load on the Fargate tasks in the most operationally efficient manner?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740318,
    "question": "Docker ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Elastic Container Service (Amazon ECS)</strong></b><span>&nbsp;is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">fine-grained</strong></b><span>&nbsp;control for custom application architectures.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">ECS.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Elastic Beanstalk&nbsp;</strong></b><span>is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS SAM&nbsp;</strong></b><span>is incorrect because&nbsp;the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">EC2</strong></b><span>&nbsp;is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/ecs/faqs/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/ecs/faqs/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569528,
        "value": "Elastic Beanstalk",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569529,
        "value": "AWS SAM",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569530,
        "value": "EC2",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569531,
        "value": "ECS",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 33,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569531,
        "questionId": 391481,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391481,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following services is the MOST suitable one to use?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740319,
    "question": "Lamda function",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Lambda</strong></b><span>&nbsp;counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on&nbsp;the total number of requests processed across all of your Lambda functions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to:</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">Increase the memory configuration of your function.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Configuring the concurrency limit of your function to be the same as the account level concurrency limit&nbsp;</strong></b><span>is incorrect because this configuration is primarily used for managing the number of simultaneous executions of your function as well as the capacity reservations for that concurrency level. This will just limit the number of simultaneous executions of your function and not increase the allocated CPU. In addition,&nbsp;AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. This means that you cannot configure the limit to be exactly the same as the account level limit.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Manually configuring the CPU settings of the Lambda function to the maximum value</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because, in the first place, you cannot manually configure&nbsp;the CPU settings of your function. You have to increase the memory configuration of your function instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use Lambda layers to optimize the performance of the Lambda function</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect. Lambda Layers is just an archive for any external dependencies or custom runtimes that you want to share between Lambda functions. You can speed up the deployment of your codes by moving external dependencies to a layer, however, this has a negligible impact on performance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/lambda/pricing/</span></a></p>",
    "choices": [
      {
        "id": 1569532,
        "value": "Use Lambda layers to optimize the performance of the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569533,
        "value": "Configure the concurrency limit of your function to be the same as the account level concurrency limit.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569534,
        "value": "Increase the memory configuration of your function.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569535,
        "value": "Manually configure the CPU settings of the Lambda function to the maximum value.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 34,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569533,
        "questionId": 391482,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391482,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>There is a requirement to improve the performance of your serverless application in AWS by increasing the allocated CPU available for your Lambda functions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST appropriate solution that you should implement to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740320,
    "question": "DynamoDB table ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue&nbsp;Query&nbsp;or&nbsp;Scan&nbsp;requests against these indexes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">secondary index</strong></b></i><span>&nbsp;is a data structure that contains a subset of attributes from a table, along with an alternate key to support&nbsp;Query&nbsp;operations. You can retrieve data from the index using a&nbsp;Query, in much the same way as you use&nbsp;Query&nbsp;with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered “global” because queries on the index can span all of the data in the base table, across all partitions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn’t even need to have the same key schema as a table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is to add a</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">Global Secondary Index</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Sparse index</strong></b><span>&nbsp;is incorrect because parse indexes are only useful for queries over a small subsection of a table.&nbsp;For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn’t appear in every table item, the index is said to be&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">“sparse”</em></i><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Local Secondary Index&nbsp;</strong></b><span>is incorrect because this is used for queries which use the same partition key value, and in addition, you can’t add this index to an already existing table. A local secondary&nbsp;index has the same partition key as the base table, but has a different sort key. It is “local” in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Primary Index&nbsp;</strong></b><span>is incorrect because this one actually refers to the partition key, which is the&nbsp;FighterId&nbsp;attribute in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569536,
        "value": "Local Secondary Index",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569537,
        "value": "Primary Index",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569538,
        "value": "Sparse Index",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569539,
        "value": "Global Secondary Index",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 35,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569539,
        "questionId": 391483,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391483,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A mobile game has a DynamoDB table named&nbsp;AbcScores&nbsp;which keeps track of the users and their respective scores. Each item in the table is identified by the&nbsp;FighterId&nbsp;attribute as its partition key and the&nbsp;FightTitle&nbsp;attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named&nbsp;AbcTopScores&nbsp;and&nbsp;AbcDateTime&nbsp;attributes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which type of index should the developer add in the table to speed up queries on non-key attributes?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740321,
    "question": "NoSQL database",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>One&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">read request unit</em></i><span>&nbsp;represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB × 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">To get the number of RCU required to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to do the following steps:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #1&nbsp;Get the Average Item Size by rounding up to 4 KB</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>= 3.5 KB = 4 KB (rounded up)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #2&nbsp;Get the&nbsp;</strong></b><u><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-underline___y38TS\">RCU per Item</strong></b></u><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;by dividing the Average Item Size by 8 KB</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>=</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;4 KB / 8 KB</strong></b><span> =&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">0.5&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Step #3&nbsp;Multiply the RCU per item to the number of items to be written per second</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>=</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;</strong></b><span>150 x 0.5</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;</strong></b><span> =&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">75&nbsp;eventually consistent read requests</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is&nbsp;</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">75.</strong></b></i></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">150</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">300</strong></b><span>&nbsp;is incorrect because this is the value for transactional read requests based on the given RCU.&nbsp;Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">600</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this is the value for transactional&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">write</strong></b><span>&nbsp;requests, which is irrelevant since only the RCU is provided in the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads</span></a></p>",
    "choices": [
      {
        "id": 1569540,
        "value": "150",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569541,
        "value": "75",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569542,
        "value": "600",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569543,
        "value": "300",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 36,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569541,
        "questionId": 391484,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391484,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of 3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>How may Read Capacity Units (RCU) should the developer provision to the table?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740322,
    "question": "DynamoDB database",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To create, update, or delete an item in a DynamoDB table, use one of the following operations:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>- PutItem</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>- UpdateItem</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>- DeleteItem</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key (partition key and sort key), you must supply a value for the partition key and a value for the sort key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To return the number of write capacity units consumed by any of these operations, set the&nbsp;ReturnConsumedCapacity&nbsp;parameter to one of the following:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">TOTAL</strong></b><span>&nbsp;— returns the total number of write capacity units consumed.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">INDEXES</strong></b><span>&nbsp;— returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">NONE</strong></b><span>&nbsp;— no write capacity details are returned. (This is the default.)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">add the&nbsp;ReturnConsumedCapacity&nbsp;parameter with a value of&nbsp;INDEXES&nbsp;in every write request</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding the&nbsp;ReturnValues&nbsp;parameter with a value of&nbsp;INDEXES&nbsp;in every write request</strong></b><span>&nbsp;is incorrect because you should use a&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">ReturnConsumedCapacity&nbsp;</em></i><span>parameter instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding the&nbsp;ReturnConsumedCapacity&nbsp;parameter with a value of&nbsp;TOTAL&nbsp;in every write request&nbsp;</strong></b><span>is incorrect because this will not return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation just as what is required by the application. You have to use&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">INDEXES</em></i><span>&nbsp;instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding the&nbsp;ReturnValues&nbsp;parameter with a value of&nbsp;TOTAL&nbsp;in every write request&nbsp;</strong></b><span>is incorrect because&nbsp;you should use a&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">ReturnConsumedCapacity&nbsp;</em></i><span>parameter instead. In addition, the value of the parameter is also incorrect as it doesn’t return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters</span></a></p>",
    "choices": [
      {
        "id": 1569544,
        "value": "Add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569545,
        "value": "Add the ReturnValues parameter with a value of INDEXES in every write request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569546,
        "value": "Add the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569547,
        "value": "Add the ReturnValues parameter with a value of TOTAL in every write request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 37,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569544,
        "questionId": 391485,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391485,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write request should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should be done to accomplish this feature?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740323,
    "question": "AWS CodeDeploy",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions regarding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the content and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For ECS deployments, the&nbsp;resources&nbsp;section specifies the Amazon ECS service to deploy and has the following structure:</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Out of these properties, only the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">TaskDefinition,&nbsp;ContainerName,&nbsp;ContainerPort&nbsp;</strong></b><span>are required by CodeDeploy for ECS deployments.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">TaskDefinition&nbsp;</strong></b><span>&nbsp;– This is the task definition for the Amazon ECS service to deploy. It is specified with the ARN of the task definition.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">ContainerName&nbsp;</strong></b><span>– This is the name of the Amazon ECS container that contains your Amazon ECS application. It must be a container specified in your Amazon ECS task definition.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">ContainerPort&nbsp;</strong></b><span>–&nbsp;This is the port on the container where traffic will be routed.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;TaskDefinition</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;ContainerName</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;ContainerPort</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">targetversion&nbsp;</strong></b><span>is incorrect. This property is specifically related to Lambda deployments when using CodeDeploy. It specifies which version of a Lambda function to route traffic to. However, for ECS deployments, it’s not a required property in the AppSpec file.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">alias</strong></b><span>&nbsp;is incorrect because this property&nbsp;is only relevant to Lambda deployments using CodeDeploy. In AWS Lambda, an alias is a pointer to a specific Lambda function version.&nbsp;In the context of ECS deployments, this property is not required in the AppSpec file.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">NetworkConfiguration&nbsp;</strong></b><span>is incorrect.&nbsp;While it can be included in the AppSpec file for ECS deployments using CodeDeploy, it’s just optional. If not specified, the ECS service’s current network configuration is used.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-ecs\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-ecs</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569548,
        "value": "targetversion",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569549,
        "value": "TaskDefinition",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569550,
        "value": "NetworkConfiguration",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569551,
        "value": "ContainerName",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569552,
        "value": "alias",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569553,
        "value": "ContainerPort",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 38,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569553,
        "questionId": 391486,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569551,
        "questionId": 391486,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569549,
        "questionId": 391486,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391486,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is writing the AppSpec file to instruct CodeDeploy on how to handle the deployment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which sets of properties are REQUIRED in the&nbsp;resources&nbsp;section to successfully deploy the microservice? (Select THREE)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740324,
    "question": "Amazon Kinesis Data Stream ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>There are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Consider a producer that experiences a network-related timeout after it makes a call to&nbsp;PutRecord, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have written to retry the call with the same data. If both&nbsp;PutRecord&nbsp;calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">embed a primary key within the record</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>to remove duplicates later when processing.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Adding more shards</strong></b><span>&nbsp;is incorrect because this is not a suitable solution for handling duplicate records in the Kinesis data stream. This is primarily used to increase the rate of data flowing through the stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Splitting shards of the data stream&nbsp;</strong></b><span>is incorrect because&nbsp;this is used to increase the capacity of the stream and not to avoid any duplicate data.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Merging shards of the data stream</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this is primarily used to make better use of the unused capacity in the stream and to save on costs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</span></a></p>",
    "choices": [
      {
        "id": 1569554,
        "value": "Merge shards of the data stream.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569555,
        "value": "Split shards of the data stream.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569556,
        "value": "Embed a primary key within the record.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569557,
        "value": "Add more shards.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 39,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569556,
        "questionId": 391487,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391487,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A clickstream application uses Amazon Kinesis Data Stream for real-time processing.&nbsp;PutRecord&nbsp;API calls are being used by the producer to send data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in sending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of the consumers.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following should you implement to resolve this issue?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740325,
    "question": "ECS",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Cluster queries</strong></b><span>&nbsp;are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata.&nbsp;You can add custom metadata to your container instances, known as&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">attributes</em></i><span>. Each attribute has a name and an optional string value. You can use the built-in attributes provided by Amazon ECS or define custom attributes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>After you have defined a group of container instances, you can customize Amazon ECS to place tasks on container instances based on group. Running tasks manually is ideal in certain situations. For example, suppose that you are developing a task but you are not ready to deploy this task with the service scheduler. Perhaps your task is a one-time or periodic batch job that does not make sense to keep running or restart when it finishes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct ECS feature which provides you with expressions that you can use to group container instances by a specific attribute is&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Cluster Query Language</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Task Group&nbsp;</strong></b><span>is incorrect because this is just a set of related tasks. This does not provide expressions that enable you to group objects. All tasks with the same task group name are considered as a set when performing spread placement.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Task Placement Constraint&nbsp;</strong></b><span>is incorrect because it is just a rule that is considered during task placement.&nbsp;Although it uses cluster queries when you are placing tasks on container instances based on a specific expression, it does not provide the actual expressions which are used to group those container instances.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Task Placement Strategies&nbsp;</strong></b><span>is incorrect because&nbsp;this is just an algorithm for selecting instances for task placement or tasks for termination.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569558,
        "value": "Task Placement Constraints",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569559,
        "value": "Task Placement Strategies",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569560,
        "value": "Task Groups",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569561,
        "value": "Cluster Query Language",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 40,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569559,
        "questionId": 391488,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391488,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is designing an application which will be hosted in ECS and uses an EC2 launch type. You need to group your container instances by certain attributes such as Availability Zone, instance type, or custom metadata. After you have defined a group of container instances, you will need to customize Amazon ECS to place tasks on container instances based on the group you specified.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following ECS features provides you with expressions that you can use to group container instances by a specific attribute?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740326,
    "question": "Elastic Beanstalk",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">ElasticBeanstalk</strong></b><span>, you can choose from a variety of deployment methods:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">All at once</strong></b><span>&nbsp;– Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Rolling</strong></b><span>&nbsp;– Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment’s capacity by the number of instances in a batch.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Rolling with additional batch</strong></b><span>&nbsp;– Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Immutable</strong></b><span>&nbsp;– Deploy the new version to a fresh group of instances by performing an&nbsp;immutable update.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Blue/Green</strong></b><span>&nbsp;– Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">rolling deployment with an additional batch</strong></b><span>. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Immutable environment updates are an alternative to&nbsp;rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment’s load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence,&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Immutable</strong></b><span>&nbsp;and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Rolling with additional batch</strong></b><span>&nbsp;are the correct deployment methods to be used in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">All at once</strong></b><span>&nbsp;is incorrect because this will deploy the new version to all existing instances immediately and will not create new EC2 instances. Hence, it is possible that there would be a degradation of the service since some instances would be unavailable during the deployment process.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Rolling</strong></b><span>&nbsp;is incorrect because this will deploy the new version in batches only to existing instances, without provisioning new resources. The compute capacity of the environment would still be compromised in this method.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Canary&nbsp;</strong></b><span>is incorrect because this type of deployment method is not readily available in Elastic Beanstalk, but primarily to Lambda.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569562,
        "value": "Rolling",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569563,
        "value": "Canary",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569564,
        "value": "All at once",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569565,
        "value": "Rolling with additional batch",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569566,
        "value": "Immutable",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 41,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569565,
        "questionId": 391489,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569566,
        "questionId": 391489,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391489,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources maintain full capacity during deployments to avoid any degradation of the service or possible down time.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740327,
    "question": "SQS",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Delay queues are similar to visibility timeouts because both features make messages unavailable to consumers for a specific period of time. The difference between the two is that, for delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To set delay seconds on individual messages rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer’s DelaySeconds value instead of the delay queue’s DelaySeconds value.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to use a&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Delay Queue</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Short Polling&nbsp;</strong></b><span>is incorrect because this is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Visibility Timeouts&nbsp;</strong></b><span>is incorrect because, with this configuration,&nbsp;a message is hidden only after it is consumed from the queue, and not before. Take note that the difference between the two is that, for delay queues, a message is hidden when it is first added to the queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Long Polling</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because this just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">when there are no messages available for a ReceiveMessage request</em></i><span>) and false empty responses (</span><i><em class=\"Editor__editor-text-italic___C9n8O\">when messages are available but aren’t included in a response</em></i><span>).</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-delay-queue.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-delay-queue.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569567,
        "value": "Long Polling",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569568,
        "value": "Visibility Timeouts",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569569,
        "value": "Short Polling",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569570,
        "value": "Delay Queue",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 42,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569568,
        "questionId": 391490,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391490,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>There is a requirement to postpone the delivery of new messages to an SQS queue for a number of seconds. You must configure the queue to ensure that any messages that you send remain invisible to consumers for a duration of time specified.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following SQS feature should you use to meet this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740328,
    "question": "Amazon Kinesis Data Streams",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can use an&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Lambda function</strong></b><span>&nbsp;to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">Concurrent executions</strong></b></i><span>&nbsp;refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard’s events in sequence.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is that:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">there will be at most 100 Lambda function invocations running concurrently.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the Lambda function has 500 concurrent executions</strong></b><span>&nbsp;is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that&nbsp;the number of shards is the unit of concurrency for the function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards</strong></b><span>&nbsp;is incorrect because, by default,&nbsp;AWS Lambda will automatically scale the function’s concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function</strong></b><span>&nbsp;is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/scaling.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569571,
        "value": "The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569572,
        "value": "There will be at most 100 Lambda function invocations running concurrently.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569573,
        "value": "The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569574,
        "value": "The Lambda function has 500 concurrent executions.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 43,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569574,
        "questionId": 391491,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391491,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following statements are TRUE regarding this scenario?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740329,
    "question": "EC2 instances",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Unlike standard queues, FIFO queues don’t introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. If you retry the&nbsp;SendMessage&nbsp;action within the 5-minute deduplication interval, Amazon SQS doesn’t introduce any duplicates into the queue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To configure deduplication, you must do one of the following:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Enable content-based deduplication. This instructs Amazon SQS to use a SHA-256 hash to generate the message deduplication ID using the body of the message – but not the attributes of the message.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Explicitly provide the message deduplication ID (or view the sequence number) for the message.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The message deduplication ID is the token used for deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren’t delivered during the 5-minute deduplication interval.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Message deduplication applies to an entire queue, not to individual message groups. Amazon SQS continues to keep track of the message deduplication ID even after the message is received and deleted.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is&nbsp;to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication</strong></b><span>&nbsp;is incorrect. Although the use of FIFO queue is valid, it is wrong to disable the content-based deduplication. This should be enabled to avoid duplicate messages in the queue.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a Standard Queue and providing the Message Group ID for each message&nbsp;</strong></b><span>is incorrect because you should use a FIFO queue instead to avoid duplicate messages.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a Standard Queue and providing the Message Deduplication ID for each message</strong></b><span>&nbsp;is incorrect. Although it is a valid answer to provide the Message Deduplication ID, this feature can’t be enabled for Standard Queues. You have to use the FIFO queues instead for this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569575,
        "value": "Use a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569576,
        "value": "Use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569577,
        "value": "Use a Standard Queue and provide the Message Group ID for each message.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569578,
        "value": "Use a Standard Queue and provide the Message Deduplication ID for each message.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 44,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569578,
        "questionId": 391492,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391492,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are managing an application which is composed of an SQS queue and an Auto Scaling group of EC2 instances. Recently, your customers are complaining that there are a lot of incidents where their orders are being erroneously sent twice.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should you do to rectify this problem?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740330,
    "question": "KMS key",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you encrypt your data, it is protected, but you have to protect your encryption key. One strategy is to encrypt it.&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Envelope encryption</strong></b><span>&nbsp;is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the root</span><i><em class=\"Editor__editor-text-italic___C9n8O\">&nbsp;key</em></i><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS KMS</strong></b><span>&nbsp;helps you to protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>It is recommended that you use the following pattern to encrypt data locally in your application:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>1. Use the&nbsp;GenerateDataKey&nbsp;operation to get a data encryption key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>2. Use the plaintext data key (returned in the&nbsp;Plaintext&nbsp;field of the response) to encrypt data locally, then erase the plaintext data key from memory.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>3. Store the encrypted data key (returned in the&nbsp;CiphertextBlob&nbsp;field of the response) alongside the locally encrypted data.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">decrypt</strong></b><span>&nbsp;data locally:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">1. Use the&nbsp;</strong></b><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html\" class=\"Editor__editor-link___3vl2C\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Decrypt</strong></b></a><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">2. Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the Decrypt operation to decrypt the plaintext data key&nbsp;</strong></b><span>is incorrect because there is no need to decrypt a&nbsp;plaintext, or ‘unencrypted’, data key. The&nbsp;correct way is to use the Decrypt operation to decrypt the encrypted data key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory</strong></b><span>&nbsp;is incorrect. Although this is a valid option for the encryption process, you still must erase the plaintext data key instead of the encrypted one. Take note that you are asked to choose valid and secure steps as per the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because you cannot decrypt data using an encrypted data key. You have to decrypt the encrypted data key first and use the plaintext copy of the data key to decrypt the files.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569579,
        "value": "Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569580,
        "value": "Use the Decrypt operation to decrypt the encrypted data key.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569581,
        "value": "Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569582,
        "value": "Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569583,
        "value": "Use the Decrypt operation to decrypt the plaintext data key.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 45,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569580,
        "questionId": 391493,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569579,
        "questionId": 391493,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391493,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is working on an application that will process files encrypted with a data key generated from a KMS key. The application needs to decrypt the files locally before it can proceed with the processing of the files.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following are valid and secure steps in decrypting data? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740331,
    "question": "CloudFront ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Lambda&nbsp;lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– After CloudFront receives a request from a viewer (viewer request)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Before CloudFront forwards the request to the origin (origin request)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– After CloudFront receives the response from the origin (origin response)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Before CloudFront forwards the response to the viewer</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the given scenario, you can use Lambda&nbsp;to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin, which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers in this scenario are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Customize the content that the CloudFront web distribution delivers to your users using Lambda, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user</strong></b><span>&nbsp;is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will&nbsp;improve the performance of the application with minimal cost.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance</strong></b><span>&nbsp;is incorrect. Although setting up multiple VPCs across various regions which&nbsp;are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda&nbsp;instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Add a&nbsp;Cache-Control max-age&nbsp;directive to your objects in CloudFront and specify the longest practical value for&nbsp;max-age&nbsp;to increase the cache hit ratio of your CloudFront distribution</strong></b><span>&nbsp;is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the slow authentication process of your global users and not just the caching of the static objects.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569584,
        "value": "Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569585,
        "value": "Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569586,
        "value": "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569587,
        "value": "Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569588,
        "value": "Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 46,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569585,
        "questionId": 391494,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": false,
        "choiceId": 1569588,
        "questionId": 391494,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391494,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a number of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also getting HTTP 504 errors which is why the developer was instructed to fix this problem immediately.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740332,
    "question": "Amazon API Gateway",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon API Gateway</strong></b><span>&nbsp;is a fully managed service for creating, deploying, and managing APIs at scale. It is an intermediary between clients and backend services, offering features like authorization, traffic management, and monitoring. API Gateway integrates with Amazon CloudWatch to provide metrics that help diagnose performance issues, analyze traffic patterns, and ensure reliable operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">API Gateway metrics</strong></b><span>&nbsp;include&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Latency</strong></b><span>, which measures the total time taken to process a request, and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">IntegrationLatency</strong></b><span>, which tracks the time API Gateway spends communicating with the backend service. Metrics like&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">4XXError</strong></b><span>&nbsp;and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">5XXError</strong></b><span>&nbsp;provide insights into client-side and server-side errors, helping pinpoint request or integration failures.&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Count</strong></b><span>&nbsp;tracks the total requests, offering visibility into traffic patterns and usage trends. Additionally,&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">CacheHitCount</strong></b><span>&nbsp;and&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">CacheMissCount</strong></b><span>&nbsp;are useful for understanding the effectiveness of caching; high cache hits reduce backend load and improve response times, while cache misses indicate requests that bypassed the cache and were processed by the backend.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>By monitoring these metrics in CloudWatch and setting alarms on critical thresholds, organizations can optimize API performance, troubleshoot issues, and maintain high availability. For more insights, enabling logging in CloudWatch Logs can provide detailed data for debugging and root cause analysis.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;IntegrationLatency.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">–&nbsp;Latency.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">4XXError</strong></b><span>&nbsp;is incorrect. This metric primarily captures client-side errors, such as unauthorized access or invalid requests, and does not reflect delays or timeouts in API Gateway. Since the scenario focuses on analyzing the root cause of timeouts during high traffic, monitoring client-side errors is simply not relevant for identifying latency issues.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Count</strong></b><span>&nbsp;is incorrect. While this metric tracks the total number of requests received by API Gateway, it only provides insights into traffic volume. It does not indicate latency, processing delays, or timeouts. Although it is useful for understanding traffic trends, it is not directly helpful in diagnosing performance issues.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">5XXError</strong></b><span>&nbsp;is incorrect because this metric tracks server-side errors, such as integration failures or configuration issues, which are not evident in this scenario. The Lambda function completes within its specified time, and delays are observed at the API Gateway. Thus, monitoring server-side errors would simply not address the identified latency problem.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569589,
        "value": "Latency",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569590,
        "value": "Count",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569591,
        "value": "IntegrationLatency",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569592,
        "value": "5XXError",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569593,
        "value": "4XXError",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 47,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569591,
        "questionId": 391495,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": true,
        "choiceId": 1569589,
        "questionId": 391495,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391495,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company is developing a real-time analytics platform that allows users to submit data through the Amazon API Gateway. The data is processed by AWS Lambda and stored in Amazon S3. However, during high-traffic periods, users experience timeouts despite the Lambda function completing tasks on time.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The company needs to analyze API Gateway metrics in Amazon CloudWatch to determine the cause of these timeouts.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which two API Gateway metrics in Amazon CloudWatch should be reviewed to troubleshoot the delay issues? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740333,
    "question": "AWS Elastic Beanstalk t",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it’s important to understand the differences in order to determine the most useful approach for you.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>It is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>-The ability to send traces to multiple different tracing backends without having to re-instrument your code</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>-Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>-Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Conversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>-A tightly integrated single-vendor solution</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>-Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://your_aws_account_id.signin.aws.amazon.com/console/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://Your_AWS_Account_ID.signin.aws.amazon.com/console/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://your_alias.signin.aws.amazon.com/console/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://Your_Alias.signin.aws.amazon.com/console/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following&nbsp;create-account-alias&nbsp;command creates the alias&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">abc</strong></b><span>&nbsp;for your AWS account:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>aws iam create-account-alias --account-alias abc</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, for this scenario, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the&nbsp;https://finance-dept.signin.aws.amazon.com/console&nbsp;sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the&nbsp;https://061218980612.aws.signin.amazon.com/console&nbsp;sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls</strong></b><span>&nbsp;is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the&nbsp;https://finance-dept.aws.signin.amazon.com/console&nbsp;sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls&nbsp;</strong></b><span>is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use the&nbsp;https://finance-dept.aws.amazon.com/console&nbsp;sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls</strong></b><span>&nbsp;is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn’t have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/cli/latest/reference/iam/create-account-alias.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/cli/latest/reference/iam/create-account-alias.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569594,
        "value": "Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569595,
        "value": "Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569596,
        "value": "Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569597,
        "value": "Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 48,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569594,
        "questionId": 391496,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391496,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the&nbsp;iam create-account-alias --account-alias finance-dept&nbsp;AWS CLI command to create a user-friendly identifier for the finance department.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following options is the MOST suitable solution that the developer implements?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740334,
    "question": "DynamoDB table.",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Optimistic locking</strong></b><span>&nbsp;is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. Optimistic concurrency depends on checking a value upon save to ensure that it has not changed. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>By default, the DynamoDB write operations (PutItem,&nbsp;UpdateItem,&nbsp;DeleteItem) are&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">unconditional</em></i><span>: each of these operations will overwrite an existing item that has the specified primary key.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a&nbsp;PutItem&nbsp;operation to succeed only if there is not already an item with the same primary key. Or you could prevent an&nbsp;UpdateItem&nbsp;operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the AWS SDK for PHP, there is a&nbsp;PessimisticLockingStrategy&nbsp;class for DynamoDB. This locking strategy uses pessimistic locking (similar to how the native PHP session handler works) to ensure that sessions are not edited while another process is reading/writing to it. Pessimistic locking can be expensive and can increase latencies, especially in cases where the user can access the session more than once at the same time (e.g. ajax, iframes, or multiple browser tabs).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence,&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">using a combination of optimistic locking and conditional writes&nbsp;</strong></b><span>is the correct answer in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a combination of pessimistic concurrency and conditional writes&nbsp;</strong></b><span>is incorrect as this will just prevent a value from being updated by locking the item or row in the database.&nbsp;This can block users from reading, updating, or deleting an entry depending on the lock type which is not suitable for the&nbsp;multithreaded application. You have to use optimistic locking strategy and conditional writes instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using batch operations&nbsp;</strong></b><span>is incorrect because this will just reduce the number of network round trips when reading or writing multiple items from your application to DynamoDB. This will not improve the concurrency of your multithreaded application.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using atomic counters</strong></b><span>&nbsp;is incorrect because this&nbsp;is just a numeric attribute that is unconditionally incremented without interfering with other write requests. The more suitable solution to use in this solution is conditional writes.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.htm</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.DynamoDb.Session.LockingStrategy.PessimisticLockingStrategy.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.DynamoDb.Session.LockingStrategy.PessimisticLockingStrategy.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569598,
        "value": "Use optimistic locking and conditional writes.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569599,
        "value": "Use atomic counters.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569600,
        "value": "Use pessimistic locking and conditional writes.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569601,
        "value": "Use batch operations.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 49,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569600,
        "questionId": 391497,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391497,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is designing a multi-threaded e-commerce application that will be reading and writing data on a DynamoDB table. There will be a lot of people who will use the application to update the price of items in the table at the same time. The application should prevent an update operation from modifying an item if one of its attributes has a certain value.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the most suitable solution that the developer should use in this application?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740335,
    "question": "Dead Letter Queue",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Any Lambda function invoked&nbsp;asynchronously&nbsp;is retried twice before the event is discarded. If the retries fail and you’re unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>AWS Lambda directs events that cannot be processed to the specified Amazon SNS topic or Amazon SQS queue. Functions that don’t specify a DLQ will discard events after they have exhausted their retries.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">specify the Amazon Resource Name&nbsp;of the SQS Queue in the Lambda function’s&nbsp;DeadLetterConfig&nbsp;parameter</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Specifying the AWS Service Namespace of the SQS Queue in the&nbsp;AWS::Lambda::Function&nbsp;resource of the CloudFormation template that you’ll use for deploying the function&nbsp;</strong></b><span>is incorrect because you have to use the&nbsp;Amazon Resource Name (ARN) of the SQS Queue and not the service namespace. Moreover, you have to configure this in the Lambda function’s&nbsp;DeadLetterConfig&nbsp;parameter&nbsp;and not in the CloudFormation template.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Specifying the AWS Service Namespace of the SQS Queue in the Lambda function’s&nbsp;DeadLetterConfig&nbsp;parameter&nbsp;</strong></b><span>is incorrect because you have to specify the ARN of the queue and not its service namespace.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Specifying the Amazon Resource Name of the SQS Queue in the&nbsp;Transform&nbsp;section of the AWS SAM template that you’ll use for deploying the function&nbsp;</strong></b><span>is incorrect. Although it is right to use the Amazon Resource Name of the SQS Queue, you still have to set this in the Lambda function’s&nbsp;&nbsp;DeadLetterConfig&nbsp;parameter&nbsp;and not on the AWS SAM template.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/dlq.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/dlq.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569602,
        "value": "Specify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you’ll use for deploying the function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569603,
        "value": "Specify the AWS Service Namespace of the SQS Queue in the Lambda function’s DeadLetterConfig parameter.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569604,
        "value": "Specify the Amazon Resource Name of the SQS Queue in the Lambda function’s DeadLetterConfig parameter.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569605,
        "value": "Specify the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you’ll use for deploying the function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 50,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569603,
        "questionId": 391498,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391498,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the function must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the correct way to implement a DLQ in Lambda?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740336,
    "question": " Amazon Kinesis Data Stream",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon Kinesis Data Streams</strong></b><span>&nbsp;is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds to your Amazon Kinesis applications, and those applications will receive data records in the order they were generated.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>One approach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide more additional capacity than you actually need and, therefore, create unnecessary costs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can also use metrics to determine which are your “hot” or “cold” shards, that is, shards that are receiving much more data or much less data than expected. You could then&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">selectively split the hot shards to increase capacity</strong></b><span>&nbsp;for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Split every shard in the stream.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Upgrade the instance type of the EC2 instances&nbsp;</strong></b><span>is incorrect. Although it will improve the processing time of the data in the stream, it will not increase the capacity of the stream itself. You have to reshard the stream in order to increase or decrease its capacity, and not just upgrade the EC2 instances which process the data in the stream.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Merge every shard in the stream</strong></b><span>&nbsp;is incorrect because&nbsp;merging shards will actually decrease the capacity of the stream rather than increase it. This is only useful if you want to save costs or if the data stream is underutilized, which are both not indicated in the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream</strong></b><i><em class=\"Editor__editor-text-italic___C9n8O\">&nbsp;</em></i><span>is incorrect because Data Firehose just provides a way to reliably transform and load streaming data into data stores and analytics tools. This method will not increase the capacity of the stream as it doesn’t mention anything about resharding.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569606,
        "value": "Split every shard in the stream.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569607,
        "value": "Upgrade the instance type of the EC2 instances.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569608,
        "value": "Merge every shard in the stream.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569609,
        "value": "Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 51,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569606,
        "questionId": 391499,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391499,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The developer has built a real-time IoT device monitoring application that leverages Amazon Kinesis Data Stream to ingest data. The application uses several EC2 instances for processing. Recently, the developer has observed a steady increase in the rate of data flowing into the stream, indicating that the stream’s capacity must be scaled up to sustain optimal performance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should the developer do to increase the capacity of the stream?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740337,
    "question": " AWS CodeBuild",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS CodePipeline</strong></b><span>&nbsp;is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This makes it a good choice for automating your CI/CD process and centrally monitoring application activity.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Moreover, AWS CodePipeline integrates with AWS CloudWatch, which provides a reliable, scalable, and flexible monitoring solution. You can create dashboards in CloudWatch to centrally monitor application activity and manage day-to-day development tasks.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS Fault Injection Simulator&nbsp;</strong></b><span>is incorrect because this is just a managed service that is commonly used in chaos engineering, and not for application development. It enables you to perform fault injection experiments on your AWS workloads to improve the performance and resiliency of your applications.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Elastic Beanstalk</strong></b><span>&nbsp;is incorrect because it is an orchestration service to quickly deploy and manage applications in AWS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon CodeGuru</strong></b><span>&nbsp;is incorrect because this is simply a developer tool that provides intelligent recommendations to improve the quality of your codebase and for identifying an application’s most “expensive” lines of code in terms of resource intensiveness, CPU performance, and code efficiency.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/codepipeline/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/codepipeline/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span style=\"font-family: Aptos, sans-serif; font-size: 12pt;\">&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569610,
        "value": "AWS Fault Injection Simulator",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569611,
        "value": "AWS CodePipeline",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569612,
        "value": "AWS Elastic Beanstalk",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569613,
        "value": "Amazon CodeGuru",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 52,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569611,
        "questionId": 391500,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391500,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A company has a development team that’s heavily relying on AWS CodeBuild, and CodeDeploy. The management would like to further automate its CI/CD process. They requested a system that monitors the status of each code change, from the moment it’s committed through to its deployment.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following AWS services will help you achieve this?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740338,
    "question": "Elastic Beanstalk ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The write-through strategy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the cache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always current.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>One of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can be rectified by simply adding TTL to minimize wasted space.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">implement a Write-Through caching strategy in the application and enable TTL.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Implementing Lazy Loading in the application in conjunction with the Write Through caching strategy&nbsp;</strong></b><span>is incorrect. Although the Write Through caching strategy can ensure that the cache always have the current data, the Lazy Loading strategy is not helpful in minimizing wasted space in the cluster. This combination is only helpful in scenarios where you have missing data which continues to be missing until it is added or updated on the database.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a Write-Through caching strategy&nbsp;</strong></b><span>is incorrect. Although the caching strategy will meet the first requirement of ensuring that the data in the cache is always current, it fails the second requirement of&nbsp;minimizing wasted space in the cluster as this strategy does not automatically delete the data that are never read. You should use a combination of Write-Through and TTL instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using a Lazy Loading caching strategy&nbsp;</strong></b><span>is incorrect because it is just a strategy to load data into the cache only when necessary. It doesn’t meet the requirement of the scenario, which is to not have stale data in the cache.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/caching/implementation-considerations/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/caching/implementation-considerations/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569614,
        "value": "Implement a Write Through caching strategy in the application and enable TTL in Elasticache.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569615,
        "value": "Use a Write Through caching strategy.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569616,
        "value": "Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569617,
        "value": "Use a Lazy Loading caching strategy.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 53,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569616,
        "questionId": 391501,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391501,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in DynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become stale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by automatically deleting the data that are never read.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What is the BEST way to implement this to satisfy the given requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740339,
    "question": "AWS KMS Keys ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Server-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3 objects. You use AWS KMS via the&nbsp;</span><a href=\"https://console.aws.amazon.com/kms\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>AWS Management Console</span></a><span>&nbsp;or&nbsp;</span><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>AWS KMS APIs</span></a><span>&nbsp;to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The first time you add an SSE-KMS–encrypted object to a bucket in a region, a default KMS key is created for you automatically. This key is used for SSE-KMS encryption unless you select a KMS key that you created separately using AWS Key Management Service. Creating your own KMS key gives you more flexibility, including the ability to create, rotate, disable, and define access controls and audit the encryption keys used to protect your data.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To upload an object to the S3 bucket, which uses&nbsp;SSE-KMS, you have&nbsp;to send a request with an&nbsp;x-amz-server-side-encryption&nbsp;header with the value of&nbsp;aws:kms. There’s also an optional&nbsp;x-amz-server-side-encryption-aws-kms-key-id&nbsp;header, which specifies the ID of the AWS KMS master encryption key that was used for the object. The Amazon S3 API also supports encryption context with the&nbsp;x-amz-server-side-encryption-context&nbsp;header.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you upload an object, you can specify the KMS key using the&nbsp;x-amz-server-side-encryption-aws-kms-key-id&nbsp;header. If the header is not present in the request, Amazon S3 assumes the default KMS key. Regardless, the KMS key ID that Amazon S3 uses for object encryption must match the KMS key ID in the policy, otherwise, Amazon S3 denies the request.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Include the&nbsp;x-amz-server-side-encryption&nbsp;header with a value of&nbsp;aws:kms&nbsp;in your upload request.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Including the&nbsp;x-amz-server-side</strong></b><b><strong class=\"Editor__editor-text-bold___25KrR\" style=\"font-family: Arial, sans-serif;\">​</strong></b><b><strong class=\"Editor__editor-text-bold___25KrR\">-encryption</strong></b><b><strong class=\"Editor__editor-text-bold___25KrR\" style=\"font-family: Arial, sans-serif;\">​</strong></b><b><strong class=\"Editor__editor-text-bold___25KrR\">-customer-algorithm,&nbsp;x-amz-server-side-encryption-customer-key&nbsp;and&nbsp;x-amz-server-side-encryption-customer-key-MD5&nbsp;headers with appropriate values in the upload request&nbsp;</strong></b><span>is incorrect because these headers are only required if your S3 bucket is using&nbsp;Server-Side Encryption with Customer-Provided Keys (SSE-C).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Including the&nbsp;x-amz-server-side-encryption&nbsp;header with a value of&nbsp;aws:kms&nbsp;as well as the&nbsp;x-amz-server-side-encryption-aws-kms-key-id&nbsp;header containing the ID of the default AWS KMS key in your upload request&nbsp;</strong></b><span>is incorrect. Although this is a valid option, you actually don’t need to add the&nbsp;x-amz-server-side-encryption-aws-kms-key-id&nbsp;header if you will be using the default AWS KMS key. Take note that the scenario explicitly mentioned to provide a solution with the LEAST amount of configuration.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Including the&nbsp;x-amz-server-side-encryption&nbsp;header with a value of&nbsp;AES256&nbsp;in your upload request&nbsp;</strong></b><span>is incorrect because the value should be set as&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">aws:kms</em></i><span>&nbsp;instead. The value of&nbsp;AES256&nbsp;is only applicable for SSE-S3 and SSE-C.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569618,
        "value": "Include the x-amz-server-side-encryption-customer-algorithm, x-amz-server-side-encryption-customer-key, and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569619,
        "value": "Include the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569620,
        "value": "Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569621,
        "value": "Include the x-amz-server-side-encryption header with a value of AES256 in your upload request.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 54,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569620,
        "questionId": 391502,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391502,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 instances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The batch job must upload files to the bucket using the default AWS KMS key to protect the data at rest.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should you do to satisfy this requirement with the LEAST amount of configuration?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740340,
    "question": "NodeJS application",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>With&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Elastic Beanstalk</strong></b><span>, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer in this scenario is&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Elastic Beanstalk.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS CloudFormation</strong></b><span>&nbsp;is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS SAM&nbsp;</strong></b><span>is incorrect because&nbsp;the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can’t host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS CodeDeploy&nbsp;</strong></b><span>is incorrect because this is primarily used for deployment and not as an orchestration service for your applications.&nbsp;AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:&nbsp;</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569622,
        "value": "AWS SAM",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569623,
        "value": "AWS CodeDeploy",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569624,
        "value": "AWS Elastic Beanstalk",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569625,
        "value": "AWS CloudFormation",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 55,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569622,
        "questionId": 391503,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391503,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don’t need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which service will you use to easily deploy and manage the application?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740341,
    "question": "Elastic Beanstalk",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. It is important to note that&nbsp;Amazon S3 does not store the encryption key you provide. Instead, it is stored in a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the valid consideration that the developer should keep in mind when implementing this architecture is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">if you lose the encryption key, you lose the object</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the salted HMAC value can be used to derive the value of the encryption key</strong></b><span>&nbsp;is incorrect because the salted HMAC is just used to validate future encryption requests. It cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">the salted HMAC value can be used to decrypt the contents of the encrypted object</strong></b><span>&nbsp;is incorrect because just as mentioned above, the HMAC&nbsp;cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">if you lose the encryption key, the salted HMAC value can be used to decrypt the object</strong></b><span>&nbsp;is incorrect because&nbsp;if you lose the encryption key, you lose the object. You cannot use the salted HMAC value to decrypt the object.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569626,
        "value": "The salted HMAC value can be used to derive the value of the encryption key.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569627,
        "value": "The salted HMAC value can be used to decrypt the contents of the encrypted object.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569628,
        "value": "If you lose the encryption key, the salted HMAC value can be used to decrypt the object.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569629,
        "value": "If you lose the encryption key, you lose the object.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 56,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569629,
        "questionId": 391504,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391504,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer is designing a multi-tiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which uses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In this configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message authentication code (HMAC) value of the encryption key in order to validate future requests.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is a valid consideration that the developer should keep in mind when implementing this architecture?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740342,
    "question": "DynamoDB table ",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Partitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a “hot” partition), or when workloads rely on short periods of time with high usage (a “burst” of read or write activity). This problem can be more pronounced in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot partitions and throttling, you must optimize your table and partition structure.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>DynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard limit. This means that adaptive capacity can’t solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize your table and partition structure.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To solve this issue, consider one or more of the following solutions:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Increase the amount of read or write capacity for your table to anticipate short-term spikes or bursts in read or write operations.&nbsp;</strong></b><span>If you decide later you don’t need the additional capacity, decrease it. Take note that Before deciding on how much to increase read or write capacity, consider the best practices in designing your partition keys.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Implement&nbsp;error retries and exponential backoff.&nbsp;</strong></b><span>This technique improves reliability in stateful applications by allowing retries after progressively longer wait times between errors. AWS SDKs have built-in support for this logic.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Distribute your read operations and&nbsp;</strong></b><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><b><strong class=\"Editor__editor-text-bold___25KrR\">write operations</strong></b></a><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;as evenly as possible across your table.</strong></b><span>&nbsp;A hot partition can degrade the overall performance, particularly for stateful applications that repeatedly access certain keys.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">– Implement a caching solution, such as&nbsp;DynamoDB Accelerator (DAX)&nbsp;or&nbsp;Amazon ElastiCache.</strong></b><span>&nbsp;For stateful workloads with frequent reads to session or static data, caching can significantly reduce database access.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;– Implement error retries and exponential backoff.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;– Refactor your application to distribute your read and write operations as evenly as possible across your table.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Using a DynamoDB Accelerator (DAX)&nbsp;</strong></b><span>is incorrect. Although it can reduce read load by caching frequently accessed items, it will simply incurs additional costs to maintain the DAX cluster. Since the issue can be addressed by improving workload distribution without extra cost, DAX is not a cost-effective solution for this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Increasing the amount of read or write capacity for your table&nbsp;</strong></b><span>is incorrect because, while it can alleviate throttling by temporarily handling higher traffic, it is an expensive solution that contradicts the minimal-cost requirement. Scaling read and write capacity (RCU and WCU) can lead to significant increases in operational costs, especially for stateful applications with continuous usage patterns.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Implementing read sharding to distribute workloads evenly</strong></b><span>&nbsp;is incorrect because&nbsp;the issue primarily involves write operations in a stateful workload. Instead, write sharding should be used to better distribute writes across the partition key space. This can be achieved by appending either random or calculated suffixes to partition key values, which spreads the data more evenly across partitions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/throttled-ddb/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/premiumsupport/knowledge-center/throttled-ddb/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569630,
        "value": "Use DynamoDB Accelerator (DAX).",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569631,
        "value": "Implement error retries and exponential backoff.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569632,
        "value": "Implement read sharding to distribute workloads evenly.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569633,
        "value": "Refactor your application to distribute your read and write operations as evenly as possible across your table.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569634,
        "value": "Increase the amount of read or write capacity for your table.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 57,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569630,
        "questionId": 391505,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": false,
        "choiceId": 1569632,
        "questionId": 391505,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391505,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The read and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user sessions. Despite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon further investigation, it is found that a “hot partition” is being accessed more frequently than others by downstream services.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should be done to resolve this issue with MINIMAL cost? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740343,
    "question": "On-premises data center",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS CloudHSM</strong></b><span>&nbsp;provides hardware security modules in AWS Cloud. A hardware security module (HSM) is a computing device that processes cryptographic operations and provides secure storage for cryptographic keys.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Use symmetric and asymmetric algorithms to encrypt and decrypt data.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Cryptographically sign data (including code signing) and verify signatures.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Generate cryptographically secure random data.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You should consider using AWS CloudHSM instead of AWS KMS if you require:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Keys stored in dedicated, third-party validated hardware security modules under your exclusive control.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– FIPS 140-2 compliance.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– Integration with applications using PKCS#11, Java JCE, or Microsoft CNG interfaces.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>– High-performance in-VPC cryptographic acceleration (bulk crypto).</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">import the encryption keys from your on-premises key management service to AWS CloudHSM</strong></b><span>.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Using AWS KMS to store and manage the encryption keys&nbsp;</strong></b><span>is incorrect. Although AWS KMS supports asymmetric encryption, it doesn’t provide dedicated, third-party validated hardware security modules which are under your exclusive control. You have to use CloudHSM instead.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Importing the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys&nbsp;</strong></b><span>is incorrect because you can’t store KMS keys to AWS Secrets Manager.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Developing a custom key management service using the AWS Encryption SDK&nbsp;</strong></b><span>is incorrect because this entails a lot of effort to implement. Moreover,&nbsp;the AWS Encryption SDK only encrypts your data using a symmetric key algorithm which doesn’t comply with the requirements provided in the scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/manage-keys.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/cloudhsm/latest/userguide/manage-keys.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/cloudhsm/faqs/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/cloudhsm/faqs/</span></a></p>",
    "choices": [
      {
        "id": 1569635,
        "value": "Import the encryption keys from your on-premises key management service to AWS CloudHSM.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569636,
        "value": "Use AWS KMS to store and manage the encryption keys.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569637,
        "value": "Develop a custom key management service using the AWS Encryption SDK.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569638,
        "value": "Import the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 58,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569635,
        "questionId": 391506,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391506,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A cryptocurrency exchange portal has a key management service hosted in their on-premises data center, which stores encryption keys and uses an RSA asymmetric encryption algorithm. The company has recently implemented a hybrid cloud architecture in AWS and you were assigned to migrate the exchange portal to their cloud infrastructure. For security compliance, the keys should be stored in dedicated, third-party validated hardware security modules under your exclusive control.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the BEST solution that you should implement to meet the above requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740344,
    "question": "AWS SAM",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want, and AWS CloudFormation takes care of provisioning and configuring those resources for you.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections for you to define your infrastructure code.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For&nbsp;serverless applications&nbsp;(also referred to as Lambda-based applications), the&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Transform</strong></b><span>&nbsp;section specifies the version of the&nbsp;AWS Serverless Application Model (AWS SAM)&nbsp;to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it is processed.&nbsp; More specifically, the&nbsp;AWS::Serverless&nbsp;transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In the following example, the template uses AWS SAM syntax to simplify the declaration of a Lambda function and its execution role.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>Transform: AWS::Serverless-2016-10-31\nResources:\n  MyServerlessFunctionLogicalID:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: index.handler\n      Runtime: nodejs8.10\n      CodeUri: 's3://testBucket/mySourceCode.zip'\n</code></pre><span>Hence, the correct answer is:</span><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;Add a&nbsp;Transform&nbsp;section in the template to specify the version of the&nbsp;AWS Serverless Application Model (AWS SAM)&nbsp;to use.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Add a&nbsp;Parameters&nbsp;section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use&nbsp;</strong></b><span>is incorrect because this is just the part of the template that contains values to pass to your template at runtime when you create or update a stack.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Add a&nbsp;Resources&nbsp;section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.&nbsp;</strong></b><span>is incorrect because this is primarily used to specify the stack resources and their properties.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Add a&nbsp;Mappings&nbsp;section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use&nbsp;</strong></b><span>is incorrect because this just lists a mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. You can match a key to a corresponding value by using the&nbsp;Fn::FindInMap&nbsp;intrinsic function in the&nbsp;Resources&nbsp;and&nbsp;Outputs&nbsp;sections.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569639,
        "value": "Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569640,
        "value": "Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569641,
        "value": "Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569642,
        "value": "Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 59,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569641,
        "questionId": 391507,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391507,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager instructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used to launch a similar infrastructure in another region.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>What should you do in order to accomplish this task?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740345,
    "question": "CloudFormation",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To share information between stacks, export a stack’s output values. Other stacks that are in the same AWS account and region can import the exported values. For example, you might have a single networking stack that exports the IDs of a subnet and security group for public web servers. Stacks with a public webserver can easily import those networking resources. You don’t need to hard code resource IDs in the stack’s template or pass IDs as input parameters. To export a stack’s output value, use the Export field in the Output section of the stack’s template. To import those values, use the Fn::ImportValue function in the template for the other stacks.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In this scenario, we can expose the API endpoint to other stacks by adding the Export property in the Outputs section. In the example below, we use ‘SimpleAPI’ as the name of the value to be exported:</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>AWSTemplateFormatVersion:  '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n  Description: Simple APT\n  Resources:\n    HelloWor1dFunction:\n    Type: ASW::Serverless:: Function\n    Properties:\n      Coders: s3://td-bucket-san/app.zip\n      PackageType: Zip\n      Handler: app. lanbda_handler\n      Runtine: python 3.9 \n      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            path: /hello\n            Method: get\n\nOutputs:\n  HelloWor1dApi :\n    Description: \"APT Gateway endpoint URL™\n    Value: ISub \"https://${ServerlessRestapi) .execute-api.S(AS: :Region} .amazonas con) abc/hello/\"\n    Export:\n      Name: SimpleAPI\n</code></pre></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>To reference the endpoint’s value in other templates, simply use the&nbsp;Fn::ImportValue&nbsp;function and specify&nbsp;SimpleAPI&nbsp;as its parameter.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Include the&nbsp;Export&nbsp;property in the original template’s&nbsp;Outputs&nbsp;section. Then use the&nbsp;Fn::ImportValue&nbsp;function in other templates to retrieve the exported value.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Include the&nbsp;Export&nbsp;property in the original template’s&nbsp;Outputs&nbsp;section. Then use the&nbsp;Ref&nbsp;function in other templates to retrieve the exported value&nbsp;</strong></b><span>is incorrect. The Ref function is not capable of returning resource/parameter values from other templates.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Specify&nbsp;HelloWorldApi&nbsp;as the parameter when using the&nbsp;&nbsp;Fn::ImportValue&nbsp;function in other templates</strong></b><span>&nbsp;is incorrect.&nbsp;Fn::ImportValue&nbsp;is not a standalone command. It cannot be used to directly reference values from the Outputs section. You must first declare the name of the resource to be exported using the&nbsp;Export&nbsp;property.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Add the&nbsp;AWS::Include&nbsp;transform in the original template to directly import the&nbsp;HelloWorldFunction&nbsp;resource to other templates&nbsp;</strong></b><span>is incorrect. The&nbsp;AWS::Include&nbsp;transform is used for referencing templates stored in Amazon S3. This allows you to reuse resources in other CloudFormation templates. Keep in mind that in this scenario, you only need to solve the issue of exporting the API endpoint in other templates.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html</span></a></p>",
    "choices": [
      {
        "id": 1569643,
        "value": "Include the Export property in the original template’s Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569644,
        "value": "Include the Export property in the original template’s Outputs section. Then use the Ref function in other templates to retrieve the exported value.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569645,
        "value": "Specify HelloWorldApi as parameter when using the Fn::ImportValue function in other templates.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569646,
        "value": "Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 60,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569646,
        "questionId": 391508,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391508,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A developer wants to deploy a REST API using the CloudFormation template shown below:</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><pre data-highlight-language=\"java\"><code>AWSTemplateFormatVersion:  '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n  Description: Simple APT\n  Resources:\n    HelloWor1dFunction:\n    Type: ASW::Serverless:: Function\n    Properties:\n      Coders: s3://td-bucket-san/app.zip\n      PackageType: Zip\n      Handler: app. lanbda_handler\n      Runtine: python 3.9 \n      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            path: /hello\n            Method: get\n\nOutputs:\n  HelloWor1dApi :\n    Description: \"APT Gateway endpoint URL™\n    Value: ISub \"https://${ServerlessRestapi) .execute-api.S(AS: :Region} .amazonas con) abc/hello/\"\n</code></pre><span>Which changes should be done so that the newly created API endpoint can be referenced to other stacks?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740346,
    "question": "CloudFormation template",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If the parameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The&nbsp;</span><a href=\"http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>Parameters</span></a><span>&nbsp;section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong></b><span>&nbsp;is incorrect because the Systems Manager State Manager service&nbsp;simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can’t be used as a parameter store that refers to the latest AMI of your application.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments</strong></b><span>&nbsp;is incorrect because&nbsp;AWS Service Catalog is not suitable in this scenario since this service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. In addition, AWS Config is simply a service that enables you to assess, audit, and evaluate the configurations of your AWS resources, which clearly is irrelevant in this case as the developer won’t be able to use this to store the latest AMI IDs.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments</strong></b><span>&nbsp;is incorrect because, just as mentioned above, the AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more appropriate solution for this scenario would be to use the Systems Manager Parameter Store to retrieve the latest AMI IDs for your template.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569647,
        "value": "Set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569648,
        "value": "Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569649,
        "value": "Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569650,
        "value": "Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 61,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569649,
        "questionId": 391509,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391509,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>For application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A developer was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740347,
    "question": "CloudFormation template",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">AWS CloudFormation StackSets</strong></b><span>&nbsp;extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>A&nbsp;</span><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">stack set</strong></b></i><span>&nbsp;lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set’s AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence,&nbsp;the correct solution in this scenario is to&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">update the stacks on multiple AWS accounts using CloudFormation StackSets.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>After you’ve defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets</strong></b><span>&nbsp;is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances</strong></b><span>&nbsp;is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts</strong></b><span>&nbsp;is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569651,
        "value": "Update the stacks on multiple AWS accounts using CloudFormation StackSets.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569652,
        "value": "Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569653,
        "value": "Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569654,
        "value": "Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 62,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": false,
        "choiceId": 1569654,
        "questionId": 391510,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391510,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740348,
    "question": "Amazon ECS Cluster",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">Amazon RDS Read Replicas</strong></b><span>&nbsp;provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>In-memory data caching can be one of the most effective strategies to improve your overall application performance and reduce your database costs. Caching can be applied to any type of database, including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB, and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement, and by doing so, your application performance regarding both scale and speed is dramatically improved.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, run, and scale popular open-source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answers in this scenario are:</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><b><strong class=\"Editor__editor-text-bold___25KrR\">– Set up read replicas for the RDS database instance and route read queries to these replicas.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">&nbsp;– Implement database caching using Amazon ElastiCache.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Replace the database with Amazon MemoryDB for Redis&nbsp;</strong></b><span>is incorrect because&nbsp;Redshift is primarily used for online analytics processing applications (OLAP) and as a data warehouse. Hence, this will not improve the read performance of your application.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Cache the database response using Amazon CloudFront&nbsp;</strong></b><span>is incorrect. Although CloudFront can provide caching and for CDN, it is not suitable to be used for database caching. Using Read Replicas and ElastiCache are more appropriate features to be used in this scenario.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Implement a Multi-AZ deployment configuration for the RDS DB instance</strong></b><i><b><strong class=\"Editor__editor-text-bold___25KrR Editor__editor-text-italic___C9n8O\">&nbsp;</strong></b></i><span>is incorrect because configuring a Multi-AZ RDS just improves the availability of the database but does not drastically improve the read performance, which Read Replicas can provide.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/caching/database-caching/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/caching/database-caching/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/rds/details/read-replicas/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/rds/details/read-replicas/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://aws.amazon.com/elasticache/\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://aws.amazon.com/elasticache/</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569655,
        "value": "Implement database caching using Amazon ElastiCache.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569656,
        "value": "Cache the database response using Amazon CloudFront.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569657,
        "value": "Replace the database with Amazon MemoryDB for Redis",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569658,
        "value": "Implement a Multi-AZ deployment configuration for the RDS DB instance.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569659,
        "value": "Set up read replicas for the RDS database instance and route read queries to these replicas.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 63,
    "type": "MULTIPLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569659,
        "questionId": 391511,
        "response": null,
        "answerFeedback": null
      },
      {
        "isCorrect": false,
        "choiceId": 1569656,
        "questionId": 391511,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391511,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>An application experiences a sluggish response whenever there is a surge in requests involving read queries. The developer has already attempted to improve performance by optimizing the queries. However, the problem still persists even after applying the change. The application is hosted in an Amazon ECS Cluster and uses a MySQL database backed by Amazon RDS.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following could the developer do to resolve the performance issue? (Select TWO.)</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  },
  {
    "answerId": 39740349,
    "question": " Lambda function",
    "questionClass": "EDU",
    "hint": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Amazon DynamoDB is integrated with AWS Lambda so that you can create&nbsp;</span><i><em class=\"Editor__editor-text-italic___C9n8O\">triggers</em></i><span>—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table’s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><br></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to simply copy each stream record to persistent storage, such as Amazon Simple Storage Service (Amazon S3), to create a permanent audit trail of write activity in your table. Or suppose you have a mobile gaming app that writes to a&nbsp;GameScores&nbsp;table. Whenever the&nbsp;TopScore&nbsp;attribute of the&nbsp;GameScores&nbsp;table is updated, a corresponding stream record is written to the table’s stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to&nbsp;GameScores&nbsp;or that do not modify the&nbsp;TopScore&nbsp;attribute.)</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Hence, the correct answer is:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Enable DynamoDB Streams and configure it as the event source for the Lambda function.</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use Amazon&nbsp;EventBridge (Amazon CloudWatch Events)</strong></b><span>&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">to track all new data in your table and configure it as the event source for the Lambda function</strong></b><span>&nbsp;is incorrect because the Amazon&nbsp;EventBridge (Amazon CloudWatch Events)&nbsp;service does not have the capability to track any new inserts or updates on the DynamoDB table. Although Amazon EventBridge (Amazon CloudWatch Events) delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources, it cannot provide tracking of the DynamoDB’s table activities.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Enable DynamoDB Transactions and configure it as the event source for the Lambda function&nbsp;</strong></b><span>is incorrect because Amazon DynamoDB transactions just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. This feature is primarily used to provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>The option that says:&nbsp;</span><b><strong class=\"Editor__editor-text-bold___25KrR\">Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function&nbsp;</strong></b><span>is incorrect because using DynamoDB Streams is a better option than Kinesis.&nbsp;In addition, Kinesis does not have the capability to immediately track any new inserts or updates on the DynamoDB table.</span></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><b><strong class=\"Editor__editor-text-bold___25KrR\">References:</strong></b></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\" target=\"_blank\" class=\"Editor__editor-link___3vl2C\"><span>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</span></a></p><p class=\"Editor__editor-paragraph___HPmFS\"><span>&nbsp;</span></p>",
    "choices": [
      {
        "id": 1569660,
        "value": "Enable DynamoDB Streams and configure it as the event source for the Lambda function.",
        "isCorrect": true,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569661,
        "value": "Use Amazon EventBridge (Amazon CloudWatch Events) to track all new data in your table and configure it as the event source for the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569662,
        "value": "Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      },
      {
        "id": 1569663,
        "value": "Enable DynamoDB Transactions and configure it as the event source for the Lambda function.",
        "isCorrect": false,
        "isAdvancedEditor": false,
        "language": "",
        "responses": []
      }
    ],
    "idx": 64,
    "type": "SINGLE",
    "userAnswers": [
      {
        "isCorrect": true,
        "choiceId": 1569660,
        "questionId": 391512,
        "response": null,
        "answerFeedback": null
      }
    ],
    "exact": null,
    "isFinalScore": false,
    "media": {
      "audioUrl": null,
      "pictureUrl": null,
      "videoUrl": null
    },
    "score": null,
    "questionId": 391512,
    "description": "<p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>You are developing an online learning platform using Lambda, Elastic Beanstalk, and DynamoDB. There is a requirement that whenever a new customer is added to the DynamoDB table, it will invoke a Lambda function that sends a welcome email to the customer.</span></p><p class=\"Editor__editor-paragraph___HPmFS\" dir=\"ltr\"><span>Which of the following is the MOST suitable solution that you should use to implement this feature?</span></p>",
    "language": "java",
    "file": null,
    "noMatchResponses": [],
    "answerReview": null,
    "answerFeedbacks": [],
    "topic": {
      "id": 8310,
      "name": "CDA-Practice exam-3",
      "skillId": "",
      "categoryId": 1653
    }
  }
]